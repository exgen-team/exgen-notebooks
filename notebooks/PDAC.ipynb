{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8763af78-fb04-4256-8f27-e0cdcfe21083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enriched 0/552\n",
      "enriched 25/552\n",
      "enriched 50/552\n",
      "enriched 75/552\n",
      "enriched 100/552\n",
      "enriched 125/552\n",
      "enriched 150/552\n",
      "enriched 225/552\n",
      "enriched 250/552\n",
      "enriched 275/552\n",
      "enriched 300/552\n",
      "enriched 325/552\n",
      "enriched 350/552\n",
      "enriched 375/552\n",
      "enriched 400/552\n",
      "enriched 425/552\n",
      "enriched 450/552\n",
      "enriched 525/552\n",
      "enriched 575/552\n",
      "enriched 600/552\n",
      "enriched 625/552\n",
      "enriched 650/552\n",
      "enriched 725/552\n",
      "enriched 850/552\n",
      "enriched 1025/552\n",
      "Wrote: C:\\path\\to\\pdac_company_enriched.csv\n",
      "\n",
      "[1/552]  - 1911 Gold Corporation (https://gcl.lk)\n",
      "\n",
      "[6/552]  - Corporation (https://corporationwiki.com)\n",
      "\n",
      "[9/552]  - Agnico Eagle Mines Limited (https://agnicoeagle.com)\n",
      "\n",
      "[10/552]  - AIL Mining (https://ailmining.com)\n",
      "\n",
      "[22/552]  - American Lithium Corp. (https://americanlithiumcorp.com)\n",
      "\n",
      "[30/552]  - Uranium Corp. (https://ucil.gov.in)\n",
      "\n",
      "[32/552]  - Arizona Metals Corp. (https://arizonametalscorp.com)\n",
      "\n",
      "[34/552]  - Artemis Gold Inc. (https://artemisgoldinc.com)\n",
      "\n",
      "[47/552]  - AURYN Mining Corp. (https://aurynminingcorp.com)\n",
      "\n",
      "[48/552]  - Inc. (https://inc.com)\n",
      "\n",
      "[50/552]  - Ltd. (https://ltdcommodities.com)\n",
      "\n",
      "[53/552]  - Ltd. (https://ltdcommodities.com)\n",
      "\n",
      "[57/552]  - Corp. (https://corporationwiki.com)\n",
      "\n",
      "[63/552]  - Becker Mining Systems (https://becker-mining.com)\n",
      "\n",
      "[64/552]  - Co., Ltd. (https://co.ltd)\n",
      "\n",
      "[66/552]  - BeMetals Corp. (https://bemetalscorp.com)\n",
      "\n",
      "[69/552]  - BID Canada Ltd. (https://bidcanadaltd.com)\n",
      "\n",
      "[71/552]  - Inc. (https://inc.com)\n",
      "\n",
      "[72/552]  - Bold Ventures Inc. (https://boldventuresinc.com)\n",
      "\n",
      "[73/552]  - Limited (https://limitedrungames.com)\n",
      "\n",
      "[84/552]  - Inc. (https://inc.com)\n",
      "\n",
      "[89/552]  - CaNickel Mining Limited (https://canickel.com)\n",
      "\n",
      "[95/552]  - Ltd. (https://ltdcommodities.com)\n",
      "\n",
      "[99/552]  - China Gold International (https://chinagoldintl.com)\n",
      "\n",
      "[100/552]  - Limited (https://limitedrungames.com)\n",
      "\n",
      "[105/552]  - Corporation (https://corporationwiki.com)\n",
      "\n",
      "[106/552]  - Corporation (https://corporationwiki.com)\n",
      "\n",
      "[108/552]  - Coppernico Metals (https://coppernicometals.com)\n",
      "\n",
      "[117/552]  - Cyprium Metals Limited (https://cypriummetals.com)\n",
      "\n",
      "[118/552]  - Dakota Gold Corp. (https://dakotagoldcorp.com)\n",
      "\n",
      "[121/552]  - Limited (https://limitedrungames.com)\n",
      "\n",
      "[122/552]  - Denarius Metals (https://denariusmetals.com)\n",
      "\n",
      "[127/552]  - Trading Inc. (https://trading-in.com)\n",
      "\n",
      "[130/552]  - Di-Corp (https://dicorpo.com.br)\n",
      "\n",
      "[132/552]  - DLP Resources Inc. (https://dlpresourcesinc.com)\n",
      "\n",
      "[133/552]  - Dolly Varden Silver (https://dollyvardensilver.com)\n",
      "\n",
      "[136/552]  - Exploration (https://exploration.io)\n",
      "\n",
      "[142/552]  - Dynasty Gold Corp. (https://dynastygoldcorp.com)\n",
      "\n",
      "[143/552]  - E3 Lithium (https://e3lithium.ca)\n",
      "\n",
      "[154/552]  - Emperor Metals (https://emperormetals.com)\n",
      "\n",
      "[158/552]  - Englobe Corp. (https://englobecorp.com)\n",
      "\n",
      "[159/552]  - Equinox Gold (https://equinoxgold.com)\n",
      "\n",
      "[160/552]  - Development Corp. (https://developmentcorporate.com)\n",
      "\n",
      "[164/552]  - Evolution Mining (https://evolutionmining.com.au)\n",
      "\n",
      "[168/552]  - Exploration Drill Masters (https://explorationdrillmasters.com)\n",
      "\n",
      "[174/552]  - Fireweed Metals (https://fireweedmetals.com)\n",
      "\n",
      "[176/552]  - First Class Metals PLC (https://firstclassmetalsplc.com)\n",
      "\n",
      "[185/552]  - Freegold Ventures Limited (https://freegoldventures.com)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 24 0 (offset 0)\n",
      "Ignoring wrong pointing object 26 0 (offset 0)\n",
      "Ignoring wrong pointing object 28 0 (offset 0)\n",
      "Ignoring wrong pointing object 30 0 (offset 0)\n",
      "Ignoring wrong pointing object 32 0 (offset 0)\n",
      "Ignoring wrong pointing object 282 0 (offset 0)\n",
      "Ignoring wrong pointing object 284 0 (offset 0)\n",
      "Ignoring wrong pointing object 357 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[187/552]  - Fury Gold Mines (https://furygoldmines.com)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 24 0 (offset 0)\n",
      "Ignoring wrong pointing object 27 0 (offset 0)\n",
      "Ignoring wrong pointing object 37 0 (offset 0)\n",
      "Ignoring wrong pointing object 44 0 (offset 0)\n",
      "Ignoring wrong pointing object 73 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[189/552]  - Communications Inc. (https://communicationsinc.co.uk)\n",
      "\n",
      "[190/552]  - Galiano Gold (https://galianogold.com)\n",
      "\n",
      "[192/552]  - Galway Metals Inc. (https://galwaymetalsinc.com)\n",
      "\n",
      "[199/552]  - LLC (https://garant.ru)\n",
      "\n",
      "[202/552]  - Minerals) (https://minerals.net)\n",
      "\n",
      "[205/552]  - Geotech Ltd. (https://geotechltd.com)\n",
      "\n",
      "[219/552]  - Goliath Resources Limited (https://goliathresourcesltd.com)\n",
      "\n",
      "[221/552]  - Partners Inc. (https://partnersbend.org)\n",
      "\n",
      "[225/552]  - Grid Metals Corp. (https://gridmetalscorp.com)\n",
      "\n",
      "[236/552]  - Honey Badger Silver (https://honeybadgersilver.com)\n",
      "\n",
      "[239/552]  - HydroTech Mining (https://hydrotechmining.com)\n",
      "\n",
      "[246/552]  - Inventus Mining (https://inventusmining.com)\n",
      "\n",
      "[252/552]  - Ivanhoe Mines (https://ivanhoemines.com)\n",
      "\n",
      "[253/552]  - Jaguar Mining (https://jaguarmining.com)\n",
      "\n",
      "[256/552]  - Jindalee Lithium (https://jindaleelithium.com)\n",
      "\n",
      "[258/552]  - Juggernaut Exploration Ltd. (https://juggernautexploration.com)\n",
      "\n",
      "[273/552]  - KPI Mining Solutions (https://kpimining.com)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\julian.diaz\\AppData\\Local\\Temp\\ipykernel_13740\\1932529031.py:179: XMLParsedAsHTMLWarning: It looks like you're using an HTML parser to parse an XML document.\n",
      "\n",
      "Assuming this really is an XML document, what you're doing might work, but you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the Python package 'lxml' installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "\n",
      "If you want or need to use an HTML parser on this document, you can make this warning go away by filtering it. To do that, run this code before calling the BeautifulSoup constructor:\n",
      "\n",
      "    from bs4 import XMLParsedAsHTMLWarning\n",
      "    import warnings\n",
      "\n",
      "    warnings.filterwarnings(\"ignore\", category=XMLParsedAsHTMLWarning)\n",
      "\n",
      "  soup = BeautifulSoup(html, \"lxml\")\n",
      "C:\\Users\\julian.diaz\\AppData\\Local\\Temp\\ipykernel_13740\\1932529031.py:269: XMLParsedAsHTMLWarning: It looks like you're using an HTML parser to parse an XML document.\n",
      "\n",
      "Assuming this really is an XML document, what you're doing might work, but you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the Python package 'lxml' installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "\n",
      "If you want or need to use an HTML parser on this document, you can make this warning go away by filtering it. To do that, run this code before calling the BeautifulSoup constructor:\n",
      "\n",
      "    from bs4 import XMLParsedAsHTMLWarning\n",
      "    import warnings\n",
      "\n",
      "    warnings.filterwarnings(\"ignore\", category=XMLParsedAsHTMLWarning)\n",
      "\n",
      "  soup = BeautifulSoup(html, \"lxml\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[277/552]  - Lahontan Gold Corp. (https://lahontangoldcorp.com)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 24 0 (offset 0)\n",
      "Ignoring wrong pointing object 26 0 (offset 0)\n",
      "Ignoring wrong pointing object 33 0 (offset 0)\n",
      "Ignoring wrong pointing object 35 0 (offset 0)\n",
      "Ignoring wrong pointing object 37 0 (offset 0)\n",
      "Ignoring wrong pointing object 39 0 (offset 0)\n",
      "Ignoring wrong pointing object 41 0 (offset 0)\n",
      "Ignoring wrong pointing object 49 0 (offset 0)\n",
      "Ignoring wrong pointing object 51 0 (offset 0)\n",
      "Ignoring wrong pointing object 56 0 (offset 0)\n",
      "Ignoring wrong pointing object 61 0 (offset 0)\n",
      "Ignoring wrong pointing object 63 0 (offset 0)\n",
      "Ignoring wrong pointing object 65 0 (offset 0)\n",
      "Ignoring wrong pointing object 67 0 (offset 0)\n",
      "Ignoring wrong pointing object 69 0 (offset 0)\n",
      "Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 26 0 (offset 0)\n",
      "Ignoring wrong pointing object 29 0 (offset 0)\n",
      "Ignoring wrong pointing object 31 0 (offset 0)\n",
      "Ignoring wrong pointing object 39 0 (offset 0)\n",
      "Ignoring wrong pointing object 41 0 (offset 0)\n",
      "Ignoring wrong pointing object 43 0 (offset 0)\n",
      "Ignoring wrong pointing object 45 0 (offset 0)\n",
      "Ignoring wrong pointing object 51 0 (offset 0)\n",
      "Ignoring wrong pointing object 56 0 (offset 0)\n",
      "Ignoring wrong pointing object 58 0 (offset 0)\n",
      "Ignoring wrong pointing object 60 0 (offset 0)\n",
      "Ignoring wrong pointing object 62 0 (offset 0)\n",
      "Ignoring wrong pointing object 64 0 (offset 0)\n",
      "Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 26 0 (offset 0)\n",
      "Ignoring wrong pointing object 28 0 (offset 0)\n",
      "Ignoring wrong pointing object 36 0 (offset 0)\n",
      "Ignoring wrong pointing object 38 0 (offset 0)\n",
      "Ignoring wrong pointing object 40 0 (offset 0)\n",
      "Ignoring wrong pointing object 42 0 (offset 0)\n",
      "Ignoring wrong pointing object 48 0 (offset 0)\n",
      "Ignoring wrong pointing object 53 0 (offset 0)\n",
      "Ignoring wrong pointing object 55 0 (offset 0)\n",
      "Ignoring wrong pointing object 57 0 (offset 0)\n",
      "Ignoring wrong pointing object 59 0 (offset 0)\n",
      "Ignoring wrong pointing object 61 0 (offset 0)\n",
      "Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 26 0 (offset 0)\n",
      "Ignoring wrong pointing object 34 0 (offset 0)\n",
      "Ignoring wrong pointing object 36 0 (offset 0)\n",
      "Ignoring wrong pointing object 38 0 (offset 0)\n",
      "Ignoring wrong pointing object 40 0 (offset 0)\n",
      "Ignoring wrong pointing object 42 0 (offset 0)\n",
      "Ignoring wrong pointing object 48 0 (offset 0)\n",
      "Ignoring wrong pointing object 53 0 (offset 0)\n",
      "Ignoring wrong pointing object 55 0 (offset 0)\n",
      "Ignoring wrong pointing object 57 0 (offset 0)\n",
      "Ignoring wrong pointing object 59 0 (offset 0)\n",
      "Ignoring wrong pointing object 61 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[293/552]  - Mako Mining Corp. (https://makominingcorp.com)\n",
      "\n",
      "[296/552]  - Manganese X Energy Corp. (https://manganesexenergycorp.com)\n",
      "\n",
      "[298/552]  - Maritime Resources Corp. (https://maritimeresourcescorp.com)\n",
      "\n",
      "[306/552]  - McFarlane Lake Mining (https://mcfarlanelakemining.com)\n",
      "\n",
      "[310/552]  - Services Limited (https://servicesltd.co.uk)\n",
      "\n",
      "[323/552]  - Mining Finland (https://miningfinland.com)\n",
      "\n",
      "[326/552]  - Natural Resources (https://nrar.nsw.gov.au)\n",
      "\n",
      "[344/552]  - New Age Metals (https://newagemetals.com)\n",
      "\n",
      "[352/552]  - Nickel North Exploration (https://nnexploration.com)\n",
      "\n",
      "[359/552]  - Northstar Gold Corp. (https://northstargoldcorp.com)\n",
      "\n",
      "[361/552]  - Novamera Inc. (https://novamerainc.com)\n",
      "\n",
      "[363/552]  - Nuinsco Resources Limited (https://nuinsco.ca)\n",
      "\n",
      "[370/552]  - Onyx Gold (https://onyxgoldsmiths.co.uk)\n",
      "\n",
      "[375/552]  - Orosur Mining (https://orosur.ca)\n",
      "\n",
      "[379/552]  - Management Limited (https://720ltd.com)\n",
      "\n",
      "[399/552]  - Prospector Metals Corp. (https://prospectormetalscorp.com)\n",
      "\n",
      "[404/552]  - R.T. Boyd Limited (https://rtboydlimited.com)\n",
      "\n",
      "[410/552]  - Relevant Gold (https://relevantgoldcorp.com)\n",
      "\n",
      "[413/552]  - Rio Silver Inc. (https://riosilverinc.com)\n",
      "\n",
      "[422/552]  - Royal Road Minerals (https://royalroadminerals.com)\n",
      "\n",
      "[429/552]  - Energy and Resources (https://energyandresources.org)\n",
      "\n",
      "[430/552]  - Resources (https://rff.org)\n",
      "\n",
      "[434/552]  - Scottie Resources (https://scottieresources.com)\n",
      "\n",
      "[438/552]  - Serabi Gold plc (https://serabigold.com)\n",
      "\n",
      "[443/552]  - Silver One Resources (https://silverone.com)\n",
      "\n",
      "[445/552]  - Silver Tiger Metals (https://silvertigermetals.com)\n",
      "\n",
      "[450/552]  - Sitka Gold Corp. (https://sitkagoldcorp.com)\n",
      "\n",
      "[455/552]  - Smith and Long Limited (https://smithandlong.com)\n",
      "\n",
      "[457/552]  - (SME) Inc. (https://smeinc.com)\n",
      "\n",
      "[458/552]  - Soilworks LLC (https://soilworksllc.com)\n",
      "\n",
      "[462/552]  - Southern Palladium Limited (https://southernpalladium.com)\n",
      "\n",
      "[468/552]  - Sterling Metals (https://sterlingmetals.ca)\n",
      "\n",
      "[470/552]  - Stillwater Critical Minerals (https://criticalminerals.com)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 26 0 (offset 0)\n",
      "Ignoring wrong pointing object 28 0 (offset 0)\n",
      "Ignoring wrong pointing object 39 0 (offset 0)\n",
      "Ignoring wrong pointing object 41 0 (offset 0)\n",
      "Ignoring wrong pointing object 43 0 (offset 0)\n",
      "Ignoring wrong pointing object 53 0 (offset 0)\n",
      "Ignoring wrong pointing object 84 0 (offset 0)\n",
      "Ignoring wrong pointing object 86 0 (offset 0)\n",
      "Ignoring wrong pointing object 88 0 (offset 0)\n",
      "Ignoring wrong pointing object 92 0 (offset 0)\n",
      "Ignoring wrong pointing object 112 0 (offset 0)\n",
      "Ignoring wrong pointing object 127 0 (offset 0)\n",
      "Ignoring wrong pointing object 129 0 (offset 0)\n",
      "Ignoring wrong pointing object 131 0 (offset 0)\n",
      "Ignoring wrong pointing object 133 0 (offset 0)\n",
      "Ignoring wrong pointing object 135 0 (offset 0)\n",
      "Ignoring wrong pointing object 137 0 (offset 0)\n",
      "Ignoring wrong pointing object 140 0 (offset 0)\n",
      "Ignoring wrong pointing object 142 0 (offset 0)\n",
      "Ignoring wrong pointing object 145 0 (offset 0)\n",
      "Ignoring wrong pointing object 147 0 (offset 0)\n",
      "Ignoring wrong pointing object 155 0 (offset 0)\n",
      "Ignoring wrong pointing object 157 0 (offset 0)\n",
      "Ignoring wrong pointing object 159 0 (offset 0)\n",
      "Ignoring wrong pointing object 161 0 (offset 0)\n",
      "Ignoring wrong pointing object 163 0 (offset 0)\n",
      "Ignoring wrong pointing object 165 0 (offset 0)\n",
      "Ignoring wrong pointing object 169 0 (offset 0)\n",
      "Ignoring wrong pointing object 176 0 (offset 0)\n",
      "Ignoring wrong pointing object 219 0 (offset 0)\n",
      "Ignoring wrong pointing object 226 0 (offset 0)\n",
      "Ignoring wrong pointing object 233 0 (offset 0)\n",
      "Ignoring wrong pointing object 255 0 (offset 0)\n",
      "Ignoring wrong pointing object 259 0 (offset 0)\n",
      "Ignoring wrong pointing object 261 0 (offset 0)\n",
      "Ignoring wrong pointing object 263 0 (offset 0)\n",
      "Ignoring wrong pointing object 265 0 (offset 0)\n",
      "Ignoring wrong pointing object 273 0 (offset 0)\n",
      "Ignoring wrong pointing object 304 0 (offset 0)\n",
      "Ignoring wrong pointing object 328 0 (offset 0)\n",
      "Ignoring wrong pointing object 332 0 (offset 0)\n",
      "Ignoring wrong pointing object 349 0 (offset 0)\n",
      "Ignoring wrong pointing object 358 0 (offset 0)\n",
      "Ignoring wrong pointing object 382 0 (offset 0)\n",
      "Ignoring wrong pointing object 401 0 (offset 0)\n",
      "Ignoring wrong pointing object 403 0 (offset 0)\n",
      "Ignoring wrong pointing object 411 0 (offset 0)\n",
      "Ignoring wrong pointing object 418 0 (offset 0)\n",
      "Ignoring wrong pointing object 452 0 (offset 0)\n",
      "Ignoring wrong pointing object 454 0 (offset 0)\n",
      "Ignoring wrong pointing object 461 0 (offset 0)\n",
      "Ignoring wrong pointing object 470 0 (offset 0)\n",
      "Ignoring wrong pointing object 486 0 (offset 0)\n",
      "Ignoring wrong pointing object 488 0 (offset 0)\n",
      "Ignoring wrong pointing object 490 0 (offset 0)\n",
      "Ignoring wrong pointing object 492 0 (offset 0)\n",
      "Ignoring wrong pointing object 494 0 (offset 0)\n",
      "Ignoring wrong pointing object 503 0 (offset 0)\n",
      "Ignoring wrong pointing object 505 0 (offset 0)\n",
      "Ignoring wrong pointing object 507 0 (offset 0)\n",
      "Ignoring wrong pointing object 509 0 (offset 0)\n",
      "Ignoring wrong pointing object 513 0 (offset 0)\n",
      "Ignoring wrong pointing object 537 0 (offset 0)\n",
      "Ignoring wrong pointing object 539 0 (offset 0)\n",
      "Ignoring wrong pointing object 544 0 (offset 0)\n",
      "Ignoring wrong pointing object 548 0 (offset 0)\n",
      "Ignoring wrong pointing object 571 0 (offset 0)\n",
      "Ignoring wrong pointing object 582 0 (offset 0)\n",
      "Ignoring wrong pointing object 621 0 (offset 0)\n",
      "Ignoring wrong pointing object 683 0 (offset 0)\n",
      "Ignoring wrong pointing object 698 0 (offset 0)\n",
      "Ignoring wrong pointing object 705 0 (offset 0)\n",
      "Ignoring wrong pointing object 708 0 (offset 0)\n",
      "Ignoring wrong pointing object 713 0 (offset 0)\n",
      "Ignoring wrong pointing object 728 0 (offset 0)\n",
      "Ignoring wrong pointing object 752 0 (offset 0)\n",
      "Ignoring wrong pointing object 765 0 (offset 0)\n",
      "Ignoring wrong pointing object 769 0 (offset 0)\n",
      "Ignoring wrong pointing object 792 0 (offset 0)\n",
      "Ignoring wrong pointing object 861 0 (offset 0)\n",
      "Ignoring wrong pointing object 867 0 (offset 0)\n",
      "Ignoring wrong pointing object 884 0 (offset 0)\n",
      "Ignoring wrong pointing object 925 0 (offset 0)\n",
      "Ignoring wrong pointing object 929 0 (offset 0)\n",
      "Ignoring wrong pointing object 933 0 (offset 0)\n",
      "Ignoring wrong pointing object 950 0 (offset 0)\n",
      "Ignoring wrong pointing object 952 0 (offset 0)\n",
      "Ignoring wrong pointing object 956 0 (offset 0)\n",
      "Ignoring wrong pointing object 975 0 (offset 0)\n",
      "Ignoring wrong pointing object 979 0 (offset 0)\n",
      "Ignoring wrong pointing object 999 0 (offset 0)\n",
      "Ignoring wrong pointing object 1017 0 (offset 0)\n",
      "Ignoring wrong pointing object 1019 0 (offset 0)\n",
      "Ignoring wrong pointing object 1023 0 (offset 0)\n",
      "Ignoring wrong pointing object 1040 0 (offset 0)\n",
      "Ignoring wrong pointing object 1044 0 (offset 0)\n",
      "Ignoring wrong pointing object 1062 0 (offset 0)\n",
      "Ignoring wrong pointing object 1081 0 (offset 0)\n",
      "Ignoring wrong pointing object 1101 0 (offset 0)\n",
      "Ignoring wrong pointing object 1125 0 (offset 0)\n",
      "Ignoring wrong pointing object 1147 0 (offset 0)\n",
      "Ignoring wrong pointing object 1160 0 (offset 0)\n",
      "Ignoring wrong pointing object 1173 0 (offset 0)\n",
      "Ignoring wrong pointing object 1180 0 (offset 0)\n",
      "Ignoring wrong pointing object 1183 0 (offset 0)\n",
      "Ignoring wrong pointing object 1214 0 (offset 0)\n",
      "Ignoring wrong pointing object 1234 0 (offset 0)\n",
      "Ignoring wrong pointing object 1274 0 (offset 0)\n",
      "Ignoring wrong pointing object 1307 0 (offset 0)\n",
      "Ignoring wrong pointing object 1315 0 (offset 0)\n",
      "Ignoring wrong pointing object 1336 0 (offset 0)\n",
      "Ignoring wrong pointing object 1357 0 (offset 0)\n",
      "Ignoring wrong pointing object 1391 0 (offset 0)\n",
      "Ignoring wrong pointing object 1398 0 (offset 0)\n",
      "Ignoring wrong pointing object 1435 0 (offset 0)\n",
      "Ignoring wrong pointing object 1468 0 (offset 0)\n",
      "Ignoring wrong pointing object 1520 0 (offset 0)\n",
      "Ignoring wrong pointing object 1527 0 (offset 0)\n",
      "Ignoring wrong pointing object 1538 0 (offset 0)\n",
      "Ignoring wrong pointing object 1545 0 (offset 0)\n",
      "Ignoring wrong pointing object 1552 0 (offset 0)\n",
      "Ignoring wrong pointing object 1571 0 (offset 0)\n",
      "Ignoring wrong pointing object 1573 0 (offset 0)\n",
      "Ignoring wrong pointing object 1575 0 (offset 0)\n",
      "Ignoring wrong pointing object 1577 0 (offset 0)\n",
      "Ignoring wrong pointing object 1582 0 (offset 0)\n",
      "Ignoring wrong pointing object 1605 0 (offset 0)\n",
      "Ignoring wrong pointing object 1610 0 (offset 0)\n",
      "Ignoring wrong pointing object 1638 0 (offset 0)\n",
      "Ignoring wrong pointing object 1662 0 (offset 0)\n",
      "Ignoring wrong pointing object 1689 0 (offset 0)\n",
      "Ignoring wrong pointing object 1713 0 (offset 0)\n",
      "Ignoring wrong pointing object 1715 0 (offset 0)\n",
      "Ignoring wrong pointing object 1773 0 (offset 0)\n",
      "Ignoring wrong pointing object 1777 0 (offset 0)\n",
      "Ignoring wrong pointing object 1796 0 (offset 0)\n",
      "Ignoring wrong pointing object 1813 0 (offset 0)\n",
      "Ignoring wrong pointing object 1817 0 (offset 0)\n",
      "Ignoring wrong pointing object 1827 0 (offset 0)\n",
      "Ignoring wrong pointing object 1853 0 (offset 0)\n",
      "Ignoring wrong pointing object 1857 0 (offset 0)\n",
      "Ignoring wrong pointing object 1923 0 (offset 0)\n",
      "Ignoring wrong pointing object 2052 0 (offset 0)\n",
      "Ignoring wrong pointing object 2158 0 (offset 0)\n",
      "Ignoring wrong pointing object 2199 0 (offset 0)\n",
      "Ignoring wrong pointing object 2218 0 (offset 0)\n",
      "Ignoring wrong pointing object 2222 0 (offset 0)\n",
      "Ignoring wrong pointing object 2244 0 (offset 0)\n",
      "Ignoring wrong pointing object 2267 0 (offset 0)\n",
      "Ignoring wrong pointing object 2293 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[475/552]  - Strikepoint Gold (https://strikepointgold.com)\n",
      "\n",
      "[476/552]  - Sun Peak Metals (https://sunpeakmetals.com)\n",
      "\n",
      "[478/552]  - Surge Battery Metals (https://surgebatterymetals.com)\n",
      "\n",
      "[480/552]  - T2 Metals (https://t2metals.com)\n",
      "\n",
      "[489/552]  - Tek84 Inc. (https://tekincogroup.com)\n",
      "\n",
      "[497/552]  - The Metals Company (https://metals.co)\n",
      "\n",
      "[499/552]  - Thunder Gold Corp. (https://thundergoldcorp.com)\n",
      "\n",
      "[501/552]  - Transition Metals Corp. (https://transitionmetalscorp.com)\n",
      "\n",
      "[504/552]  - Triumph Gold Corp. (https://triumphgoldcorp.com)\n",
      "\n",
      "[507/552]  - Trade Inc. (https://tradein.com)\n",
      "\n",
      "[510/552]  - LLC (https://garant.ru)\n",
      "\n",
      "[518/552]  - Zijin Mining Group (https://zjky.cn)\n",
      "\n",
      "[523/552]  - Canada Inc. (https://canadaincredible.com)\n",
      "\n",
      "[533/552]  - Canada Inc. (https://canadaincredible.com)\n",
      "\n",
      "[539/552]  - Viva Gold Corp. (https://vivagoldcorp.com)\n",
      "\n",
      "[540/552]  - Vizsla Silver Corp. (https://vizslasilvercorp.com)\n",
      "\n",
      "[546/552]  - Western Exploration (https://westernexploration.com)\n",
      "\n",
      "[547/552]  - Vanadium Corp. (https://vanadiumcorp.com)\n",
      "\n",
      "[551/552]  - White Gold Corp. (https://whitegoldcorp.ca)\n",
      "\n",
      "Wrote: C:\\path\\to\\pdac_company_commodities.csv\n",
      "Wrote: C:\\path\\to\\pdac_summary_stats.json\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# PDAC: Companies enrichment + commodities/jurisdiction summariser\n",
    "# - Input: PDAC_2026_Exhibitors_all_with_classification.csv (all entries, companies flagged)\n",
    "# - Output (1): pdac_company_enriched.csv (companies only: Name, Exchange_Ticker, Website, Capacity, Classification)\n",
    "# - Output (2): pdac_company_commodities.csv (companies only + top commodities/jurisdictions + evidence)\n",
    "# - Output (3): pdac_summary_stats.json (aggregate counts)\n",
    "# =========================\n",
    "\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import hashlib\n",
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# -------- PDF text extraction --------\n",
    "PDF_OK = False\n",
    "try:\n",
    "    from pypdf import PdfReader\n",
    "    PDF_OK = True\n",
    "except Exception:\n",
    "    PDF_OK = False\n",
    "\n",
    "# -------------------------\n",
    "# USER SETTINGS\n",
    "# -------------------------\n",
    "INPUT_CSV = r\"C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULTING PTY LTD\\Documents\\01_BD\\95_2026_PDAC\\PDAC_2026_Exhibitors_all_with_classification.csv\"\n",
    "\n",
    "OUT_COMPANY_ENRICHED = r\"C:\\path\\to\\pdac_company_enriched.csv\"\n",
    "OUT_COMPANY_COMMS    = r\"C:\\path\\to\\pdac_company_commodities.csv\"\n",
    "OUT_SUMMARY_JSON     = r\"C:\\path\\to\\pdac_summary_stats.json\"\n",
    "\n",
    "DOWNLOAD_DIR = Path(r\"C:\\path\\to\\pdac_downloaded_pdfs\")\n",
    "DEBUG_DIR    = Path(r\"C:\\path\\to\\pdac_debug_pages\")\n",
    "\n",
    "ONLY_TICKERS = None  # e.g. [\"BHP.AX\"] for testing; else None\n",
    "\n",
    "REQUEST_TIMEOUT = 25\n",
    "SLEEP_S = 0.6\n",
    "MAX_PAGES_TO_VISIT = 25\n",
    "MAX_SITEMAP_URLS = 4000\n",
    "MAX_SITEMAP_CHILDREN = 25\n",
    "MAX_PDFS_PER_COMPANY = 6\n",
    "\n",
    "VERBOSE = True\n",
    "USER_AGENT = (\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "    \"(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# COMMODITY + JURISDICTION KEYWORDS\n",
    "# -------------------------\n",
    "COMMODITY_KW = {\n",
    "    \"Gold\":      [r\"\\bgold\\b\", r\"\\bau\\b\"],\n",
    "    \"Copper\":    [r\"\\bcopper\\b\", r\"\\bcu\\b\"],\n",
    "    \"Nickel\":    [r\"\\bnickel\\b\", r\"\\bni\\b\"],\n",
    "    \"Lithium\":   [r\"\\blithium\\b\", r\"\\bli\\b\", r\"\\bspodumene\\b\"],\n",
    "    \"Uranium\":   [r\"\\buranium\\b\", r\"\\bu3o8\\b\"],\n",
    "    \"Silver\":    [r\"\\bsilver\\b\", r\"\\bag\\b\"],\n",
    "    \"Zinc\":      [r\"\\bzinc\\b\", r\"\\bzn\\b\"],\n",
    "    \"Lead\":      [r\"\\blead\\b\", r\"\\bpb\\b\"],\n",
    "    \"REE\":       [r\"\\brare earth\", r\"\\bree\\b\", r\"\\bndpr\\b\", r\"\\bneodymium\\b\", r\"\\bpraseodymium\\b\"],\n",
    "    \"PGEs\":      [r\"\\bplatinum\\b\", r\"\\bpalladium\\b\", r\"\\bpge\\b\"],\n",
    "    \"Graphite\":  [r\"\\bgraphite\\b\"],\n",
    "    \"Tin\":       [r\"\\btin\\b\", r\"\\bsn\\b\"],\n",
    "    \"Cobalt\":    [r\"\\bcobalt\\b\", r\"\\bco\\b\"],\n",
    "    \"Manganese\": [r\"\\bmanganese\\b\", r\"\\bmn\\b\"],\n",
    "    \"Iron Ore\":  [r\"\\biron ore\\b\", r\"\\bhematite\\b\", r\"\\bmagnetite\\b\"],\n",
    "    \"Potash\":    [r\"\\bpotash\\b\"],\n",
    "}\n",
    "\n",
    "# Keep it simple and useful: country/state mentions\n",
    "JURISDICTION_KW = {\n",
    "    \"Canada\":   [r\"\\bcanada\\b\", r\"\\bontario\\b\", r\"\\bquebec\\b\", r\"\\bbritish columbia\\b\", r\"\\balberta\\b\", r\"\\bnunavut\\b\", r\"\\byukon\\b\"],\n",
    "    \"USA\":      [r\"\\busa\\b\", r\"\\bunited states\\b\", r\"\\bnevada\\b\", r\"\\balaska\\b\", r\"\\barizona\\b\"],\n",
    "    \"Australia\":[r\"\\baustralia\\b\", r\"\\bwestern australia\\b|\\bwa\\b\", r\"\\bqueensland\\b|\\bqld\\b\", r\"\\bnsw\\b|\\bnew south wales\\b\", r\"\\bsa\\b|\\bsouth australia\\b\", r\"\\bnt\\b|\\bnorthern territory\\b\", r\"\\btas\\b|\\btasmania\\b\", r\"\\bvic\\b|\\bvictoria\\b\"],\n",
    "    \"Chile\":    [r\"\\bchile\\b\"],\n",
    "    \"Peru\":     [r\"\\bperu\\b\"],\n",
    "    \"Brazil\":   [r\"\\bbrazil\\b\"],\n",
    "    \"Argentina\":[r\"\\bargentina\\b\"],\n",
    "    \"Mexico\":   [r\"\\bmexico\\b\"],\n",
    "    \"Africa\":   [r\"\\bafrica\\b\", r\"\\bnamibia\\b\", r\"\\bbotswana\\b\", r\"\\bghana\\b\", r\"\\bsouth africa\\b\", r\"\\bdr congo\\b|\\bdrc\\b\", r\"\\bzambia\\b\"],\n",
    "}\n",
    "\n",
    "PDF_EXT_RE = re.compile(r\"\\.pdf(\\?|$)\", re.IGNORECASE)\n",
    "\n",
    "REPORT_URL_HINT = re.compile(\n",
    "    r\"(investor|investors|asx|tsx|nyse|announce|announcement|release|news|media|report|results|financial|presentation|quarter|appendix|4c|4e|sedar)\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# HELPERS\n",
    "# -------------------------\n",
    "def log(msg: str):\n",
    "    if VERBOSE:\n",
    "        print(msg)\n",
    "\n",
    "def normalize_site(site: str) -> str:\n",
    "    site = (site or \"\").strip()\n",
    "    if not site:\n",
    "        return \"\"\n",
    "    if not site.startswith((\"http://\", \"https://\")):\n",
    "        site = \"https://\" + site\n",
    "    return site.rstrip(\"/\")\n",
    "\n",
    "def short_hash(s: str) -> str:\n",
    "    return hashlib.md5(s.encode(\"utf-8\", errors=\"ignore\")).hexdigest()[:10]\n",
    "\n",
    "def safe_filename(s: str) -> str:\n",
    "    s = re.sub(r\"[^a-zA-Z0-9._-]+\", \"_\", s)\n",
    "    return s[:120]\n",
    "\n",
    "def session() -> requests.Session:\n",
    "    s = requests.Session()\n",
    "    s.headers.update({\"User-Agent\": USER_AGENT, \"Accept-Language\": \"en-US,en;q=0.9\"})\n",
    "    return s\n",
    "\n",
    "def fetch_text(sess: requests.Session, url: str):\n",
    "    try:\n",
    "        r = sess.get(url, timeout=REQUEST_TIMEOUT, allow_redirects=True)\n",
    "        ctype = (r.headers.get(\"content-type\") or \"\").lower()\n",
    "        return r.status_code, ctype, (r.text or \"\")\n",
    "    except Exception:\n",
    "        return 0, \"\", \"\"\n",
    "\n",
    "def looks_like_pdf_bytes(first_bytes: bytes) -> bool:\n",
    "    return first_bytes.startswith(b\"%PDF-\")\n",
    "\n",
    "def download_pdf(sess: requests.Session, url: str, outpath: Path):\n",
    "    try:\n",
    "        r = sess.get(url, timeout=REQUEST_TIMEOUT, stream=True, allow_redirects=True)\n",
    "        if r.status_code >= 400:\n",
    "            return False, f\"http_{r.status_code}\"\n",
    "\n",
    "        ctype = (r.headers.get(\"content-type\") or \"\").lower()\n",
    "\n",
    "        it = r.iter_content(chunk_size=1024 * 64)\n",
    "        first = next(it, b\"\") or b\"\"\n",
    "\n",
    "        is_pdf = (\"pdf\" in ctype) or PDF_EXT_RE.search(url) or looks_like_pdf_bytes(first)\n",
    "        if not is_pdf:\n",
    "            return False, f\"not_pdf_ctype={ctype[:40]}\"\n",
    "\n",
    "        outpath.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(outpath, \"wb\") as f:\n",
    "            f.write(first)\n",
    "            for chunk in it:\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "\n",
    "        if outpath.stat().st_size < 10_000:\n",
    "            return False, \"download_too_small\"\n",
    "        return True, \"ok\"\n",
    "    except Exception as e:\n",
    "        return False, f\"exception_{type(e).__name__}\"\n",
    "\n",
    "def extract_pdf_text(pdf_path: Path) -> str:\n",
    "    if not PDF_OK:\n",
    "        return \"\"\n",
    "    try:\n",
    "        reader = PdfReader(str(pdf_path))\n",
    "        parts = []\n",
    "        for p in reader.pages[:40]:  # cap to reduce time\n",
    "            t = p.extract_text() or \"\"\n",
    "            if t:\n",
    "                parts.append(t)\n",
    "        return \"\\n\".join(parts)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def extract_pdf_links_from_html(base_url: str, html: str):\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    out = []\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = (a.get(\"href\") or \"\").strip()\n",
    "        if not href:\n",
    "            continue\n",
    "        absu = urljoin(base_url, href)\n",
    "        label = \" \".join(a.get_text(\" \", strip=True).split())\n",
    "        if PDF_EXT_RE.search(absu) or (\"pdf\" in absu.lower() and (\"download\" in absu.lower() or \"uploads\" in absu.lower() or \"media\" in absu.lower())):\n",
    "            out.append({\"url\": absu, \"label\": label})\n",
    "    return out\n",
    "\n",
    "def get_sitemap_urls(sess: requests.Session, site: str):\n",
    "    starts = [\"/sitemap.xml\", \"/sitemap_index.xml\", \"/wp-sitemap.xml\"]\n",
    "    urls = []\n",
    "    for p in starts:\n",
    "        sm = site + p\n",
    "        st, ct, txt = fetch_text(sess, sm)\n",
    "        time.sleep(SLEEP_S)\n",
    "        if st >= 400 or not txt or \"xml\" not in ct:\n",
    "            continue\n",
    "        soup = BeautifulSoup(txt, \"xml\")\n",
    "        # sitemap index?\n",
    "        smaps = soup.find_all(\"sitemap\")\n",
    "        if smaps:\n",
    "            for smap in smaps[:MAX_SITEMAP_CHILDREN]:\n",
    "                loc = smap.find(\"loc\")\n",
    "                if not loc:\n",
    "                    continue\n",
    "                child = loc.get_text(strip=True)\n",
    "                st2, ct2, t2 = fetch_text(sess, child)\n",
    "                time.sleep(SLEEP_S)\n",
    "                if st2 >= 400 or not t2 or \"xml\" not in ct2:\n",
    "                    continue\n",
    "                s2 = BeautifulSoup(t2, \"xml\")\n",
    "                for loc2 in s2.find_all(\"loc\"):\n",
    "                    u = loc2.get_text(strip=True)\n",
    "                    if REPORT_URL_HINT.search(u):\n",
    "                        urls.append(u)\n",
    "                    if len(urls) >= MAX_SITEMAP_URLS:\n",
    "                        return urls[:MAX_SITEMAP_URLS]\n",
    "        else:\n",
    "            for loc in soup.find_all(\"loc\"):\n",
    "                u = loc.get_text(strip=True)\n",
    "                if REPORT_URL_HINT.search(u):\n",
    "                    urls.append(u)\n",
    "            if urls:\n",
    "                return urls[:MAX_SITEMAP_URLS]\n",
    "    return urls[:MAX_SITEMAP_URLS]\n",
    "\n",
    "def discover_report_pages(site: str):\n",
    "    paths = [\n",
    "        \"/investors\", \"/investor\", \"/investor-centre\",\n",
    "        \"/asx-announcements\", \"/announcements\", \"/news\", \"/media\",\n",
    "        \"/reports\", \"/financial-reports\", \"/presentations\", \"/results\"\n",
    "    ]\n",
    "    return [site + p for p in paths]\n",
    "\n",
    "def pick_pdf_candidates(sess: requests.Session, site: str, debug_dir: Path):\n",
    "    debug_dir.mkdir(parents=True, exist_ok=True)\n",
    "    notes = []\n",
    "    pdfs = []\n",
    "\n",
    "    seed_pages = get_sitemap_urls(sess, site)\n",
    "    if seed_pages:\n",
    "        notes.append(f\"sitemap_filtered_pages={len(seed_pages)}\")\n",
    "    else:\n",
    "        notes.append(\"no_sitemap_filtered_pages\")\n",
    "        seed_pages = discover_report_pages(site)\n",
    "\n",
    "    visited = set()\n",
    "    queue = seed_pages[:80]\n",
    "\n",
    "    while queue and len(visited) < MAX_PAGES_TO_VISIT:\n",
    "        url = queue.pop(0)\n",
    "        if url in visited:\n",
    "            continue\n",
    "        visited.add(url)\n",
    "\n",
    "        st, ct, html = fetch_text(sess, url)\n",
    "        time.sleep(SLEEP_S)\n",
    "        if st >= 400 or not html:\n",
    "            continue\n",
    "\n",
    "        dbg = debug_dir / f\"{len(visited):02d}_{short_hash(url)}_{safe_filename(urlparse(url).path)}.html\"\n",
    "        dbg.write_text(html, encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "        pdfs.extend(extract_pdf_links_from_html(url, html))\n",
    "\n",
    "        # expand a bit\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        added = 0\n",
    "        for a in soup.find_all(\"a\", href=True):\n",
    "            href = (a.get(\"href\") or \"\").strip()\n",
    "            if not href:\n",
    "                continue\n",
    "            absu = urljoin(url, href)\n",
    "            if absu in visited:\n",
    "                continue\n",
    "            if REPORT_URL_HINT.search(absu):\n",
    "                queue.append(absu)\n",
    "                added += 1\n",
    "                if added >= 25:\n",
    "                    break\n",
    "\n",
    "    # dedupe\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for p in pdfs:\n",
    "        u = p.get(\"url\")\n",
    "        if not u or u in seen:\n",
    "            continue\n",
    "        seen.add(u)\n",
    "        out.append(p)\n",
    "\n",
    "    notes += [f\"visited_pages={len(visited)}\", f\"pdf_candidates={len(out)}\"]\n",
    "    # prioritize likely investor docs\n",
    "    out.sort(key=lambda x: 0 if REPORT_URL_HINT.search((x.get(\"label\",\"\")+x.get(\"url\",\"\"))) else 1)\n",
    "    return out[:MAX_PDFS_PER_COMPANY], notes\n",
    "\n",
    "def score_keywords(text: str, kw_dict: dict):\n",
    "    scores = {}\n",
    "    evidence = {}\n",
    "    for key, patterns in kw_dict.items():\n",
    "        hits = 0\n",
    "        ev = None\n",
    "        for pat in patterns:\n",
    "            m = re.search(pat, text, flags=re.I)\n",
    "            if m:\n",
    "                hits += len(re.findall(pat, text, flags=re.I))\n",
    "                if ev is None:\n",
    "                    # take short window around first hit\n",
    "                    start = max(m.start() - 80, 0)\n",
    "                    end   = min(m.end() + 120, len(text))\n",
    "                    ev = re.sub(r\"\\s+\", \" \", text[start:end]).strip()\n",
    "        if hits > 0:\n",
    "            scores[key] = hits\n",
    "            evidence[key] = ev or \"\"\n",
    "    # top 3\n",
    "    top = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "    top_keys = [k for k, v in top]\n",
    "    return top_keys, evidence\n",
    "\n",
    "# --- Simple enrichment helpers (same style as earlier) ---\n",
    "def clearbit_domain(company_name: str) -> str:\n",
    "    url = \"https://autocomplete.clearbit.com/v1/companies/suggest\"\n",
    "    try:\n",
    "        r = requests.get(url, params={\"query\": company_name}, timeout=15)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        if not data:\n",
    "            return \"\"\n",
    "        d = data[0].get(\"domain\",\"\") or \"\"\n",
    "        if d and not d.startswith((\"http://\", \"https://\")):\n",
    "            return \"https://\" + d\n",
    "        return d\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def yahoo_best_symbol(company_name: str) -> str:\n",
    "    url = \"https://query2.finance.yahoo.com/v1/finance/search\"\n",
    "    try:\n",
    "        r = requests.get(url, params={\"q\": company_name, \"quotesCount\": 6, \"newsCount\": 0}, timeout=15)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        quotes = data.get(\"quotes\", []) or []\n",
    "        for q in quotes:\n",
    "            if q.get(\"quoteType\") == \"EQUITY\" and q.get(\"symbol\"):\n",
    "                return q[\"symbol\"]\n",
    "        if quotes and quotes[0].get(\"symbol\"):\n",
    "            return quotes[0][\"symbol\"]\n",
    "        return \"\"\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def run():\n",
    "    DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    DEBUG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df = pd.read_csv(INPUT_CSV)\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "    # keep all entries for reference, but operate on companies only\n",
    "    companies = df[df[\"Is_Company\"].astype(str).str.upper().eq(\"YES\")].copy()\n",
    "\n",
    "    # optionally filter by ticker list\n",
    "    if ONLY_TICKERS:\n",
    "        companies = companies[companies[\"Exchange_Ticker\"].fillna(\"\").astype(str).isin(ONLY_TICKERS)].copy()\n",
    "\n",
    "    # --- Enrich missing Website / Exchange_Ticker ---\n",
    "    companies[\"Website\"] = companies[\"Website\"].fillna(\"\").astype(str)\n",
    "    companies[\"Exchange_Ticker\"] = companies[\"Exchange_Ticker\"].fillna(\"\").astype(str)\n",
    "\n",
    "    for i, row in companies.iterrows():\n",
    "        name = str(row[\"Name\"]).strip()\n",
    "\n",
    "        if not row[\"Website\"].strip():\n",
    "            w = clearbit_domain(name)\n",
    "            if w:\n",
    "                companies.at[i, \"Website\"] = w\n",
    "\n",
    "        if not row[\"Exchange_Ticker\"].strip():\n",
    "            t = yahoo_best_symbol(name)\n",
    "            if t:\n",
    "                companies.at[i, \"Exchange_Ticker\"] = t\n",
    "\n",
    "        if VERBOSE and (i % 25 == 0):\n",
    "            log(f\"enriched {i}/{len(companies)}\")\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    companies.to_csv(OUT_COMPANY_ENRICHED, index=False, encoding=\"utf-8\")\n",
    "    log(f\"Wrote: {OUT_COMPANY_ENRICHED}\")\n",
    "\n",
    "    # --- Commodity/Jurisdiction inference from PDFs ---\n",
    "    sess = session()\n",
    "    out_rows = []\n",
    "    commodity_totals = {}\n",
    "    jurisdiction_totals = {}\n",
    "\n",
    "    for idx, r in companies.iterrows():\n",
    "        name = str(r[\"Name\"]).strip()\n",
    "        tkr  = str(r[\"Exchange_Ticker\"]).strip()\n",
    "        site = normalize_site(str(r[\"Website\"]).strip())\n",
    "        if not site:\n",
    "            out_rows.append({\n",
    "                \"Name\": name, \"Exchange_Ticker\": tkr, \"Website\": \"\",\n",
    "                \"Top_Commodities\": \"\", \"Top_Jurisdictions\": \"\",\n",
    "                \"Evidence\": \"\", \"Notes\": \"no_website\"\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        log(f\"\\n[{len(out_rows)+1}/{len(companies)}] {tkr} - {name} ({site})\")\n",
    "\n",
    "        dbg_dir = DEBUG_DIR / (tkr or short_hash(name))\n",
    "        pdf_list, notes = pick_pdf_candidates(sess, site, dbg_dir)\n",
    "\n",
    "        combined_text = \"\"\n",
    "        downloaded = 0\n",
    "        for p in pdf_list:\n",
    "            url = p.get(\"url\",\"\")\n",
    "            if not url:\n",
    "                continue\n",
    "            outp = DOWNLOAD_DIR / (tkr or short_hash(name)) / f\"{safe_filename(urlparse(url).path)}_{short_hash(url)}.pdf\"\n",
    "            ok, why = download_pdf(sess, url, outp)\n",
    "            time.sleep(SLEEP_S)\n",
    "            if ok:\n",
    "                downloaded += 1\n",
    "                combined_text += \"\\n\" + extract_pdf_text(outp)\n",
    "            else:\n",
    "                notes.append(f\"pdf_fail({why})\")\n",
    "\n",
    "        if not combined_text.strip():\n",
    "            out_rows.append({\n",
    "                \"Name\": name, \"Exchange_Ticker\": tkr, \"Website\": site,\n",
    "                \"Top_Commodities\": \"\", \"Top_Jurisdictions\": \"\",\n",
    "                \"Evidence\": \"\", \"Notes\": \"; \".join(notes + [\"no_pdf_text\"])\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        top_comms, comm_ev = score_keywords(combined_text, COMMODITY_KW)\n",
    "        top_jurs, jur_ev   = score_keywords(combined_text, JURISDICTION_KW)\n",
    "\n",
    "        for c in top_comms:\n",
    "            commodity_totals[c] = commodity_totals.get(c, 0) + 1\n",
    "        for j in top_jurs:\n",
    "            jurisdiction_totals[j] = jurisdiction_totals.get(j, 0) + 1\n",
    "\n",
    "        # build short evidence string\n",
    "        ev_parts = []\n",
    "        for c in top_comms[:2]:\n",
    "            if comm_ev.get(c):\n",
    "                ev_parts.append(f\"[{c}] {comm_ev[c][:220]}\")\n",
    "        for j in top_jurs[:1]:\n",
    "            if jur_ev.get(j):\n",
    "                ev_parts.append(f\"[{j}] {jur_ev[j][:220]}\")\n",
    "        evidence = \" | \".join(ev_parts)\n",
    "\n",
    "        out_rows.append({\n",
    "            \"Name\": name,\n",
    "            \"Exchange_Ticker\": tkr,\n",
    "            \"Website\": site,\n",
    "            \"Capacity\": r.get(\"Capacity\",\"\"),\n",
    "            \"Classification\": r.get(\"Classification\",\"\"),\n",
    "            \"Top_Commodities\": \", \".join(top_comms),\n",
    "            \"Top_Jurisdictions\": \", \".join(top_jurs),\n",
    "            \"Evidence\": evidence,\n",
    "            \"Notes\": \"; \".join(notes + [f\"pdf_downloaded={downloaded}\"])\n",
    "        })\n",
    "\n",
    "    out_df = pd.DataFrame(out_rows)\n",
    "    out_df.to_csv(OUT_COMPANY_COMMS, index=False, encoding=\"utf-8\")\n",
    "    log(f\"\\nWrote: {OUT_COMPANY_COMMS}\")\n",
    "\n",
    "    summary = {\n",
    "        \"companies_total\": int(len(companies)),\n",
    "        \"companies_with_website\": int((companies[\"Website\"].astype(str).str.len() > 0).sum()),\n",
    "        \"companies_with_ticker\": int((companies[\"Exchange_Ticker\"].astype(str).str.len() > 0).sum()),\n",
    "        \"commodity_totals_top\": dict(sorted(commodity_totals.items(), key=lambda x: x[1], reverse=True)[:15]),\n",
    "        \"jurisdiction_totals_top\": dict(sorted(jurisdiction_totals.items(), key=lambda x: x[1], reverse=True)[:15]),\n",
    "    }\n",
    "    Path(OUT_SUMMARY_JSON).write_text(json.dumps(summary, indent=2), encoding=\"utf-8\")\n",
    "    log(f\"Wrote: {OUT_SUMMARY_JSON}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344cf64e-fe05-4059-875a-d8652d1442d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py311)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
