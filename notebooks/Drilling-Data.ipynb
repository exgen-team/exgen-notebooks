{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00c2eceb-ee55-487f-9d06-1c012ab1e4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Loaded geochem workbook → (382408, 63)\n",
      "[Info] Loaded locations workbook → (53960, 59)\n",
      "[Saved] Cleaned and merged CSV → C:\\Users\\Julian.Diaz\\OneDrive - XENITH CONSULTING PTY LTD\\Documents\\05_Geodatabases\\05_Excel\\NW-East_Isa\\NW-East_Isa_Geochem_Drilling_Samples_cleaned.csv\n",
      "[Summary] Rows with ≥1 coordinate value: 382300 / 382408\n",
      "[Summary] Rows with New_Hole_ID not found in location file: 108\n"
     ]
    }
   ],
   "source": [
    "# JUPYTER NOTEBOOK CELL — Clean Excel, merge coords, drop columns, save CSV\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# =========================\n",
    "# Paths / Settings\n",
    "# =========================\n",
    "FOLDER = r\"C:\\Users\\Julian.Diaz\\OneDrive - XENITH CONSULTING PTY LTD\\Documents\\05_Geodatabases\\05_Excel\"\n",
    "GEOCHEM_FILE  = os.path.join(FOLDER, \"NW-East_QLD_Geochem_Drilling_Samples.xlsx\")\n",
    "LOCATION_FILE = os.path.join(FOLDER, \"NW-East_QLD_Geochem_Drilling_Samples_Loc.xlsx\")\n",
    "\n",
    "# Output folder + filenames (CSV output)\n",
    "OUTDIR = r\"C:\\Users\\Julian.Diaz\\OneDrive - XENITH CONSULTING PTY LTD\\Documents\\05_Geodatabases\\05_Excel\\NW-East_Isa\"\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "OUTPUT_CSV  = os.path.join(OUTDIR, \"NW-East_Isa_Geochem_Drilling_Samples_cleaned.csv\")\n",
    "BACKUP_CSV  = os.path.join(OUTDIR, \"NW-East_Isa_Geochem_Drilling_Samples_cleaned.backup.csv\")\n",
    "\n",
    "# Sheet selection (change if needed)\n",
    "GEO_SHEET = 0\n",
    "LOC_SHEET = 0\n",
    "\n",
    "# Coordinate overwrite rule\n",
    "OVERWRITE_EXISTING_COORDS = False\n",
    "\n",
    "# Columns to convert to numbers + apply negative rules\n",
    "NUMERIC_COLUMNS = [\n",
    "    \"Au\",\"Au1\",\"Au2\",\"Au3\",\"Au4\",\"Cu\",\"Pb\",\"Zn\",\"Ag\",\"As\",\"Bi\",\"Mo\",\"Mn\",\"Fe\",\"Ni\",\"Co\",\"Cr\",\n",
    "    \"V\",\"Ba\",\"Cd\",\"Sn\",\"Sb\",\"Hg\",\"Te\",\"P\",\"W\",\"Zr\",\"Ti\",\"Mg\",\"Th\",\"U\",\"Pt\",\"Pd\",\"S\",\"F\"\n",
    "]\n",
    "\n",
    "# Coordinate columns expected in LOCATION_FILE\n",
    "COORD_COLS = [\"N_GDA2020z\", \"E_GDA2020z\", \"Latitude_GDA2020\", \"Longitude_GDA2020\"]\n",
    "\n",
    "# Columns to drop from final output\n",
    "DROP_COLS = [\n",
    "    \"Job_No\",\"HasParam\",\"HasTextParams\",\"Duplicate_Flag\",\"Preferred_Interval\",\n",
    "    \"Associated_Sample_Number\",\"Pref_Au_Assay_Field\",\"Wet_Dry_Flag\",\"TagCharacter\",\n",
    "    \"Collar_ID\",\"Sample_ID\",\"Load_Number\",\"Date_Updated\",\"User_Updating\",\"Update_Status\"\n",
    "]\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def normalize_number_string(s: str) -> str:\n",
    "    \"\"\"Normalize number-like text for robust numeric parsing (preserves decimals).\"\"\"\n",
    "    if s is None or (isinstance(s, float) and np.isnan(s)):\n",
    "        return \"\"\n",
    "    if not isinstance(s, str):\n",
    "        s = str(s)\n",
    "    s = s.strip()\n",
    "    s = s.replace(\"%\", \"\").replace(\"−\", \"-\")\n",
    "    if \",\" in s and \".\" not in s:\n",
    "        s = s.replace(\",\", \".\")\n",
    "    s = re.sub(r\"[^0-9.\\-]+\", \"\", s)\n",
    "    if s.count(\".\") > 1:\n",
    "        first = s.find(\".\")\n",
    "        s = s[: first + 1] + s[first + 1:].replace(\".\", \"\")\n",
    "    return s\n",
    "\n",
    "def to_numeric(series: pd.Series) -> pd.Series:\n",
    "    return pd.to_numeric(series.map(normalize_number_string), errors=\"coerce\")\n",
    "\n",
    "def apply_negative_rules_to_series(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"< -99 → NaN; other negatives → abs(value)/2\"\"\"\n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    s = s.mask(s < -99, np.nan)\n",
    "    s = s.mask((s < 0) & (s >= -99), abs(s) / 2)\n",
    "    return s\n",
    "\n",
    "def make_new_hole_id(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    New_Hole_ID = Prospect_Code + '_' + Company + '_' + Sheet_Number + '_' + (Hole_ID or Sheet_Number)\n",
    "    \"\"\"\n",
    "    for col in [\"Prospect_Code\", \"Company\", \"Sheet_Number\"]:\n",
    "        if col not in df.columns:\n",
    "            print(f\"[Warn] Missing column '{col}', using blanks.\")\n",
    "            df[col] = \"\"\n",
    "        df[col] = df[col].fillna(\"\").astype(str).str.strip()\n",
    "\n",
    "    tail = df[\"Hole_ID\"].fillna(\"\").astype(str).str.strip() if \"Hole_ID\" in df.columns else df[\"Sheet_Number\"]\n",
    "    df[\"New_Hole_ID\"] = (\n",
    "        df[\"Prospect_Code\"] + \"_\" + df[\"Company\"] + \"_\" + df[\"Sheet_Number\"] + \"_\" + tail\n",
    "    ).str.replace(r\"\\s+\", \"_\", regex=True).str.replace(r\"_+\", \"_\", regex=True).str.strip(\"_\")\n",
    "    return df\n",
    "\n",
    "def numeric_or_nan(series: pd.Series) -> pd.Series:\n",
    "    return pd.to_numeric(series, errors=\"coerce\")\n",
    "\n",
    "def drop_columns_if_present(df: pd.DataFrame, cols: list[str]) -> tuple[pd.DataFrame, list[str]]:\n",
    "    present = [c for c in cols if c in df.columns]\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if present:\n",
    "        df = df.drop(columns=present)\n",
    "    return df, missing\n",
    "\n",
    "# =========================\n",
    "# 1) Load geochem Excel, build New_Hole_ID, clean numeric columns\n",
    "# =========================\n",
    "df_geo = pd.read_excel(GEOCHEM_FILE, sheet_name=GEO_SHEET, dtype=str)\n",
    "print(f\"[Info] Loaded geochem workbook → {df_geo.shape}\")\n",
    "\n",
    "df_geo = make_new_hole_id(df_geo)\n",
    "\n",
    "missing_cols = [c for c in NUMERIC_COLUMNS if c not in df_geo.columns]\n",
    "for col in NUMERIC_COLUMNS:\n",
    "    if col in df_geo.columns:\n",
    "        df_geo[col] = to_numeric(df_geo[col])\n",
    "        df_geo[col] = apply_negative_rules_to_series(df_geo[col])\n",
    "    else:\n",
    "        print(f\"[Warn] Geochem: column '{col}' not found; skipped.\")\n",
    "\n",
    "if missing_cols:\n",
    "    print(f\"[Note] Geochem missing numeric columns (skipped): {missing_cols}\")\n",
    "\n",
    "# =========================\n",
    "# 2) Load location Excel, build New_Hole_ID, prep coordinates\n",
    "# =========================\n",
    "df_loc = pd.read_excel(LOCATION_FILE, sheet_name=LOC_SHEET, dtype=str)\n",
    "df_loc.columns = [c.strip() for c in df_loc.columns]\n",
    "print(f\"[Info] Loaded locations workbook → {df_loc.shape}\")\n",
    "\n",
    "df_loc = make_new_hole_id(df_loc)\n",
    "\n",
    "missing_coords = [c for c in COORD_COLS if c not in df_loc.columns]\n",
    "if missing_coords:\n",
    "    print(f\"[Warn] Missing coordinate columns in location file: {missing_coords}\")\n",
    "\n",
    "present_coords = [c for c in COORD_COLS if c in df_loc.columns]\n",
    "coords = df_loc[[\"New_Hole_ID\"] + present_coords].copy()\n",
    "\n",
    "if present_coords:\n",
    "    # Deduplicate by New_Hole_ID, prefer rows with more filled coords\n",
    "    coords[\"_nn\"] = coords[present_coords].notna().sum(axis=1)\n",
    "    coords = coords.sort_values([\"New_Hole_ID\", \"_nn\"], ascending=[True, False])\n",
    "    coords = coords.drop_duplicates(subset=[\"New_Hole_ID\"], keep=\"first\").drop(columns=\"_nn\")\n",
    "\n",
    "    for c in present_coords:\n",
    "        coords[c] = numeric_or_nan(coords[c])\n",
    "\n",
    "# =========================\n",
    "# 3) Merge coordinates into geochem (left join on New_Hole_ID)\n",
    "# =========================\n",
    "if present_coords:\n",
    "    df_merged = df_geo.merge(coords, on=\"New_Hole_ID\", how=\"left\", suffixes=(\"\", \"_loc\"))\n",
    "\n",
    "    for c in present_coords:\n",
    "        incoming = pd.to_numeric(df_merged[c], errors=\"coerce\")\n",
    "        if c in df_geo.columns:\n",
    "            if OVERWRITE_EXISTING_COORDS:\n",
    "                df_geo[c] = incoming\n",
    "            else:\n",
    "                existing = pd.to_numeric(df_geo[c], errors=\"coerce\")\n",
    "                df_geo[c] = existing.where(existing.notna(), incoming)\n",
    "        else:\n",
    "            df_geo[c] = incoming\n",
    "\n",
    "# =========================\n",
    "# 4) Drop requested columns\n",
    "# =========================\n",
    "df_geo, missing_drop = drop_columns_if_present(df_geo, DROP_COLS)\n",
    "if missing_drop:\n",
    "    print(f\"[Note] Drop list columns not found (already absent): {missing_drop}\")\n",
    "\n",
    "# =========================\n",
    "# 5) Save CSV (backup if exists)\n",
    "# =========================\n",
    "if os.path.exists(OUTPUT_CSV):\n",
    "    if not os.path.exists(BACKUP_CSV):\n",
    "        prev = pd.read_csv(OUTPUT_CSV, low_memory=False)\n",
    "        prev.to_csv(BACKUP_CSV, index=False)\n",
    "        print(f\"[Backup created] {BACKUP_CSV}\")\n",
    "    else:\n",
    "        print(f\"[Backup exists] {BACKUP_CSV}\")\n",
    "\n",
    "df_geo.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"[Saved] Cleaned and merged CSV → {OUTPUT_CSV}\")\n",
    "\n",
    "# =========================\n",
    "# 6) Summary\n",
    "# =========================\n",
    "if present_coords:\n",
    "    matched = df_geo[present_coords].notna().any(axis=1)\n",
    "    print(f\"[Summary] Rows with ≥1 coordinate value: {int(matched.sum())} / {len(df_geo)}\")\n",
    "    unmatched = ~df_geo[\"New_Hole_ID\"].isin(coords[\"New_Hole_ID\"])\n",
    "    print(f\"[Summary] Rows with New_Hole_ID not found in location file: {int(unmatched.sum())}\")\n",
    "else:\n",
    "    print(\"[Summary] No coordinate columns found for merge.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646800bc-5438-40fc-b404-ab036dabb6fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py311] *",
   "language": "python",
   "name": "conda-env-py311-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
