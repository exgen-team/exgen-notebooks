{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fc3f72e-5f99-4bfa-b067-ac7b5d5d3557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/91] RMS - Ramelius Resources Ltd (rameliusresources.com.au)\n",
      "  - quarterly: https://www.rameliusresources.com.au/wp-content/uploads/bsk-pdf-manager/2026/01/3016456.pdf\n",
      "  - annual: https://www.rameliusresources.com.au/wp-content/uploads/bsk-pdf-manager/2025/10/2025-Annual-Report-1.pdf\n",
      "\n",
      "[2/91] FFM - FireFly Metals Ltd (fireflymetals.com.au)\n",
      "\n",
      "[3/91] CHN - Chalice Mining Ltd (chalicemining.com)\n",
      "  - quarterly: https://chalicemining.com/wp-content/uploads/2026/01/61309309.pdf\n",
      "  - annual: https://chalicemining.com/wp-content/uploads/2025/09/61286187-2.pdf\n",
      "\n",
      "[4/91] RXL - Rox Resources Ltd (roxresources.com.au)\n",
      "\n",
      "[5/91] NMG - New Murchison Gold Ltd (newmurchgold.com.au)\n",
      "\n",
      "[6/91] MEI - Meteoric Resources NL (meteoric.com.au)\n",
      "\n",
      "[7/91] SVL - Silver Mines Ltd (silvermines.com.au)\n",
      "  - quarterly: https://www.silvermines.com.au/wp-content/uploads/2025/11/06rb480zcsf5jd.pdf\n",
      "  - annual: https://www.silvermines.com.au/wp-content/uploads/2025/10/06pwx1cz93f4cj.pdf\n",
      "\n",
      "[8/91] STK - Strickland Metals Ltd (stricklandmetals.com.au)\n",
      "  - quarterly: https://stricklandmetals.com.au/wp-content/uploads/2024/12/6.-Canaccord-Genuity-Industry-Update-Metals-and-Mining.pdf\n",
      "  - annual: https://stricklandmetals.com.au/wp-content/uploads/2024/12/6.-Canaccord-Genuity-Industry-Update-Metals-and-Mining.pdf\n",
      "\n",
      "[9/91] AAR - Astral Resources NL (astralresources.com.au)\n",
      "\n",
      "[10/91] AZY - Antipa Minerals Ltd (antipaminerals.com.au)\n",
      "  - quarterly: https://antipaminerals.com.au/upload/documents/investors/asx-announcements/260128010802_26-01-28-AZY_Dec25QuarterlyActivitiesReport_FINAL.pdf\n",
      "  - annual: https://antipaminerals.com.au/upload/documents/investors/asx-announcements/201130020947_2020-03-121.pdf\n",
      "\n",
      "[11/91] MAU - Magnetic Resources NL (magres.com.au)\n",
      "  - quarterly: https://magres.com.au/wp-content/uploads/2026/02/CR_PK_MAU_20260204_75f4099177f540429a26b14d1fed82f7-1.pdf\n",
      "  - annual: https://magres.com.au/wp-content/uploads/2026/02/CR_PK_MAU_20260204_75f4099177f540429a26b14d1fed82f7-1.pdf\n",
      "\n",
      "[12/91] GG8 - Gorilla Gold Mines Ltd (gorillagold8.com)\n",
      "  - quarterly: https://gorillagold8.com/wp-content/uploads/2026/01/311025-Gorilla-Gold-Mines-Ltd-TCs.pdf\n",
      "  - annual: https://gorillagold8.com/wp-content/uploads/2026/01/311025-Gorilla-Gold-Mines-Ltd-TCs.pdf\n",
      "\n",
      "[13/91] BTR - Brightstar Resources Ltd (brightstarresources.com.au)\n",
      "  - quarterly: https://brightstarresources.com.au/wp-content/uploads/2025/04/20221221_WA-gold-results-boost-ambitions-for-Brightstar-flagship.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CAUTION: startxref found while searching for %%EOF. The file might be truncated and some data might not be read.\n",
      "EOF marker not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - annual: https://brightstarresources.com.au/wp-content/uploads/2025/04/20221221_WA-gold-results-boost-ambitions-for-Brightstar-flagship.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CAUTION: startxref found while searching for %%EOF. The file might be truncated and some data might not be read.\n",
      "EOF marker not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[14/91] WWI - West Wits Mining Ltd (westwitsmining.com)\n",
      "  - quarterly: http://westwitsmining.com/wp-content/uploads/2021/11/Quarterly-Report-July-Sep-2021.pdf\n",
      "  - annual: https://westwitsmining.com/wp-content/uploads/2026/02/WWI-Corporate-Presentation-February-2026.pdf\n",
      "\n",
      "[15/91] CTM - Centaurus Metals Ltd (centaurus.com.au)\n",
      "\n",
      "[16/91] MM8 - Medallion Metals Ltd (medallionmetals.com.au)\n",
      "  - quarterly: https://medallionmetals.com.au/wp-content/uploads/2025/09/Medallion-Metals-Group-Purchase-Order-TCs.pdf\n",
      "  - annual: https://medallionmetals.com.au/wp-content/uploads/2025/09/Medallion-Metals-Group-Purchase-Order-TCs.pdf\n",
      "\n",
      "[17/91] BGD - Barton Gold Holdings Ltd (bartongold.com.au)\n",
      "  - quarterly: https://bartongold.com.au/wp-content/uploads/BartonGoldProspectus14May21.pdf\n",
      "  - annual: https://bartongold.com.au/wp-content/uploads/BartonGoldProspectus14May21.pdf\n",
      "\n",
      "[18/91] RHI - Red Hill Minerals Ltd (redhillminerals.com.au)\n",
      "\n",
      "[19/91] HCH - Hot Chili Ltd (hotchili.net.au)\n",
      "  - quarterly: https://www.hotchili.net.au/UploadImages/announcements/20260204-Hot-Chili-Arranges-A$40-Million-Funding-41.pdf\n",
      "  - annual: https://www.hotchili.net.au/UploadImages/announcements/20260204-Hot-Chili-Arranges-A$40-Million-Funding-41.pdf\n",
      "\n",
      "[20/91] IVR - Investigator Silver Ltd (investres.com.au)\n",
      "\n",
      "[21/91] BM1 - Ballard Mining Ltd (ballardmining.com.au)\n",
      "\n",
      "[22/91] HRZ - Horizon Minerals Ltd (horizonminerals.com.au)\n",
      "\n",
      "[23/91] PLA - Pacific Lime & Cement Ltd (placltd.com)\n",
      "\n",
      "[24/91] AUE - Aurum Resources Ltd (aurumres.com.au)\n",
      "\n",
      "[25/91] RNU - Renascor Resources Ltd (renascor.com.au)\n",
      "  - quarterly: https://renascor.com.au/wp-content/uploads/2019/11/20191129-South-Australian-Exploration-and-Mining-Conference-2005596.pdf\n",
      "  - annual: https://renascor.com.au/wp-content/uploads/2019/11/20191129-South-Australian-Exploration-and-Mining-Conference-2005596.pdf\n",
      "\n",
      "[26/91] DLI - Delta Lithium Ltd (deltalithium.com.au)\n",
      "\n",
      "[27/91] GHM - Golden Horse Minerals Ltd (goldenhorseminerals.com)\n",
      "  - quarterly: https://goldenhorseminerals.com/wp-content/uploads/2025/11/20251119-GHM-CDI-Holder-Letter-2025-AGSM.pdf\n",
      "  - annual: https://goldenhorseminerals.com/wp-content/uploads/2025/11/20251119-GHM-CDI-Holder-Letter-2025-AGSM.pdf\n",
      "\n",
      "[28/91] WTM - Waratah Minerals Ltd (waratahminerals.com)\n",
      "  - quarterly: https://waratahminerals.com/wp-content/uploads/WTM-Securities-Trading-Policy-2025.pdf\n",
      "  - annual: https://waratahminerals.com/wp-content/uploads/WTM-Securities-Trading-Policy-2025.pdf\n",
      "\n",
      "[29/91] GBZ - GBM Resources Ltd (gbmr.com.au)\n",
      "\n",
      "[30/91] CNB - Carnaby Resources Ltd (carnabyresources.com.au)\n",
      "  - quarterly: https://carnabyresources.com.au/wp-content/uploads/TopHoldersReportJan26.pdf\n",
      "  - annual: https://carnabyresources.com.au/wp-content/uploads/TopHoldersReportJan26.pdf\n",
      "\n",
      "[31/91] HRN - Horizon Gold Ltd (horizongold.com.au)\n",
      "  - quarterly: https://horizongold.com.au/wp-content/uploads/2019/08/190802-HRN-MACEQUITY.pdf\n",
      "  - annual: https://horizongold.com.au/wp-content/uploads/2019/08/190802-HRN-MACEQUITY.pdf\n",
      "\n",
      "[32/91] DRE - Dreadnought Resources Ltd (dreadnoughtresources.com.au)\n",
      "\n",
      "[33/91] ARL - Ardea Resources Ltd (ardearesources.com.au)\n",
      "  - quarterly: https://ardearesources.com.au/downloads/reports/arl_qa202503.pdf\n",
      "  - annual: https://ardearesources.com.au/downloads/reports/arl_af2019.pdf\n",
      "\n",
      "[34/91] FAL - Falcon Metals Ltd (falconmetals.com.au)\n",
      "  - quarterly: https://www.falconmetals.com.au/wp-content/uploads/2026/01/2026.01.22-FAL_December-Quarterly-2025_Final-V1.pdf\n",
      "  - annual: https://www.falconmetals.com.au/wp-content/uploads/2025/09/61281756.pdf\n",
      "\n",
      "[35/91] AGE - Alligator Energy Ltd (alligatorenergy.com.au)\n",
      "\n",
      "[36/91] MMA - Maronan Metals Ltd (maronanmetals.com.au)\n",
      "  - quarterly: https://maronanmetals.com.au/images/asx/MMA_ASX_-_Mar_23_Quarterly_Report_final.pdf\n",
      "  - annual: https://maronanmetals.com.au/images/Annual Report June 2025.pdf\n",
      "\n",
      "[37/91] MAT - Matsa Resources Ltd (matsa.com.au)\n",
      "  - quarterly: https://www.matsa.com.au/wp-content/uploads/Matsa-Resources-IIR-Research-Note-Aug-2024.pdf\n",
      "  - annual: https://www.matsa.com.au/wp-content/uploads/Matsa-Resources-IIR-Research-Note-Aug-2024.pdf\n",
      "\n",
      "[38/91] HAS - Hastings Technology Metals Ltd (hastingstechmetals.com)\n",
      "  - quarterly: https://api.investi.com.au/api/announcements/has/72cf23f7-71b.pdf\n",
      "  - annual: https://api.investi.com.au/api/announcements/has/72cf23f7-71b.pdf\n",
      "\n",
      "[39/91] PC2 - PC Gold Ltd (pcgold.com.au)\n",
      "  - quarterly: https://pcgold.com.au/wp-content/uploads/2025/11/251126_PCG_AGM_Presentation.pdf\n",
      "  - annual: https://pcgold.com.au/wp-content/uploads/2025/11/251126_PCG_AGM_Presentation.pdf\n",
      "\n",
      "[40/91] DEV - DevEx Resources Ltd (devexresources.com.au)\n",
      "  - quarterly: https://www.devexresources.com.au/wp-content/uploads/2026/01/61307920.pdf\n",
      "  - annual: https://www.devexresources.com.au/wp-content/uploads/2025/09/61284182.pdf\n",
      "\n",
      "[41/91] KSN - Kingston Resources Ltd (kingstonresources.com.au)\n",
      "\n",
      "[42/91] TM1 - Terra Metals Ltd (terrametals.com.au)\n",
      "\n",
      "[43/91] PTR - PTR Minerals Ltd (ptrminerals.com.au)\n",
      "\n",
      "[44/91] LM8 - Lunnon Metals Ltd (lunnonmetals.com.au)\n",
      "\n",
      "[45/91] YRL - Yandal Resources Ltd (yandalresources.com.au)\n",
      "\n",
      "[46/91] BSX - Blackstone Minerals Ltd (blackstoneminerals.com.au)\n",
      "\n",
      "[47/91] FLG - Flagship Minerals Ltd (flagshipminerals.com)\n",
      "  - quarterly: https://flagshipminerals.com/wp-content/uploads/2025/01/02834157.pdf\n",
      "  - annual: https://wcsecure.weblink.com.au/pdf/FLG/03001772.pdf\n",
      "\n",
      "[48/91] COD - Coda Minerals Ltd (codaminerals.com)\n",
      "\n",
      "[49/91] CRS - Caprice Resources Ltd (capriceresources.com)\n",
      "  - quarterly: https://www.capriceresources.com/caprice/wp-content/uploads/TheMarketHeraldCapriceResourcesConfirmsREEsAtMukinbudinProjectExpandsLandholdingBy52Percent17Aug23.pdf#new_tab\n",
      "  - annual: https://www.capriceresources.com/caprice/wp-content/uploads/TheMarketHeraldCapriceResourcesConfirmsREEsAtMukinbudinProjectExpandsLandholdingBy52Percent17Aug23.pdf#new_tab\n",
      "\n",
      "[50/91] RDM - Red Metal Ltd (redmetal.com.au)\n",
      "  - quarterly: https://redmetal.com.au/wp-content/uploads/2022/08/RDM-June-2022-Quarterly-Report-and-Cashflow.pdf\n",
      "  - annual: https://redmetal.com.au/wp-content/uploads/2025/03/2863525.pdf\n",
      "\n",
      "[51/91] RTR - Rumble Resources Ltd (rumbleresources.com.au)\n",
      "\n",
      "[52/91] M79 - Mammoth Minerals Ltd (firetailresources.com.au)\n",
      "\n",
      "[53/91] BCA - Black Canyon Ltd (blackcanyon.com.au)\n",
      "\n",
      "[54/91] CXU - Cauldron Energy Ltd (cauldronenergy.com.au)\n",
      "  - quarterly: https://www.cauldronenergy.com.au/wp-content/uploads/2025/04/20250225_CXU_Investor-Pres_RIU-Explorers.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 26 0 (offset 0)\n",
      "Ignoring wrong pointing object 28 0 (offset 0)\n",
      "Ignoring wrong pointing object 40 0 (offset 0)\n",
      "Ignoring wrong pointing object 42 0 (offset 0)\n",
      "Ignoring wrong pointing object 61 0 (offset 0)\n",
      "Ignoring wrong pointing object 63 0 (offset 0)\n",
      "Ignoring wrong pointing object 75 0 (offset 0)\n",
      "Ignoring wrong pointing object 83 0 (offset 0)\n",
      "Ignoring wrong pointing object 85 0 (offset 0)\n",
      "Ignoring wrong pointing object 87 0 (offset 0)\n",
      "Ignoring wrong pointing object 89 0 (offset 0)\n",
      "Ignoring wrong pointing object 91 0 (offset 0)\n",
      "Ignoring wrong pointing object 98 0 (offset 0)\n",
      "Ignoring wrong pointing object 100 0 (offset 0)\n",
      "Ignoring wrong pointing object 107 0 (offset 0)\n",
      "Ignoring wrong pointing object 118 0 (offset 0)\n",
      "Ignoring wrong pointing object 122 0 (offset 0)\n",
      "Ignoring wrong pointing object 124 0 (offset 0)\n",
      "Ignoring wrong pointing object 139 0 (offset 0)\n",
      "Ignoring wrong pointing object 141 0 (offset 0)\n",
      "Ignoring wrong pointing object 151 0 (offset 0)\n",
      "Ignoring wrong pointing object 153 0 (offset 0)\n",
      "Ignoring wrong pointing object 162 0 (offset 0)\n",
      "Ignoring wrong pointing object 164 0 (offset 0)\n",
      "Ignoring wrong pointing object 189 0 (offset 0)\n",
      "Ignoring wrong pointing object 272 0 (offset 0)\n",
      "Ignoring wrong pointing object 274 0 (offset 0)\n",
      "Ignoring wrong pointing object 282 0 (offset 0)\n",
      "Ignoring wrong pointing object 284 0 (offset 0)\n",
      "Ignoring wrong pointing object 286 0 (offset 0)\n",
      "Ignoring wrong pointing object 290 0 (offset 0)\n",
      "Ignoring wrong pointing object 313 0 (offset 0)\n",
      "Ignoring wrong pointing object 320 0 (offset 0)\n",
      "Ignoring wrong pointing object 322 0 (offset 0)\n",
      "Ignoring wrong pointing object 324 0 (offset 0)\n",
      "Ignoring wrong pointing object 326 0 (offset 0)\n",
      "Ignoring wrong pointing object 328 0 (offset 0)\n",
      "Ignoring wrong pointing object 330 0 (offset 0)\n",
      "Ignoring wrong pointing object 332 0 (offset 0)\n",
      "Ignoring wrong pointing object 341 0 (offset 0)\n",
      "Ignoring wrong pointing object 343 0 (offset 0)\n",
      "Ignoring wrong pointing object 359 0 (offset 0)\n",
      "Ignoring wrong pointing object 361 0 (offset 0)\n",
      "Ignoring wrong pointing object 364 0 (offset 0)\n",
      "Ignoring wrong pointing object 366 0 (offset 0)\n",
      "Ignoring wrong pointing object 379 0 (offset 0)\n",
      "Ignoring wrong pointing object 386 0 (offset 0)\n",
      "Ignoring wrong pointing object 405 0 (offset 0)\n",
      "Ignoring wrong pointing object 407 0 (offset 0)\n",
      "Ignoring wrong pointing object 409 0 (offset 0)\n",
      "Ignoring wrong pointing object 411 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - annual: https://www.cauldronenergy.com.au/wp-content/uploads/2025/04/20250225_CXU_Investor-Pres_RIU-Explorers.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 26 0 (offset 0)\n",
      "Ignoring wrong pointing object 28 0 (offset 0)\n",
      "Ignoring wrong pointing object 40 0 (offset 0)\n",
      "Ignoring wrong pointing object 42 0 (offset 0)\n",
      "Ignoring wrong pointing object 61 0 (offset 0)\n",
      "Ignoring wrong pointing object 63 0 (offset 0)\n",
      "Ignoring wrong pointing object 75 0 (offset 0)\n",
      "Ignoring wrong pointing object 83 0 (offset 0)\n",
      "Ignoring wrong pointing object 85 0 (offset 0)\n",
      "Ignoring wrong pointing object 87 0 (offset 0)\n",
      "Ignoring wrong pointing object 89 0 (offset 0)\n",
      "Ignoring wrong pointing object 91 0 (offset 0)\n",
      "Ignoring wrong pointing object 98 0 (offset 0)\n",
      "Ignoring wrong pointing object 100 0 (offset 0)\n",
      "Ignoring wrong pointing object 107 0 (offset 0)\n",
      "Ignoring wrong pointing object 118 0 (offset 0)\n",
      "Ignoring wrong pointing object 122 0 (offset 0)\n",
      "Ignoring wrong pointing object 124 0 (offset 0)\n",
      "Ignoring wrong pointing object 139 0 (offset 0)\n",
      "Ignoring wrong pointing object 141 0 (offset 0)\n",
      "Ignoring wrong pointing object 151 0 (offset 0)\n",
      "Ignoring wrong pointing object 153 0 (offset 0)\n",
      "Ignoring wrong pointing object 162 0 (offset 0)\n",
      "Ignoring wrong pointing object 164 0 (offset 0)\n",
      "Ignoring wrong pointing object 189 0 (offset 0)\n",
      "Ignoring wrong pointing object 272 0 (offset 0)\n",
      "Ignoring wrong pointing object 274 0 (offset 0)\n",
      "Ignoring wrong pointing object 282 0 (offset 0)\n",
      "Ignoring wrong pointing object 284 0 (offset 0)\n",
      "Ignoring wrong pointing object 286 0 (offset 0)\n",
      "Ignoring wrong pointing object 290 0 (offset 0)\n",
      "Ignoring wrong pointing object 313 0 (offset 0)\n",
      "Ignoring wrong pointing object 320 0 (offset 0)\n",
      "Ignoring wrong pointing object 322 0 (offset 0)\n",
      "Ignoring wrong pointing object 324 0 (offset 0)\n",
      "Ignoring wrong pointing object 326 0 (offset 0)\n",
      "Ignoring wrong pointing object 328 0 (offset 0)\n",
      "Ignoring wrong pointing object 330 0 (offset 0)\n",
      "Ignoring wrong pointing object 332 0 (offset 0)\n",
      "Ignoring wrong pointing object 341 0 (offset 0)\n",
      "Ignoring wrong pointing object 343 0 (offset 0)\n",
      "Ignoring wrong pointing object 359 0 (offset 0)\n",
      "Ignoring wrong pointing object 361 0 (offset 0)\n",
      "Ignoring wrong pointing object 364 0 (offset 0)\n",
      "Ignoring wrong pointing object 366 0 (offset 0)\n",
      "Ignoring wrong pointing object 379 0 (offset 0)\n",
      "Ignoring wrong pointing object 386 0 (offset 0)\n",
      "Ignoring wrong pointing object 405 0 (offset 0)\n",
      "Ignoring wrong pointing object 407 0 (offset 0)\n",
      "Ignoring wrong pointing object 409 0 (offset 0)\n",
      "Ignoring wrong pointing object 411 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[55/91] AR3 - Australian Rare Earths Ltd (ar3.com.au)\n",
      "\n",
      "[56/91] OR3 - Ore Resources Ltd (oreresources.com.au)\n",
      "\n",
      "[57/91] NMT - Neometals Ltd (neometals.com.au)\n",
      "  - quarterly: https://www.neometals.com.au/wp-content/uploads/2025/10/2025_Neometals-Sustainability-Report-FINAL.pdf\n",
      "  - annual: https://www.neometals.com.au/wp-content/uploads/2025/10/2025_Neometals-Sustainability-Report-FINAL.pdf\n",
      "\n",
      "[58/91] PGO - Pacgold Ltd (pacgold.com.au)\n",
      "\n",
      "[59/91] CAE - Cannindah Resources Ltd (cannindah.com.au)\n",
      "  - quarterly: https://cannindah.com.au/wp-content/uploads/2023/08/Noosa-mining-powerpoint.pdf\n",
      "  - annual: https://cannindah.com.au/wp-content/uploads/2023/08/Noosa-mining-powerpoint.pdf\n",
      "\n",
      "[60/91] ORD - Ordell Minerals Ltd (ordellminerals.com.au)\n",
      "  - quarterly: https://www.ordellminerals.com.au/downloads/reports/ord_qa202512.pdf\n",
      "  - annual: https://www.ordellminerals.com.au/downloads/reports/ord_af2025.pdf\n",
      "\n",
      "[61/91] GA8 - GoldArc Resources Ltd (goldarcres.com.au)\n",
      "\n",
      "[62/91] HMX - Hammer Metals Ltd (hammermetals.com.au)\n",
      "  - quarterly: https://hammermetals.com.au/wp-content/uploads/Hammer-Shareholder-Communications-Aug-2022.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - annual: https://hammermetals.com.au/wp-content/uploads/Hammer-Shareholder-Communications-Aug-2022.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[63/91] IPT - Impact Minerals Ltd (impactminerals.com.au)\n",
      "\n",
      "[64/91] DYM - Dynamic Metals Ltd (dynamicmetals.com.au)\n",
      "\n",
      "[65/91] CPN - Caspin Resources Ltd (caspin.com.au)\n",
      "  - quarterly: https://www.caspin.com.au/wp-content/uploads/2023/11/Website-Terms-of-Use.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 28 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - annual: https://www.caspin.com.au/wp-content/uploads/2023/11/Website-Terms-of-Use.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 28 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[66/91] WIN - WIN Metals Ltd (winmetals.com.au)\n",
      "\n",
      "[67/91] MAG - Magmatic Resources Ltd (magmaticresources.com)\n",
      "  - quarterly: https://magmaticresources.com/mag/wp-content/uploads/2020/02/Eagle-Research-Advisory-Report-Feb-2020.pdf\n",
      "  - annual: https://magmaticresources.com/mag/wp-content/uploads/2020/02/Eagle-Research-Advisory-Report-Feb-2020.pdf\n",
      "\n",
      "[68/91] IDA - Indiana Resources Ltd (indianaresources.com.au)\n",
      "  - quarterly: https://indianaresources.com.au/wp-content/uploads/2025/06/IDA-company-profile-2025-June-1.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 25 0 (offset 0)\n",
      "Ignoring wrong pointing object 65 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - annual: https://indianaresources.com.au/wp-content/uploads/2025/06/IDA-company-profile-2025-June-1.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 25 0 (offset 0)\n",
      "Ignoring wrong pointing object 65 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[69/91] PGM - Platina Resources Ltd (platinaresources.com.au)\n",
      "\n",
      "[70/91] ALM - Alma Metals Ltd (almametals.com.au)\n",
      "  - quarterly: https://almametals.com.au/wp-content/uploads/2024/09/Alma-Metals-Research-Report_September-2024.pdf\n",
      "  - annual: https://almametals.com.au/wp-content/uploads/2024/09/Alma-Metals-Research-Report_September-2024.pdf\n",
      "\n",
      "[71/91] ADG - Adelong Gold Ltd (adelonggold.com)\n",
      "  - quarterly: https://adelonggold.com/wp-content/uploads/ADGTargetMarketDetermination21May24.pdf\n",
      "  - annual: https://adelonggold.com/wp-content/uploads/ADGTargetMarketDetermination21May24.pdf\n",
      "\n",
      "[72/91] TG6 - TG Metals Ltd (tgmetals.com.au)\n",
      "\n",
      "[73/91] JAV - Javelin Minerals Ltd (javelinminerals.com.au)\n",
      "  - quarterly: https://javelinminerals.com.au/wp-content/uploads/2026/01/MLB-Jan22_26.pdf\n",
      "  - annual: https://javelinminerals.com.au/wp-content/uploads/2026/01/MLB-Jan22_26.pdf\n",
      "\n",
      "[74/91] SGA - Sarytogan Graphite Ltd (sarytogangraphite.com.au)\n",
      "\n",
      "[75/91] VMS - Venari Minerals NL (venariminerals.com)\n",
      "\n",
      "[76/91] AZ9 - Asian Battery Metals PLC (asianbatterymetals.com)\n",
      "  - quarterly: https://asianbatterymetals.com/wp-content/uploads/2025/01/02906992.pdf\n",
      "  - annual: https://asianbatterymetals.com/wp-content/uploads/2025/05/2886557.pdf\n",
      "\n",
      "[77/91] BNR - Bulletin Resources Ltd (bulletinresources.com)\n",
      "  - quarterly: https://www.bulletinresources.com/wp-content/uploads/2024/02/BNR-31-December-2023-Quarterly-Report-5B-FINAL.pdf\n",
      "  - annual: https://www.bulletinresources.com/wp-content/uploads/2024/03/BNR-Half-Yearly-31-Dec-2023-FINAL-Audit-Report.pdf\n",
      "\n",
      "[78/91] LAT - Latitude 66 Ltd (lat66.com)\n",
      "\n",
      "[79/91] IVG - InVert Graphite Ltd (invertgraphite.com.au)\n",
      "\n",
      "[80/91] FMR - FMR Resources Ltd (fmrresources.com.au)\n",
      "\n",
      "[81/91] HMG - Hamelin Gold Ltd (hamelingold.com.au)\n",
      "  - quarterly: https://hamelingold.com.au/wp-content/uploads/2026/01/HMG-Quarterly-Report-December-2025-FINAL.pdf\n",
      "  - annual: https://hamelingold.com.au/wp-content/uploads/2026/02/High-Grade-Aurora-Gold-Lode-at-Day-Dawn-Project.pdf\n",
      "\n",
      "[82/91] TMX - Terrain Minerals Ltd (terrainminerals.com.au)\n",
      "  - quarterly: https://terrainminerals.com.au/upload/documents/investors/quarterly-reports/260111014044_20260105December2025QuarteltyReportFNLandAppendix5BMERGED.pdf\n",
      "  - annual: https://terrainminerals.com.au/upload/documents/investors/annual-reports/250930000724_202509292025AnnualReport-SignedFinal29Sept2025Combined.pdf\n",
      "\n",
      "[83/91] SVY - Stavely Minerals Ltd (stavely.com.au)\n",
      "  - quarterly: https://www.stavely.com.au/wp-content/uploads/2025/09/20250915-SVY-Options-Prospectus-ASX-VERSION.pdf\n",
      "  - annual: https://www.stavely.com.au/wp-content/uploads/2025/09/20250915-SVY-Options-Prospectus-ASX-VERSION.pdf\n",
      "\n",
      "[84/91] KOB - Koba Resources Ltd (kobaresources.com)\n",
      "  - quarterly: https://kobaresources.com/wp-content/uploads/AustraliasPaydirtYellowCakeDebateBackOnTheMenuJul24.pdf#new_tab\n",
      "  - annual: https://kobaresources.com/wp-content/uploads/AustraliasPaydirtYellowCakeDebateBackOnTheMenuJul24.pdf#new_tab\n",
      "\n",
      "[85/91] NSM - North Stawell Minerals Ltd (northstawellminerals.com)\n",
      "  - quarterly: https://northstawellminerals.com/wp-content/uploads/2025/04/Annual-Report-to-Shareholders-1.pdf\n",
      "  - annual: https://northstawellminerals.com/wp-content/uploads/2025/04/Annual-Report-to-Shareholders-1.pdf\n",
      "\n",
      "[86/91] ATT - Altitude Minerals Ltd (coppersearch.com.au)\n",
      "\n",
      "[87/91] RNX - Renegade Exploration Ltd (renegadeexploration.com)\n",
      "  - quarterly: https://renegadeexploration.com/wp-content/uploads/2025/06/Renegade-Exploration-Limited-Investor-Presentations_Gold-Coast_for-print.pdf\n",
      "  - annual: https://renegadeexploration.com/wp-content/uploads/2025/06/Renegade-Exploration-Limited-Investor-Presentations_Gold-Coast_for-print.pdf\n",
      "\n",
      "[88/91] YAR - Yari Minerals Ltd (yariminerals.com.au)\n",
      "\n",
      "[89/91] AM5 - Antares Metals Ltd (antaresmetals.com.au)\n",
      "\n",
      "[90/91] CMO - Cosmo Metals Ltd (cosmometals.com.au)\n",
      "  - quarterly: https://cosmometals.com.au/wp-content/uploads/2026/02/Entitlement-Offer-Prospectus-Cosmo-Metals-Limited-ASIC-Lodgement-Version-2026-02-10.pdf\n",
      "  - annual: https://cosmometals.com.au/wp-content/uploads/2026/02/Entitlement-Offer-Prospectus-Cosmo-Metals-Limited-ASIC-Lodgement-Version-2026-02-10.pdf\n",
      "\n",
      "[91/91] SLM - Solis Minerals Ltd (solisminerals.com)\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'C:\\\\Users\\\\julian.diaz\\\\OneDrive - XENITH CONSULTING PTY LTD\\\\Documents\\\\01_BD\\\\96_2026_RIU_Conference_Perth\\\\riu_stage_flags.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPermissionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 747\u001b[39m\n\u001b[32m    744\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out_df, hits\n\u001b[32m    746\u001b[39m \u001b[38;5;66;03m# ---- RUN ----\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m747\u001b[39m out_df, hits_df = run_all()\n\u001b[32m    748\u001b[39m hits_df\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 738\u001b[39m, in \u001b[36mrun_all\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    735\u001b[39m     out_rows.append(asdict(row))\n\u001b[32m    737\u001b[39m out_df = pd.DataFrame(out_rows)\n\u001b[32m--> \u001b[39m\u001b[32m738\u001b[39m out_df.to_csv(OUT_CSV, index=\u001b[38;5;28;01mFalse\u001b[39;00m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    739\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mDone. Wrote: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOUT_CSV\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    740\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDownloaded reports folder: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDOWNLOAD_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\py311\\Lib\\site-packages\\pandas\\core\\generic.py:3988\u001b[39m, in \u001b[36mNDFrame.to_csv\u001b[39m\u001b[34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[39m\n\u001b[32m   3977\u001b[39m df = \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.to_frame()\n\u001b[32m   3979\u001b[39m formatter = DataFrameFormatter(\n\u001b[32m   3980\u001b[39m     frame=df,\n\u001b[32m   3981\u001b[39m     header=header,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3985\u001b[39m     decimal=decimal,\n\u001b[32m   3986\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3988\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrameRenderer(formatter).to_csv(\n\u001b[32m   3989\u001b[39m     path_or_buf,\n\u001b[32m   3990\u001b[39m     lineterminator=lineterminator,\n\u001b[32m   3991\u001b[39m     sep=sep,\n\u001b[32m   3992\u001b[39m     encoding=encoding,\n\u001b[32m   3993\u001b[39m     errors=errors,\n\u001b[32m   3994\u001b[39m     compression=compression,\n\u001b[32m   3995\u001b[39m     quoting=quoting,\n\u001b[32m   3996\u001b[39m     columns=columns,\n\u001b[32m   3997\u001b[39m     index_label=index_label,\n\u001b[32m   3998\u001b[39m     mode=mode,\n\u001b[32m   3999\u001b[39m     chunksize=chunksize,\n\u001b[32m   4000\u001b[39m     quotechar=quotechar,\n\u001b[32m   4001\u001b[39m     date_format=date_format,\n\u001b[32m   4002\u001b[39m     doublequote=doublequote,\n\u001b[32m   4003\u001b[39m     escapechar=escapechar,\n\u001b[32m   4004\u001b[39m     storage_options=storage_options,\n\u001b[32m   4005\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\py311\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1025\u001b[39m, in \u001b[36mDataFrameRenderer.to_csv\u001b[39m\u001b[34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[39m\n\u001b[32m   1004\u001b[39m     created_buffer = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1006\u001b[39m csv_formatter = CSVFormatter(\n\u001b[32m   1007\u001b[39m     path_or_buf=path_or_buf,\n\u001b[32m   1008\u001b[39m     lineterminator=lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1023\u001b[39m     formatter=\u001b[38;5;28mself\u001b[39m.fmt,\n\u001b[32m   1024\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1025\u001b[39m csv_formatter.save()\n\u001b[32m   1027\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[32m   1028\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\py311\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:251\u001b[39m, in \u001b[36mCSVFormatter.save\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03mCreate the writer & save.\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[32m    252\u001b[39m     \u001b[38;5;28mself\u001b[39m.filepath_or_buffer,\n\u001b[32m    253\u001b[39m     \u001b[38;5;28mself\u001b[39m.mode,\n\u001b[32m    254\u001b[39m     encoding=\u001b[38;5;28mself\u001b[39m.encoding,\n\u001b[32m    255\u001b[39m     errors=\u001b[38;5;28mself\u001b[39m.errors,\n\u001b[32m    256\u001b[39m     compression=\u001b[38;5;28mself\u001b[39m.compression,\n\u001b[32m    257\u001b[39m     storage_options=\u001b[38;5;28mself\u001b[39m.storage_options,\n\u001b[32m    258\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[32m    259\u001b[39m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;66;03m# error: Argument \"quoting\" to \"writer\" has incompatible type \"int\";\u001b[39;00m\n\u001b[32m    261\u001b[39m     \u001b[38;5;66;03m# expected \"Literal[0, 1, 2, 3]\"\u001b[39;00m\n\u001b[32m    262\u001b[39m     \u001b[38;5;28mself\u001b[39m.writer = csvlib.writer(\n\u001b[32m    263\u001b[39m         handles.handle,\n\u001b[32m    264\u001b[39m         lineterminator=\u001b[38;5;28mself\u001b[39m.lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m    269\u001b[39m         quotechar=\u001b[38;5;28mself\u001b[39m.quotechar,\n\u001b[32m    270\u001b[39m     )\n\u001b[32m    272\u001b[39m     \u001b[38;5;28mself\u001b[39m._save()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\py311\\Lib\\site-packages\\pandas\\io\\common.py:926\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    921\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    922\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    923\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    924\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m926\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    927\u001b[39m             handle,\n\u001b[32m    928\u001b[39m             ioargs.mode,\n\u001b[32m    929\u001b[39m             encoding=ioargs.encoding,\n\u001b[32m    930\u001b[39m             errors=errors,\n\u001b[32m    931\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    932\u001b[39m         )\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    935\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mPermissionError\u001b[39m: [Errno 13] Permission denied: 'C:\\\\Users\\\\julian.diaz\\\\OneDrive - XENITH CONSULTING PTY LTD\\\\Documents\\\\01_BD\\\\96_2026_RIU_Conference_Perth\\\\riu_stage_flags.csv'"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# RIU: Resource Definition / PFS flagger (v2)\n",
    "# - Reads companies_with_website.csv (Company, Ticker, Website)\n",
    "# - Crawls each company website (sitemap + light crawl) to find PDF links\n",
    "# - Downloads Quarterly + Annual PDFs, extracts text, flags + summarizes evidence (<= 50 words)\n",
    "# - Handles:\n",
    "#   * invalid date parsing (no crash)\n",
    "#   * PDFs where Content-Type isn't application/pdf (checks PDF signature)\n",
    "#   * ASX \"Access to this site\" HTML wall by optionally using Playwright to fetch PDF bytes\n",
    "# - Outputs: riu_stage_flags.csv + downloaded_reports/<TICKER>/*.pdf + debug_pages/<TICKER>/*.html\n",
    "# =========================\n",
    "\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import hashlib\n",
    "import shutil\n",
    "import datetime as dt\n",
    "from dataclasses import dataclass, asdict\n",
    "from pathlib import Path\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# -------- Optional: Playwright (for ASX links that return \"Access to this site\") --------\n",
    "PLAYWRIGHT_OK = False\n",
    "try:\n",
    "    from playwright.sync_api import sync_playwright\n",
    "    PLAYWRIGHT_OK = True\n",
    "except Exception:\n",
    "    PLAYWRIGHT_OK = False\n",
    "\n",
    "# -------- PDF text extraction --------\n",
    "PDF_OK = False\n",
    "try:\n",
    "    from pypdf import PdfReader\n",
    "    PDF_OK = True\n",
    "except Exception:\n",
    "    PDF_OK = False\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# USER SETTINGS\n",
    "# -------------------------\n",
    "INPUT_CSV  = r\"C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULTING PTY LTD\\Documents\\01_BD\\96_2026_RIU_Conference_Perth\\companies_with_website.csv\"\n",
    "OUT_CSV    = r\"C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULTING PTY LTD\\Documents\\01_BD\\96_2026_RIU_Conference_Perth\\riu_stage_flags.csv\"\n",
    "\n",
    "DOWNLOAD_DIR = Path(r\"C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULTING PTY LTD\\Documents\\01_BD\\96_2026_RIU_Conference_Perth\\downloaded_reports\")\n",
    "DEBUG_DIR    = Path(r\"C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULTING PTY LTD\\Documents\\01_BD\\96_2026_RIU_Conference_Perth\\debug_pages\")\n",
    "\n",
    "ONLY_TICKERS = None  # e.g. [\"CHN\"] for testing; else None for all\n",
    "\n",
    "REQUEST_TIMEOUT = 25\n",
    "SLEEP_S = 0.6\n",
    "MAX_PAGES_TO_VISIT = 35\n",
    "MAX_SITEMAP_URLS = 6000\n",
    "MAX_SITEMAP_CHILDREN = 25\n",
    "\n",
    "# If True, and a PDF link is on asx.com.au and we hit \"Access to this site\",\n",
    "# the script will try Playwright to fetch the real PDF bytes.\n",
    "USE_PLAYWRIGHT_FOR_ASX_PDFS = True\n",
    "\n",
    "# Debug verbosity\n",
    "VERBOSE = True\n",
    "\n",
    "USER_AGENT = (\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "    \"(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    ")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# KEYWORDS / HEURISTICS\n",
    "# -------------------------\n",
    "INTENT_RE = re.compile(\n",
    "    r\"\\b(will|plan(?:s|ned)?|intend(?:s|ed)?|to\\s+(?:commence|start|undertake|complete|deliver|progress|advance)|\"\n",
    "    r\"target(?:s|ing)?|scheduled|expected|underway|next\\s+quarter)\\b\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "RESOURCE_RE_LIST = [\n",
    "    r\"\\bresource definition\\b\",\n",
    "    r\"\\bresource drilling\\b\",\n",
    "    r\"\\binfill drilling\\b\",\n",
    "    r\"\\bmineral resource\\b\",\n",
    "    r\"\\bresource estimate\\b\",\n",
    "    r\"\\bmaiden resource\\b\",\n",
    "    r\"\\bJORC\\b\",\n",
    "    r\"\\bMRE\\b\",\n",
    "    r\"\\bupgrade\\b.{0,40}\\bresource\\b\",\n",
    "]\n",
    "\n",
    "PFS_RE_LIST = [\n",
    "    r\"\\bpre[-\\s]?feasibility\\b\",\n",
    "    r\"\\bPFS\\b\",\n",
    "    r\"\\bfeasibility study\\b\",\n",
    "    r\"\\bDFS\\b\",\n",
    "    r\"\\bdefinitive feasibility\\b\",\n",
    "    r\"\\bscoping study\\b\",\n",
    "]\n",
    "\n",
    "RESOURCE_RE = re.compile(\"|\".join(RESOURCE_RE_LIST), re.IGNORECASE)\n",
    "PFS_RE      = re.compile(\"|\".join(PFS_RE_LIST), re.IGNORECASE)\n",
    "\n",
    "QUARTERLY_HINT = re.compile(r\"(quarterly|appendix\\s*4c|activities\\s+report|quarter\\s+report|3[-\\s]?month)\", re.IGNORECASE)\n",
    "ANNUAL_HINT    = re.compile(r\"(annual\\s+report|appendix\\s*4e|year\\s+end|full\\s+year|financial\\s+report|annual\\s+financial)\", re.IGNORECASE)\n",
    "\n",
    "REPORT_URL_HINT = re.compile(\n",
    "    r\"(investor|investors|asx|announce|announcement|release|news|media|report|results|financial|presentation|quarter|appendix|4c|4e)\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "PDF_EXT_RE = re.compile(r\"\\.pdf(\\?|$)\", re.IGNORECASE)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# DATA MODEL\n",
    "# -------------------------\n",
    "@dataclass\n",
    "class StageRow:\n",
    "    Ticker: str\n",
    "    Company: str\n",
    "    Website: str\n",
    "    Quarterly_URL: str\n",
    "    Annual_URL: str\n",
    "    Resource_Definition_Flag: str\n",
    "    PFS_Flag: str\n",
    "    Evidence_Summary_50w: str\n",
    "    Evidence_Snippets: str\n",
    "    Notes: str\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# HELPERS\n",
    "# -------------------------\n",
    "def log(msg: str):\n",
    "    if VERBOSE:\n",
    "        print(msg)\n",
    "\n",
    "def safe_filename(s: str) -> str:\n",
    "    # Keep filenames short for Windows + OneDrive\n",
    "    s = re.sub(r\"[^a-zA-Z0-9._-]+\", \"_\", s)\n",
    "    return s[:120]\n",
    "\n",
    "def short_hash(s: str) -> str:\n",
    "    return hashlib.md5(s.encode(\"utf-8\", errors=\"ignore\")).hexdigest()[:10]\n",
    "\n",
    "def normalize_site(site: str) -> str:\n",
    "    site = (site or \"\").strip()\n",
    "    if not site:\n",
    "        return \"\"\n",
    "    if not site.startswith((\"http://\", \"https://\")):\n",
    "        site = \"https://\" + site\n",
    "    return site.rstrip(\"/\")\n",
    "\n",
    "def same_domain(base: str, url: str) -> bool:\n",
    "    try:\n",
    "        b = urlparse(base)\n",
    "        u = urlparse(url)\n",
    "        bn = b.netloc.lower()\n",
    "        un = u.netloc.lower()\n",
    "        return (bn == un) or un.endswith(\".\" + bn) or bn.endswith(\".\" + un)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def is_asx_url(url: str) -> bool:\n",
    "    try:\n",
    "        host = urlparse(url).netloc.lower()\n",
    "        return host.endswith(\"asx.com.au\") or host.endswith(\"announcements.asx.com.au\")\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def session() -> requests.Session:\n",
    "    s = requests.Session()\n",
    "    s.headers.update({\"User-Agent\": USER_AGENT, \"Accept-Language\": \"en-US,en;q=0.9\"})\n",
    "    return s\n",
    "\n",
    "def fetch_text(sess: requests.Session, url: str) -> tuple[int, str, str]:\n",
    "    try:\n",
    "        r = sess.get(url, timeout=REQUEST_TIMEOUT, allow_redirects=True)\n",
    "        ctype = (r.headers.get(\"content-type\") or \"\").lower()\n",
    "        return r.status_code, ctype, r.text or \"\"\n",
    "    except Exception:\n",
    "        return 0, \"\", \"\"\n",
    "\n",
    "def looks_like_pdf_bytes(first_bytes: bytes) -> bool:\n",
    "    return first_bytes.startswith(b\"%PDF-\")\n",
    "\n",
    "def download_via_requests(sess: requests.Session, url: str, outpath: Path) -> tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Download using requests. Accept if:\n",
    "      - URL endswith .pdf OR content-type includes pdf OR file starts with %PDF-\n",
    "    \"\"\"\n",
    "    try:\n",
    "        r = sess.get(url, timeout=REQUEST_TIMEOUT, stream=True, allow_redirects=True)\n",
    "        status = r.status_code\n",
    "        if status >= 400:\n",
    "            return False, f\"http_{status}\"\n",
    "\n",
    "        ctype = (r.headers.get(\"content-type\") or \"\").lower()\n",
    "        cd    = (r.headers.get(\"content-disposition\") or \"\").lower()\n",
    "\n",
    "        # Peek first chunk to detect PDF signature / ASX access wall\n",
    "        it = r.iter_content(chunk_size=1024 * 64)\n",
    "        first = next(it, b\"\") or b\"\"\n",
    "\n",
    "        # Detect ASX access wall\n",
    "        if b\"Access to this site\" in first or b\"Access to this Site\" in first:\n",
    "            return False, \"asx_access_wall\"\n",
    "\n",
    "        is_pdf = (\n",
    "            (\"pdf\" in ctype)\n",
    "            or PDF_EXT_RE.search(url)\n",
    "            or (\".pdf\" in cd)\n",
    "            or looks_like_pdf_bytes(first)\n",
    "        )\n",
    "        if not is_pdf:\n",
    "            return False, f\"not_pdf_ctype={ctype[:40]}\"\n",
    "\n",
    "        outpath.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(outpath, \"wb\") as f:\n",
    "            if first:\n",
    "                f.write(first)\n",
    "            for chunk in it:\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "\n",
    "        if not outpath.exists() or outpath.stat().st_size < 10_000:\n",
    "            return False, \"download_too_small\"\n",
    "        return True, \"ok\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return False, f\"exception_{type(e).__name__}\"\n",
    "\n",
    "def download_via_playwright(url: str, outpath: Path) -> tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Fetch bytes via Playwright (helps with ASX access wall).\n",
    "    \"\"\"\n",
    "    if not PLAYWRIGHT_OK:\n",
    "        return False, \"playwright_not_installed\"\n",
    "\n",
    "    try:\n",
    "        with sync_playwright() as p:\n",
    "            browser = p.chromium.launch(headless=True)\n",
    "            context = browser.new_context(user_agent=USER_AGENT)\n",
    "            page = context.new_page()\n",
    "\n",
    "            resp = page.goto(url, wait_until=\"networkidle\", timeout=REQUEST_TIMEOUT * 1000)\n",
    "            content = page.content() or \"\"\n",
    "\n",
    "            # If we landed on ASX access page, try clicking confirmation text/button if present.\n",
    "            if \"Access to this site\" in content:\n",
    "                # Try a few likely selectors / text patterns\n",
    "                clicked = False\n",
    "                candidates = [\n",
    "                    \"text=I confirm\",\n",
    "                    \"text=I Confirm\",\n",
    "                    \"text=confirm that any content\",\n",
    "                    \"input[type=submit]\",\n",
    "                    \"button:has-text('I confirm')\",\n",
    "                    \"a:has-text('I confirm')\",\n",
    "                ]\n",
    "                for sel in candidates:\n",
    "                    try:\n",
    "                        if page.locator(sel).count() > 0:\n",
    "                            page.locator(sel).first.click(timeout=3000)\n",
    "                            clicked = True\n",
    "                            break\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "                # Re-try navigation after confirming\n",
    "                if clicked:\n",
    "                    resp = page.goto(url, wait_until=\"networkidle\", timeout=REQUEST_TIMEOUT * 1000)\n",
    "\n",
    "            if resp is None:\n",
    "                browser.close()\n",
    "                return False, \"no_response\"\n",
    "\n",
    "            headers = {k.lower(): v for k, v in (resp.headers or {}).items()}\n",
    "            ctype = headers.get(\"content-type\", \"\").lower()\n",
    "\n",
    "            body = resp.body()\n",
    "            if not body:\n",
    "                browser.close()\n",
    "                return False, \"empty_body\"\n",
    "\n",
    "            if ((\"pdf\" not in ctype) and (not looks_like_pdf_bytes(body[:8]))):\n",
    "                # Still not a PDF\n",
    "                browser.close()\n",
    "                return False, f\"not_pdf_playwright_ctype={ctype[:40]}\"\n",
    "\n",
    "            outpath.parent.mkdir(parents=True, exist_ok=True)\n",
    "            outpath.write_bytes(body)\n",
    "\n",
    "            browser.close()\n",
    "            if outpath.stat().st_size < 10_000:\n",
    "                return False, \"pw_download_too_small\"\n",
    "            return True, \"ok\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return False, f\"pw_exception_{type(e).__name__}\"\n",
    "\n",
    "def download_file(sess: requests.Session, url: str, outpath: Path) -> tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Try requests first. If ASX access wall and allowed, try Playwright.\n",
    "    \"\"\"\n",
    "    ok, why = download_via_requests(sess, url, outpath)\n",
    "    if ok:\n",
    "        return True, \"requests_ok\"\n",
    "\n",
    "    if why == \"asx_access_wall\" and USE_PLAYWRIGHT_FOR_ASX_PDFS and is_asx_url(url):\n",
    "        ok2, why2 = download_via_playwright(url, outpath)\n",
    "        return ok2, f\"playwright_{why2}\"\n",
    "\n",
    "    return False, why\n",
    "\n",
    "def extract_pdf_text(pdf_path: Path) -> str:\n",
    "    if not PDF_OK:\n",
    "        return \"\"\n",
    "    try:\n",
    "        reader = PdfReader(str(pdf_path))\n",
    "        parts = []\n",
    "        for p in reader.pages:\n",
    "            t = p.extract_text() or \"\"\n",
    "            if t:\n",
    "                parts.append(t)\n",
    "        return \"\\n\".join(parts)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def split_sentences(text: str) -> list[str]:\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "    text = re.sub(r\"\\n{2,}\", \"\\n\", text)\n",
    "    sents = re.split(r\"(?<=[\\.\\?\\!])\\s+|\\n+\", text)\n",
    "    sents = [s.strip() for s in sents if s and len(s.strip()) > 20]\n",
    "    return sents\n",
    "\n",
    "def trim_to_50_words(s: str) -> str:\n",
    "    words = s.split()\n",
    "    if len(words) <= 50:\n",
    "        return s.strip()\n",
    "    return \" \".join(words[:50]).strip() + \"\"\n",
    "\n",
    "def safe_dt(y: int, mo: int, d: int) -> dt.datetime | None:\n",
    "    try:\n",
    "        return dt.datetime(y, mo, d)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def guess_date_from_text(s: str) -> dt.datetime | None:\n",
    "    s = s or \"\"\n",
    "\n",
    "    # YYYY-MM-DD / YYYY/MM/DD\n",
    "    m = re.search(r\"(20\\d{2})[-/](\\d{1,2})[-/](\\d{1,2})\", s)\n",
    "    if m:\n",
    "        y, mo, d = map(int, m.groups())\n",
    "        out = safe_dt(y, mo, d)\n",
    "        if out:\n",
    "            return out\n",
    "\n",
    "    # DD MMM YYYY\n",
    "    m = re.search(r\"(\\d{1,2})\\s+(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s+(20\\d{2})\", s, re.IGNORECASE)\n",
    "    if m:\n",
    "        d = int(m.group(1)); mon = m.group(2).lower(); y = int(m.group(3))\n",
    "        months = {\"jan\":1,\"feb\":2,\"mar\":3,\"apr\":4,\"may\":5,\"jun\":6,\"jul\":7,\"aug\":8,\"sep\":9,\"oct\":10,\"nov\":11,\"dec\":12}\n",
    "        out = safe_dt(y, months[mon[:3]], d)\n",
    "        if out:\n",
    "            return out\n",
    "\n",
    "    # MMM YYYY\n",
    "    m = re.search(r\"(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s+(20\\d{2})\", s, re.IGNORECASE)\n",
    "    if m:\n",
    "        mon = m.group(1).lower(); y = int(m.group(2))\n",
    "        months = {\"jan\":1,\"feb\":2,\"mar\":3,\"apr\":4,\"may\":5,\"jun\":6,\"jul\":7,\"aug\":8,\"sep\":9,\"oct\":10,\"nov\":11,\"dec\":12}\n",
    "        return dt.datetime(y, months[mon[:3]], 1)\n",
    "\n",
    "    # YYYYMMDD\n",
    "    m = re.search(r\"(20\\d{2})(\\d{2})(\\d{2})\", s)\n",
    "    if m:\n",
    "        y, mo, d = map(int, m.groups())\n",
    "        out = safe_dt(y, mo, d)\n",
    "        if out:\n",
    "            return out\n",
    "\n",
    "    return None\n",
    "\n",
    "def extract_pdf_links_from_html(base_url: str, html: str) -> list[dict]:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    out = []\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = (a.get(\"href\") or \"\").strip()\n",
    "        if not href:\n",
    "            continue\n",
    "        absu = urljoin(base_url, href)\n",
    "        label = \" \".join(a.get_text(\" \", strip=True).split())\n",
    "\n",
    "        # Prefer explicit .pdf links, but also catch \"download\" style links that include \"pdf\" in query/path\n",
    "        if PDF_EXT_RE.search(absu) or (\"pdf\" in absu.lower() and (\"download\" in absu.lower() or \"media\" in absu.lower() or \"uploads\" in absu.lower())):\n",
    "            out.append({\n",
    "                \"url\": absu,\n",
    "                \"label\": label,\n",
    "                \"date_guess\": guess_date_from_text(label + \" \" + absu),\n",
    "                \"host\": \"asx\" if is_asx_url(absu) else \"site\"\n",
    "            })\n",
    "    return out\n",
    "\n",
    "def best_doc_candidate(cands: list[dict], want: str) -> dict | None:\n",
    "    if not cands:\n",
    "        return None\n",
    "\n",
    "    # Prefer site-hosted PDFs over ASX-hosted PDFs (ASX may block automation)\n",
    "    site_first = sorted(cands, key=lambda c: 0 if c.get(\"host\") == \"site\" else 1)\n",
    "\n",
    "    if want == \"quarterly\":\n",
    "        filt = [c for c in site_first if QUARTERLY_HINT.search((c.get(\"label\",\"\") + \" \" + c.get(\"url\",\"\")))]\n",
    "    else:\n",
    "        filt = [c for c in site_first if ANNUAL_HINT.search((c.get(\"label\",\"\") + \" \" + c.get(\"url\",\"\")))]\n",
    "\n",
    "    if not filt:\n",
    "        filt = site_first[:]  # fallback\n",
    "\n",
    "    def key(c):\n",
    "        d = c.get(\"date_guess\")\n",
    "        return d or dt.datetime(1970, 1, 1)\n",
    "\n",
    "    filt.sort(key=key, reverse=True)\n",
    "    return filt[0]\n",
    "\n",
    "def get_sitemap_urls(sess: requests.Session, site: str) -> list[str]:\n",
    "    starts = [\"/sitemap.xml\", \"/sitemap_index.xml\", \"/wp-sitemap.xml\"]\n",
    "    all_urls = []\n",
    "\n",
    "    for p in starts:\n",
    "        sm = site + p\n",
    "        status, ctype, txt = fetch_text(sess, sm)\n",
    "        time.sleep(SLEEP_S)\n",
    "\n",
    "        if status >= 400 or not txt or \"xml\" not in ctype:\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(txt, \"xml\")\n",
    "        sitemaps = [loc.get_text(strip=True) for loc in soup.find_all(\"sitemap\") for loc in loc.find_all(\"loc\")]  # type: ignore\n",
    "\n",
    "        if sitemaps:\n",
    "            for child in sitemaps[:MAX_SITEMAP_CHILDREN]:\n",
    "                st, ct, t2 = fetch_text(sess, child)\n",
    "                time.sleep(SLEEP_S)\n",
    "                if st >= 400 or not t2 or \"xml\" not in ct:\n",
    "                    continue\n",
    "                s2 = BeautifulSoup(t2, \"xml\")\n",
    "                locs = [loc.get_text(strip=True) for loc in s2.find_all(\"loc\")]\n",
    "                for u in locs:\n",
    "                    if REPORT_URL_HINT.search(u):\n",
    "                        all_urls.append(u)\n",
    "                if len(all_urls) >= MAX_SITEMAP_URLS:\n",
    "                    return all_urls[:MAX_SITEMAP_URLS]\n",
    "        else:\n",
    "            locs = [loc.get_text(strip=True) for loc in soup.find_all(\"loc\")]\n",
    "            for u in locs:\n",
    "                if REPORT_URL_HINT.search(u):\n",
    "                    all_urls.append(u)\n",
    "            if all_urls:\n",
    "                return all_urls[:MAX_SITEMAP_URLS]\n",
    "\n",
    "    return all_urls[:MAX_SITEMAP_URLS]\n",
    "\n",
    "def discover_report_pages(site: str) -> list[str]:\n",
    "    paths = [\n",
    "        \"/investors\", \"/investor\", \"/investor-centre\",\n",
    "        \"/asx-announcements\", \"/announcements\", \"/asx-releases\",\n",
    "        \"/reports\", \"/financial-reports\", \"/results\", \"/news\", \"/media\",\n",
    "        \"/investors/asx-announcements\", \"/investors/announcements\", \"/investors/reports\",\n",
    "    ]\n",
    "    return [site + p for p in paths]\n",
    "\n",
    "def pick_latest_quarterly_and_annual_from_site(sess: requests.Session, site: str, debug_dir: Path) -> tuple[dict|None, dict|None, list[str]]:\n",
    "    notes = []\n",
    "    pdf_candidates = []\n",
    "\n",
    "    debug_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    pages = get_sitemap_urls(sess, site)\n",
    "    if pages:\n",
    "        notes.append(f\"sitemap_pages={len(pages)}\")\n",
    "    else:\n",
    "        notes.append(\"no_sitemap_or_no_filtered_urls\")\n",
    "        pages = discover_report_pages(site)\n",
    "\n",
    "    visited = set()\n",
    "    queue = []\n",
    "\n",
    "    for u in pages[:250]:\n",
    "        if REPORT_URL_HINT.search(u):\n",
    "            queue.append(u)\n",
    "\n",
    "    while queue and len(visited) < MAX_PAGES_TO_VISIT:\n",
    "        url = queue.pop(0)\n",
    "        if url in visited:\n",
    "            continue\n",
    "        if not same_domain(site, url):\n",
    "            continue\n",
    "\n",
    "        visited.add(url)\n",
    "        st, ct, html = fetch_text(sess, url)\n",
    "        time.sleep(SLEEP_S)\n",
    "\n",
    "        if not html or st >= 400:\n",
    "            continue\n",
    "\n",
    "        # Safer debug filename (hash-based)\n",
    "        dbg_name = f\"{len(visited):02d}_{short_hash(url)}_{safe_filename(urlparse(url).path)}.html\"\n",
    "        (debug_dir / dbg_name).write_text(html, encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "        pdfs = extract_pdf_links_from_html(url, html)\n",
    "        for p in pdfs:\n",
    "            p[\"source_page\"] = url\n",
    "            pdf_candidates.append(p)\n",
    "\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        new_links = 0\n",
    "        for a in soup.find_all(\"a\", href=True):\n",
    "            href = (a.get(\"href\") or \"\").strip()\n",
    "            if not href:\n",
    "                continue\n",
    "            absu = urljoin(url, href)\n",
    "            if absu in visited:\n",
    "                continue\n",
    "            if not same_domain(site, absu):\n",
    "                continue\n",
    "            if REPORT_URL_HINT.search(absu):\n",
    "                queue.append(absu)\n",
    "                new_links += 1\n",
    "                if new_links >= 30:\n",
    "                    break\n",
    "\n",
    "    notes += [f\"visited_pages={len(visited)}\", f\"pdf_candidates_raw={len(pdf_candidates)}\"]\n",
    "\n",
    "    if not pdf_candidates:\n",
    "        return None, None, notes + [\"no_pdf_links_found\"]\n",
    "\n",
    "    # de-dup by URL\n",
    "    seen = set()\n",
    "    dedup = []\n",
    "    for c in pdf_candidates:\n",
    "        u = c.get(\"url\")\n",
    "        if not u or u in seen:\n",
    "            continue\n",
    "        seen.add(u)\n",
    "        if not c.get(\"date_guess\"):\n",
    "            c[\"date_guess\"] = guess_date_from_text((c.get(\"label\",\"\") + \" \" + u))\n",
    "        dedup.append(c)\n",
    "\n",
    "    q = best_doc_candidate(dedup, want=\"quarterly\")\n",
    "    a = best_doc_candidate(dedup, want=\"annual\")\n",
    "\n",
    "    if q and a and q.get(\"url\") == a.get(\"url\"):\n",
    "        annuals = [c for c in dedup if ANNUAL_HINT.search((c.get(\"label\",\"\") + \" \" + c.get(\"url\",\"\")))]\n",
    "        if annuals:\n",
    "            annuals.sort(key=lambda c: c.get(\"date_guess\") or dt.datetime(1970,1,1), reverse=True)\n",
    "            a = annuals[0]\n",
    "\n",
    "    notes.append(f\"pdf_candidates_dedup={len(dedup)}\")\n",
    "    return q, a, notes\n",
    "\n",
    "def evidence_from_text(text: str) -> tuple[bool, bool, list[str]]:\n",
    "    sents = split_sentences(text)\n",
    "    resource_hits = []\n",
    "    pfs_hits = []\n",
    "\n",
    "    for s in sents:\n",
    "        has_intent = bool(INTENT_RE.search(s))\n",
    "        if RESOURCE_RE.search(s):\n",
    "            if has_intent or (\"program\" in s.lower()) or (\"next\" in s.lower()):\n",
    "                resource_hits.append(s)\n",
    "        if PFS_RE.search(s):\n",
    "            if has_intent or (\"study\" in s.lower()) or (\"engineering\" in s.lower()):\n",
    "                pfs_hits.append(s)\n",
    "\n",
    "    resource_hits = sorted(resource_hits, key=len)[:2]\n",
    "    pfs_hits      = sorted(pfs_hits, key=len)[:2]\n",
    "\n",
    "    return (len(resource_hits) > 0), (len(pfs_hits) > 0), (resource_hits + pfs_hits)\n",
    "\n",
    "def process_company(ticker: str, company: str, website: str, base_out: Path, base_debug: Path) -> StageRow:\n",
    "    site = normalize_site(website)\n",
    "    sess = session()\n",
    "    tkr = (ticker or \"\").strip().upper()\n",
    "\n",
    "    if not site or not tkr:\n",
    "        return StageRow(tkr, company, website, \"\", \"\", \"N\", \"N\", \"\", \"\", \"missing ticker or website\")\n",
    "\n",
    "    company_out = base_out / tkr\n",
    "    company_dbg = base_debug / tkr\n",
    "    company_out.mkdir(parents=True, exist_ok=True)\n",
    "    company_dbg.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    q_doc, a_doc, notes = pick_latest_quarterly_and_annual_from_site(sess, site, company_dbg)\n",
    "\n",
    "    q_url = q_doc[\"url\"] if q_doc else \"\"\n",
    "    a_url = a_doc[\"url\"] if a_doc else \"\"\n",
    "\n",
    "    resource_flag = \"N\"\n",
    "    pfs_flag = \"N\"\n",
    "    snippets = []\n",
    "    summary = \"\"\n",
    "    note_txt = \"; \".join(notes)\n",
    "\n",
    "    # Save a small debug JSON about what we picked\n",
    "    try:\n",
    "        dbg = {\"ticker\": tkr, \"site\": site, \"quarterly\": q_doc, \"annual\": a_doc, \"notes\": notes}\n",
    "        (company_dbg / \"picked_docs.json\").write_text(json.dumps(dbg, default=str, indent=2), encoding=\"utf-8\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    def handle_doc(doc: dict|None, doc_tag: str):\n",
    "        nonlocal resource_flag, pfs_flag, snippets, summary, note_txt\n",
    "        if not doc:\n",
    "            note_txt += f\"; no_{doc_tag}_doc\"\n",
    "            return\n",
    "\n",
    "        url = (doc.get(\"url\") or \"\").strip()\n",
    "        if not url:\n",
    "            note_txt += f\"; empty_{doc_tag}_url\"\n",
    "            return\n",
    "\n",
    "        date_guess = doc.get(\"date_guess\")\n",
    "        datestr = date_guess.strftime(\"%Y%m%d\") if isinstance(date_guess, dt.datetime) else \"unknown\"\n",
    "        pdf_path = company_out / f\"{tkr}_{doc_tag}_{datestr}.pdf\"\n",
    "\n",
    "        log(f\"  - {doc_tag}: {url}\")\n",
    "        ok, why = download_file(sess, url, pdf_path)\n",
    "        time.sleep(SLEEP_S)\n",
    "\n",
    "        if not ok:\n",
    "            note_txt += f\"; download_failed_{doc_tag}({why})\"\n",
    "            return\n",
    "        else:\n",
    "            note_txt += f\"; downloaded_{doc_tag}({why})\"\n",
    "\n",
    "        txt = extract_pdf_text(pdf_path)\n",
    "        if not txt.strip():\n",
    "            note_txt += f\"; no_text_{doc_tag}_pdf\"\n",
    "            return\n",
    "\n",
    "        r_ok, p_ok, hits = evidence_from_text(txt)\n",
    "        if r_ok:\n",
    "            resource_flag = \"Y\"\n",
    "        if p_ok:\n",
    "            pfs_flag = \"Y\"\n",
    "        if hits:\n",
    "            snippets.extend([f\"[{doc_tag.upper()}] {h}\" for h in hits])\n",
    "\n",
    "    handle_doc(q_doc, \"quarterly\")\n",
    "    handle_doc(a_doc, \"annual\")\n",
    "\n",
    "    if snippets:\n",
    "        combined = \" \".join(snippets)\n",
    "        summary = trim_to_50_words(combined)\n",
    "        snippet_txt = \"\\n\".join(snippets[:6])\n",
    "    else:\n",
    "        snippet_txt = \"\"\n",
    "\n",
    "    return StageRow(\n",
    "        Ticker=tkr,\n",
    "        Company=company,\n",
    "        Website=website,\n",
    "        Quarterly_URL=q_url,\n",
    "        Annual_URL=a_url,\n",
    "        Resource_Definition_Flag=resource_flag,\n",
    "        PFS_Flag=pfs_flag,\n",
    "        Evidence_Summary_50w=summary,\n",
    "        Evidence_Snippets=snippet_txt,\n",
    "        Notes=note_txt\n",
    "    )\n",
    "\n",
    "def read_companies(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, sep=None, engine=\"python\")\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "    colmap = {c.lower(): c for c in df.columns}\n",
    "    def pick(*names):\n",
    "        for n in names:\n",
    "            if n in colmap:\n",
    "                return colmap[n]\n",
    "        return None\n",
    "\n",
    "    c_company = pick(\"company\", \"name\", \"co\", \"issuer\")\n",
    "    c_ticker  = pick(\"ticker\", \"asx\", \"code\", \"symbol\")\n",
    "    c_web     = pick(\"website\", \"web\", \"url\", \"site\")\n",
    "\n",
    "    if not c_company or not c_ticker or not c_web:\n",
    "        raise ValueError(f\"Missing required columns. Found: {list(df.columns)}. Need Company/Ticker/Website (or similar).\")\n",
    "\n",
    "    df = df.rename(columns={c_company:\"Company\", c_ticker:\"Ticker\", c_web:\"Website\"})\n",
    "    df[\"Ticker\"] = df[\"Ticker\"].astype(str).str.strip()\n",
    "    df[\"Company\"] = df[\"Company\"].astype(str).str.strip()\n",
    "    df[\"Website\"] = df[\"Website\"].astype(str).str.strip()\n",
    "    return df\n",
    "\n",
    "def run_all():\n",
    "    DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    DEBUG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if USE_PLAYWRIGHT_FOR_ASX_PDFS and not PLAYWRIGHT_OK:\n",
    "        print(\"NOTE: Playwright not installed, ASX links may fail. Install with: pip install playwright; python -m playwright install chromium\")\n",
    "\n",
    "    df = read_companies(INPUT_CSV)\n",
    "    if ONLY_TICKERS:\n",
    "        df = df[df[\"Ticker\"].str.upper().isin([t.upper() for t in ONLY_TICKERS])].copy()\n",
    "\n",
    "    out_rows = []\n",
    "    total = len(df)\n",
    "\n",
    "    for idx, r in df.iterrows():\n",
    "        tkr = str(r[\"Ticker\"]).strip().upper()\n",
    "        company = str(r[\"Company\"]).strip()\n",
    "        web = str(r[\"Website\"]).strip()\n",
    "\n",
    "        print(f\"\\n[{len(out_rows)+1}/{total}] {tkr} - {company} ({web})\")\n",
    "\n",
    "        try:\n",
    "            row = process_company(tkr, company, web, DOWNLOAD_DIR, DEBUG_DIR)\n",
    "        except Exception as e:\n",
    "            row = StageRow(\n",
    "                Ticker=tkr, Company=company, Website=web,\n",
    "                Quarterly_URL=\"\", Annual_URL=\"\",\n",
    "                Resource_Definition_Flag=\"N\", PFS_Flag=\"N\",\n",
    "                Evidence_Summary_50w=\"\", Evidence_Snippets=\"\",\n",
    "                Notes=f\"FATAL_{type(e).__name__}:{e}\"\n",
    "            )\n",
    "\n",
    "        out_rows.append(asdict(row))\n",
    "\n",
    "    out_df = pd.DataFrame(out_rows)\n",
    "    out_df.to_csv(OUT_CSV, index=False, encoding=\"utf-8\")\n",
    "    print(f\"\\nDone. Wrote: {OUT_CSV}\")\n",
    "    print(f\"Downloaded reports folder: {DOWNLOAD_DIR}\")\n",
    "\n",
    "    hits = out_df[(out_df[\"Resource_Definition_Flag\"]==\"Y\") | (out_df[\"PFS_Flag\"]==\"Y\")].copy()\n",
    "    print(f\"Flagged companies: {len(hits)} / {len(out_df)}\")\n",
    "    return out_df, hits\n",
    "\n",
    "# ---- RUN ----\n",
    "out_df, hits_df = run_all()\n",
    "hits_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac6d8400-722b-4063-9444-401dcc717b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found ticker folders: 91 in C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULTING PTY LTD\\Documents\\01_BD\\96_2026_RIU_Conference_Perth\\downloaded_reports\n",
      "[2/91] ADG | q=ADG_quarterly_unknown.pdf | a=ADG_annual_unknown.pdf\n",
      "[4/91] ALM | q=ALM_quarterly_unknown.pdf | a=ALM_annual_unknown.pdf\n",
      "[7/91] ARL | q=ARL_quarterly_20250301.pdf | a=ARL_annual_20190630.pdf\n",
      "[10/91] AZ9 | q=AZ9_quarterly_20250102.pdf | a=AZ9_annual_20250528.pdf\n",
      "[11/91] AZY | q=AZY_quarterly_20251201.pdf | a=AZY_annual_20200312.pdf\n",
      "[13/91] BGD | q=BGD_quarterly_unknown.pdf | a=BGD_annual_unknown.pdf\n",
      "[15/91] BNR | q=BNR_quarterly_20231231.pdf | a=BNR_annual_20231201.pdf\n",
      "[17/91] BTR | q=BTR_quarterly_20250420.pdf | a=BTR_annual_20250420.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CAUTION: startxref found while searching for %%EOF. The file might be truncated and some data might not be read.\n",
      "EOF marker not found\n",
      "CAUTION: startxref found while searching for %%EOF. The file might be truncated and some data might not be read.\n",
      "EOF marker not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18/91] CAE | q=CAE_quarterly_unknown.pdf | a=CAE_annual_unknown.pdf\n",
      "[19/91] CHN | q=CHN_quarterly_20251226.pdf | a=CHN_annual_unknown.pdf\n",
      "[20/91] CMO | q=CMO_quarterly_20260210.pdf | a=CMO_annual_20260210.pdf\n",
      "[21/91] CNB | q=CNB_quarterly_20260101.pdf | a=CNB_annual_20260101.pdf\n",
      "[23/91] CPN | q=CPN_quarterly_unknown.pdf | a=CPN_annual_unknown.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 28 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 28 0 (offset 0)\n",
      "Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 26 0 (offset 0)\n",
      "Ignoring wrong pointing object 28 0 (offset 0)\n",
      "Ignoring wrong pointing object 40 0 (offset 0)\n",
      "Ignoring wrong pointing object 42 0 (offset 0)\n",
      "Ignoring wrong pointing object 61 0 (offset 0)\n",
      "Ignoring wrong pointing object 63 0 (offset 0)\n",
      "Ignoring wrong pointing object 75 0 (offset 0)\n",
      "Ignoring wrong pointing object 83 0 (offset 0)\n",
      "Ignoring wrong pointing object 85 0 (offset 0)\n",
      "Ignoring wrong pointing object 87 0 (offset 0)\n",
      "Ignoring wrong pointing object 89 0 (offset 0)\n",
      "Ignoring wrong pointing object 91 0 (offset 0)\n",
      "Ignoring wrong pointing object 98 0 (offset 0)\n",
      "Ignoring wrong pointing object 100 0 (offset 0)\n",
      "Ignoring wrong pointing object 107 0 (offset 0)\n",
      "Ignoring wrong pointing object 118 0 (offset 0)\n",
      "Ignoring wrong pointing object 122 0 (offset 0)\n",
      "Ignoring wrong pointing object 124 0 (offset 0)\n",
      "Ignoring wrong pointing object 139 0 (offset 0)\n",
      "Ignoring wrong pointing object 141 0 (offset 0)\n",
      "Ignoring wrong pointing object 151 0 (offset 0)\n",
      "Ignoring wrong pointing object 153 0 (offset 0)\n",
      "Ignoring wrong pointing object 162 0 (offset 0)\n",
      "Ignoring wrong pointing object 164 0 (offset 0)\n",
      "Ignoring wrong pointing object 189 0 (offset 0)\n",
      "Ignoring wrong pointing object 272 0 (offset 0)\n",
      "Ignoring wrong pointing object 274 0 (offset 0)\n",
      "Ignoring wrong pointing object 282 0 (offset 0)\n",
      "Ignoring wrong pointing object 284 0 (offset 0)\n",
      "Ignoring wrong pointing object 286 0 (offset 0)\n",
      "Ignoring wrong pointing object 290 0 (offset 0)\n",
      "Ignoring wrong pointing object 313 0 (offset 0)\n",
      "Ignoring wrong pointing object 320 0 (offset 0)\n",
      "Ignoring wrong pointing object 322 0 (offset 0)\n",
      "Ignoring wrong pointing object 324 0 (offset 0)\n",
      "Ignoring wrong pointing object 326 0 (offset 0)\n",
      "Ignoring wrong pointing object 328 0 (offset 0)\n",
      "Ignoring wrong pointing object 330 0 (offset 0)\n",
      "Ignoring wrong pointing object 332 0 (offset 0)\n",
      "Ignoring wrong pointing object 341 0 (offset 0)\n",
      "Ignoring wrong pointing object 343 0 (offset 0)\n",
      "Ignoring wrong pointing object 359 0 (offset 0)\n",
      "Ignoring wrong pointing object 361 0 (offset 0)\n",
      "Ignoring wrong pointing object 364 0 (offset 0)\n",
      "Ignoring wrong pointing object 366 0 (offset 0)\n",
      "Ignoring wrong pointing object 379 0 (offset 0)\n",
      "Ignoring wrong pointing object 386 0 (offset 0)\n",
      "Ignoring wrong pointing object 405 0 (offset 0)\n",
      "Ignoring wrong pointing object 407 0 (offset 0)\n",
      "Ignoring wrong pointing object 409 0 (offset 0)\n",
      "Ignoring wrong pointing object 411 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26/91] CXU | q=CXU_quarterly_20250420.pdf | a=CXU_annual_20250420.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 26 0 (offset 0)\n",
      "Ignoring wrong pointing object 28 0 (offset 0)\n",
      "Ignoring wrong pointing object 40 0 (offset 0)\n",
      "Ignoring wrong pointing object 42 0 (offset 0)\n",
      "Ignoring wrong pointing object 61 0 (offset 0)\n",
      "Ignoring wrong pointing object 63 0 (offset 0)\n",
      "Ignoring wrong pointing object 75 0 (offset 0)\n",
      "Ignoring wrong pointing object 83 0 (offset 0)\n",
      "Ignoring wrong pointing object 85 0 (offset 0)\n",
      "Ignoring wrong pointing object 87 0 (offset 0)\n",
      "Ignoring wrong pointing object 89 0 (offset 0)\n",
      "Ignoring wrong pointing object 91 0 (offset 0)\n",
      "Ignoring wrong pointing object 98 0 (offset 0)\n",
      "Ignoring wrong pointing object 100 0 (offset 0)\n",
      "Ignoring wrong pointing object 107 0 (offset 0)\n",
      "Ignoring wrong pointing object 118 0 (offset 0)\n",
      "Ignoring wrong pointing object 122 0 (offset 0)\n",
      "Ignoring wrong pointing object 124 0 (offset 0)\n",
      "Ignoring wrong pointing object 139 0 (offset 0)\n",
      "Ignoring wrong pointing object 141 0 (offset 0)\n",
      "Ignoring wrong pointing object 151 0 (offset 0)\n",
      "Ignoring wrong pointing object 153 0 (offset 0)\n",
      "Ignoring wrong pointing object 162 0 (offset 0)\n",
      "Ignoring wrong pointing object 164 0 (offset 0)\n",
      "Ignoring wrong pointing object 189 0 (offset 0)\n",
      "Ignoring wrong pointing object 272 0 (offset 0)\n",
      "Ignoring wrong pointing object 274 0 (offset 0)\n",
      "Ignoring wrong pointing object 282 0 (offset 0)\n",
      "Ignoring wrong pointing object 284 0 (offset 0)\n",
      "Ignoring wrong pointing object 286 0 (offset 0)\n",
      "Ignoring wrong pointing object 290 0 (offset 0)\n",
      "Ignoring wrong pointing object 313 0 (offset 0)\n",
      "Ignoring wrong pointing object 320 0 (offset 0)\n",
      "Ignoring wrong pointing object 322 0 (offset 0)\n",
      "Ignoring wrong pointing object 324 0 (offset 0)\n",
      "Ignoring wrong pointing object 326 0 (offset 0)\n",
      "Ignoring wrong pointing object 328 0 (offset 0)\n",
      "Ignoring wrong pointing object 330 0 (offset 0)\n",
      "Ignoring wrong pointing object 332 0 (offset 0)\n",
      "Ignoring wrong pointing object 341 0 (offset 0)\n",
      "Ignoring wrong pointing object 343 0 (offset 0)\n",
      "Ignoring wrong pointing object 359 0 (offset 0)\n",
      "Ignoring wrong pointing object 361 0 (offset 0)\n",
      "Ignoring wrong pointing object 364 0 (offset 0)\n",
      "Ignoring wrong pointing object 366 0 (offset 0)\n",
      "Ignoring wrong pointing object 379 0 (offset 0)\n",
      "Ignoring wrong pointing object 386 0 (offset 0)\n",
      "Ignoring wrong pointing object 405 0 (offset 0)\n",
      "Ignoring wrong pointing object 407 0 (offset 0)\n",
      "Ignoring wrong pointing object 409 0 (offset 0)\n",
      "Ignoring wrong pointing object 411 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27/91] DEV | q=DEV_quarterly_20251201.pdf | a=DEV_annual_20250601.pdf\n",
      "[31/91] FAL | q=FAL_quarterly_20260120.pdf | a=FAL_annual_unknown.pdf\n",
      "[33/91] FLG | q=FLG_quarterly_20250102.pdf | a=FLG_annual_20251001.pdf\n",
      "[37/91] GG8 | q=GG8_quarterly_20260131.pdf | a=GG8_annual_20260131.pdf\n",
      "[38/91] GHM | q=GHM_quarterly_20251120.pdf | a=GHM_annual_20251120.pdf\n",
      "[39/91] HAS | q=HAS_quarterly_unknown.pdf | a=HAS_annual_unknown.pdf\n",
      "[40/91] HCH | q=HCH_quarterly_20260204.pdf | a=HCH_annual_20260204.pdf\n",
      "[41/91] HMG | q=HMG_quarterly_20260129.pdf | a=HMG_annual_20260209.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42/91] HMX | q=HMX_quarterly_unknown.pdf | a=HMX_annual_unknown.pdf\n",
      "[43/91] HRN | q=HRN_quarterly_20190819.pdf | a=HRN_annual_20190819.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 25 0 (offset 0)\n",
      "Ignoring wrong pointing object 65 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[45/91] IDA | q=IDA_quarterly_unknown.pdf | a=IDA_annual_unknown.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 25 0 (offset 0)\n",
      "Ignoring wrong pointing object 65 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[49/91] JAV | q=JAV_quarterly_20260122.pdf | a=JAV_annual_20260122.pdf\n",
      "[50/91] KOB | q=KOB_quarterly_unknown.pdf | a=KOB_annual_unknown.pdf\n",
      "[55/91] MAG | q=MAG_quarterly_unknown.pdf | a=MAG_annual_unknown.pdf\n",
      "[56/91] MAT | q=MAT_quarterly_unknown.pdf | a=MAT_annual_unknown.pdf\n",
      "[57/91] MAU | q=MAU_quarterly_20260204.pdf | a=MAU_annual_20260204.pdf\n",
      "[59/91] MM8 | q=MM8_quarterly_unknown.pdf | a=MM8_annual_unknown.pdf\n",
      "[60/91] MMA | q=MMA_quarterly_20230301.pdf | a=MMA_annual_20250601.pdf\n",
      "[62/91] NMT | q=NMT_quarterly_20251020.pdf | a=NMT_annual_20251020.pdf\n",
      "[63/91] NSM | q=NSM_quarterly_unknown.pdf | a=NSM_annual_unknown.pdf\n",
      "[65/91] ORD | q=ORD_quarterly_20251201.pdf | a=ORD_annual_unknown.pdf\n",
      "[66/91] PC2 | q=PC2_quarterly_20251125.pdf | a=PC2_annual_20251125.pdf\n",
      "[71/91] RDM | q=RDM_quarterly_unknown.pdf | a=RDM_annual_20250328.pdf\n",
      "[73/91] RMS | q=RMS_quarterly_20260129.pdf | a=RMS_annual_20251020.pdf\n",
      "[74/91] RNU | q=RNU_quarterly_20191120.pdf | a=RNU_annual_20191120.pdf\n",
      "[75/91] RNX | q=RNX_quarterly_20250601.pdf | a=RNX_annual_20250601.pdf\n",
      "[80/91] STK | q=STK_quarterly_20241206.pdf | a=STK_annual_20241206.pdf\n",
      "[81/91] SVL | q=SVL_quarterly_20251106.pdf | a=SVL_annual_20251006.pdf\n",
      "[82/91] SVY | q=SVY_quarterly_20250920.pdf | a=SVY_annual_20250920.pdf\n",
      "[85/91] TMX | q=TMX_quarterly_20260105.pdf | a=TMX_annual_20250929.pdf\n",
      "[88/91] WTM | q=WTM_quarterly_unknown.pdf | a=WTM_annual_unknown.pdf\n",
      "[89/91] WWI | q=WWI_annual_20260201.pdf | a=WWI_annual_20260201.pdf\n",
      "\n",
      "Rebuilt rows: 91 | hits: 25\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Quarterly_PDF</th>\n",
       "      <th>Annual_PDF</th>\n",
       "      <th>Resource_Definition_Flag</th>\n",
       "      <th>PFS_Flag</th>\n",
       "      <th>Evidence_Summary_50w</th>\n",
       "      <th>Evidence_Snippets</th>\n",
       "      <th>Notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAR</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>no_pdfs_in_folder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADG</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AGE</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>no_pdfs_in_folder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ALM</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>[QUARTERLY] The results of the drilling will f...</td>\n",
       "      <td>[QUARTERLY] The results of the drilling will f...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AM5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>no_pdfs_in_folder</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Ticker                                      Quarterly_PDF  \\\n",
       "0    AAR                                                      \n",
       "1    ADG  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "2    AGE                                                      \n",
       "3    ALM  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "4    AM5                                                      \n",
       "\n",
       "                                          Annual_PDF Resource_Definition_Flag  \\\n",
       "0                                                                           N   \n",
       "1  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...                        N   \n",
       "2                                                                           N   \n",
       "3  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...                        Y   \n",
       "4                                                                           N   \n",
       "\n",
       "  PFS_Flag                               Evidence_Summary_50w  \\\n",
       "0        N                                                      \n",
       "1        N                                                      \n",
       "2        N                                                      \n",
       "3        Y  [QUARTERLY] The results of the drilling will f...   \n",
       "4        N                                                      \n",
       "\n",
       "                                   Evidence_Snippets              Notes  \n",
       "0                                                     no_pdfs_in_folder  \n",
       "1                                                                        \n",
       "2                                                     no_pdfs_in_folder  \n",
       "3  [QUARTERLY] The results of the drilling will f...                     \n",
       "4                                                     no_pdfs_in_folder  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========================\n",
    "# RIU: REBUILD CSV from already-downloaded PDFs (NO crawling, NO downloading)\n",
    "# - Scans downloaded_reports/<TICKER>/*.pdf\n",
    "# - Picks latest quarterly + annual per ticker\n",
    "# - Extracts text (pypdf -> fallback PyMuPDF)\n",
    "# - Flags Resource Definition + PFS, writes riu_stage_flags_REBUILT.csv and _hits.csv\n",
    "# =========================\n",
    "\n",
    "import re\n",
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# --- PDF text extractors (try pypdf first, then fitz) ---\n",
    "def extract_pdf_text_pypdf(pdf_path: Path) -> str:\n",
    "    try:\n",
    "        from pypdf import PdfReader\n",
    "        reader = PdfReader(str(pdf_path))\n",
    "        parts = []\n",
    "        for p in reader.pages:\n",
    "            t = p.extract_text() or \"\"\n",
    "            if t.strip():\n",
    "                parts.append(t)\n",
    "        return \"\\n\".join(parts)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def extract_pdf_text_fitz(pdf_path: Path) -> str:\n",
    "    try:\n",
    "        import fitz  # PyMuPDF\n",
    "        doc = fitz.open(str(pdf_path))\n",
    "        parts = []\n",
    "        for page in doc:\n",
    "            t = page.get_text(\"text\") or \"\"\n",
    "            if t.strip():\n",
    "                parts.append(t)\n",
    "        doc.close()\n",
    "        return \"\\n\".join(parts)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def extract_pdf_text(pdf_path: Path) -> str:\n",
    "    txt = extract_pdf_text_pypdf(pdf_path)\n",
    "    if txt.strip():\n",
    "        return txt\n",
    "    return extract_pdf_text_fitz(pdf_path)\n",
    "\n",
    "# -------------------------\n",
    "# SETTINGS (EDIT THESE TWO)\n",
    "# -------------------------\n",
    "DOWNLOAD_DIR = Path(r\"C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULTING PTY LTD\\Documents\\01_BD\\96_2026_RIU_Conference_Perth\\downloaded_reports\")\n",
    "OUT_CSV_REBUILT = r\"C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULTING PTY LTD\\Documents\\01_BD\\96_2026_RIU_Conference_Perth\\riu_stage_flags_REBUILT.csv\"\n",
    "\n",
    "# -------------------------\n",
    "# KEYWORDS / HEURISTICS (same spirit as original)\n",
    "# -------------------------\n",
    "INTENT_RE = re.compile(\n",
    "    r\"\\b(will|plan(?:s|ned)?|intend(?:s|ed)?|to\\s+(?:commence|start|undertake|complete|deliver|progress|advance)|\"\n",
    "    r\"target(?:s|ing)?|scheduled|expected|underway|next\\s+quarter|H[12]|FY\\d{2})\\b\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "RESOURCE_RE_LIST = [\n",
    "    r\"\\bresource definition\\b\",\n",
    "    r\"\\bresource drilling\\b\",\n",
    "    r\"\\binfill drilling\\b\",\n",
    "    r\"\\bmineral resource\\b\",\n",
    "    r\"\\bresource estimate\\b\",\n",
    "    r\"\\bmaiden resource\\b\",\n",
    "    r\"\\bJORC\\b\",\n",
    "    r\"\\bMRE\\b\",\n",
    "    r\"\\bupgrade\\b.{0,40}\\bresource\\b\",\n",
    "]\n",
    "PFS_RE_LIST = [\n",
    "    r\"\\bpre[-\\s]?feasibility\\b\",\n",
    "    r\"\\bPFS\\b\",\n",
    "    r\"\\bfeasibility study\\b\",\n",
    "    r\"\\bDFS\\b\",\n",
    "    r\"\\bdefinitive feasibility\\b\",\n",
    "    r\"\\bscoping study\\b\",\n",
    "]\n",
    "\n",
    "RESOURCE_RE = re.compile(\"|\".join(RESOURCE_RE_LIST), re.IGNORECASE)\n",
    "PFS_RE      = re.compile(\"|\".join(PFS_RE_LIST), re.IGNORECASE)\n",
    "\n",
    "# -------------------------\n",
    "# UTILS\n",
    "# -------------------------\n",
    "def split_sentences(text: str) -> list[str]:\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "    text = re.sub(r\"\\n{2,}\", \"\\n\", text)\n",
    "    sents = re.split(r\"(?<=[\\.\\?\\!])\\s+|\\n+\", text)\n",
    "    return [s.strip() for s in sents if s and len(s.strip()) > 20]\n",
    "\n",
    "def trim_to_50_words(s: str) -> str:\n",
    "    words = s.split()\n",
    "    if len(words) <= 50:\n",
    "        return s.strip()\n",
    "    return \" \".join(words[:50]).strip() + \"\"\n",
    "\n",
    "def evidence_from_text(text: str) -> tuple[bool, bool, list[str]]:\n",
    "    sents = split_sentences(text)\n",
    "    resource_hits = []\n",
    "    pfs_hits = []\n",
    "\n",
    "    for s in sents:\n",
    "        has_intent = bool(INTENT_RE.search(s))\n",
    "        if RESOURCE_RE.search(s):\n",
    "            if has_intent or (\"program\" in s.lower()) or (\"next\" in s.lower()):\n",
    "                resource_hits.append(s)\n",
    "        if PFS_RE.search(s):\n",
    "            if has_intent or (\"study\" in s.lower()) or (\"engineering\" in s.lower()):\n",
    "                pfs_hits.append(s)\n",
    "\n",
    "    resource_hits = sorted(resource_hits, key=len)[:2]\n",
    "    pfs_hits      = sorted(pfs_hits, key=len)[:2]\n",
    "    return (len(resource_hits) > 0), (len(pfs_hits) > 0), (resource_hits + pfs_hits)\n",
    "\n",
    "def parse_date_from_filename(name: str) -> dt.datetime | None:\n",
    "    \"\"\"\n",
    "    Supports:\n",
    "      - 20250131 (YYYYMMDD)\n",
    "      - 2025-01-31 or 2025_01_31\n",
    "      - fallback: None\n",
    "    \"\"\"\n",
    "    s = name\n",
    "    m = re.search(r\"(20\\d{2})[.\\-_]?(0[1-9]|1[0-2])[.\\-_]?([0-2]\\d|3[01])\", s)\n",
    "    if m:\n",
    "        y, mo, d = int(m.group(1)), int(m.group(2)), int(m.group(3))\n",
    "        try:\n",
    "            return dt.datetime(y, mo, d)\n",
    "        except ValueError:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def doc_tag_from_filename(name: str) -> str:\n",
    "    n = name.lower()\n",
    "    # match your saved pattern: TKR_quarterly_YYYYMMDD.pdf etc.\n",
    "    if \"quarter\" in n or \"_qtr\" in n or \"4c\" in n:\n",
    "        return \"quarterly\"\n",
    "    if \"annual\" in n or \"4e\" in n or \"fy\" in n:\n",
    "        return \"annual\"\n",
    "    # if unknown, keep it as \"other\"\n",
    "    return \"other\"\n",
    "\n",
    "def pick_latest_pdf(files: list[Path]) -> Path | None:\n",
    "    if not files:\n",
    "        return None\n",
    "    def key(p: Path):\n",
    "        d = parse_date_from_filename(p.name)\n",
    "        mtime = dt.datetime.fromtimestamp(p.stat().st_mtime)\n",
    "        # prefer parsed date if present; else use modified time\n",
    "        return d or mtime\n",
    "    return sorted(files, key=key, reverse=True)[0]\n",
    "\n",
    "@dataclass\n",
    "class StageRow:\n",
    "    Ticker: str\n",
    "    Quarterly_PDF: str\n",
    "    Annual_PDF: str\n",
    "    Resource_Definition_Flag: str\n",
    "    PFS_Flag: str\n",
    "    Evidence_Summary_50w: str\n",
    "    Evidence_Snippets: str\n",
    "    Notes: str\n",
    "\n",
    "# -------------------------\n",
    "# MAIN: rebuild\n",
    "# -------------------------\n",
    "def rebuild_from_downloads(download_dir: Path) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    if not download_dir.exists():\n",
    "        raise FileNotFoundError(f\"DOWNLOAD_DIR not found: {download_dir}\")\n",
    "\n",
    "    rows = []\n",
    "    tickers = sorted([p for p in download_dir.iterdir() if p.is_dir()])\n",
    "\n",
    "    print(f\"Found ticker folders: {len(tickers)} in {download_dir}\")\n",
    "\n",
    "    for i, tdir in enumerate(tickers, start=1):\n",
    "        ticker = tdir.name.upper().strip()\n",
    "        pdfs = list(tdir.glob(\"*.pdf\"))\n",
    "        if not pdfs:\n",
    "            rows.append(asdict(StageRow(ticker, \"\", \"\", \"N\", \"N\", \"\", \"\", \"no_pdfs_in_folder\")))\n",
    "            continue\n",
    "\n",
    "        quarterly_files = [p for p in pdfs if doc_tag_from_filename(p.name) == \"quarterly\"]\n",
    "        annual_files    = [p for p in pdfs if doc_tag_from_filename(p.name) == \"annual\"]\n",
    "\n",
    "        # If your filenames don't contain quarterly/annual, fall back to \"newest 2\"\n",
    "        q_pdf = pick_latest_pdf(quarterly_files) if quarterly_files else None\n",
    "        a_pdf = pick_latest_pdf(annual_files) if annual_files else None\n",
    "\n",
    "        notes = []\n",
    "        if q_pdf is None and a_pdf is None:\n",
    "            # fallback: use two newest PDFs\n",
    "            newest = sorted(pdfs, key=lambda p: p.stat().st_mtime, reverse=True)[:2]\n",
    "            if newest:\n",
    "                q_pdf = newest[0]\n",
    "                a_pdf = newest[1] if len(newest) > 1 else newest[0]\n",
    "                notes.append(\"fallback_used_newest_2_pdfs\")\n",
    "        else:\n",
    "            if q_pdf is None and pdfs:\n",
    "                q_pdf = pick_latest_pdf(pdfs)\n",
    "                notes.append(\"no_quarterly_named_pdf_used_latest_any\")\n",
    "            if a_pdf is None and pdfs:\n",
    "                a_pdf = pick_latest_pdf(pdfs)\n",
    "                notes.append(\"no_annual_named_pdf_used_latest_any\")\n",
    "\n",
    "        print(f\"[{i}/{len(tickers)}] {ticker} | q={q_pdf.name if q_pdf else 'None'} | a={a_pdf.name if a_pdf else 'None'}\")\n",
    "\n",
    "        resource_flag = \"N\"\n",
    "        pfs_flag = \"N\"\n",
    "        snippets = []\n",
    "\n",
    "        def scan_one(p: Path, tag: str):\n",
    "            nonlocal resource_flag, pfs_flag, snippets, notes\n",
    "            if p is None:\n",
    "                return\n",
    "            txt = extract_pdf_text(p)\n",
    "            if not txt.strip():\n",
    "                notes.append(f\"no_text_{tag}\")\n",
    "                return\n",
    "            r_ok, p_ok, hits = evidence_from_text(txt)\n",
    "            if r_ok:\n",
    "                resource_flag = \"Y\"\n",
    "            if p_ok:\n",
    "                pfs_flag = \"Y\"\n",
    "            if hits:\n",
    "                snippets.extend([f\"[{tag.upper()}] {h}\" for h in hits])\n",
    "\n",
    "        scan_one(q_pdf, \"quarterly\")\n",
    "        scan_one(a_pdf, \"annual\")\n",
    "\n",
    "        if snippets:\n",
    "            summary = trim_to_50_words(\" \".join(snippets))\n",
    "            snippet_txt = \"\\n\".join(snippets[:6])\n",
    "        else:\n",
    "            summary = \"\"\n",
    "            snippet_txt = \"\"\n",
    "\n",
    "        rows.append(asdict(StageRow(\n",
    "            Ticker=ticker,\n",
    "            Quarterly_PDF=str(q_pdf) if q_pdf else \"\",\n",
    "            Annual_PDF=str(a_pdf) if a_pdf else \"\",\n",
    "            Resource_Definition_Flag=resource_flag,\n",
    "            PFS_Flag=pfs_flag,\n",
    "            Evidence_Summary_50w=summary,\n",
    "            Evidence_Snippets=snippet_txt,\n",
    "            Notes=\"; \".join(notes) if notes else \"\"\n",
    "        )))\n",
    "\n",
    "    df = pd.DataFrame(rows).sort_values(\"Ticker\").reset_index(drop=True)\n",
    "    hits = df[(df[\"Resource_Definition_Flag\"]==\"Y\") | (df[\"PFS_Flag\"]==\"Y\")].copy()\n",
    "    return df, hits\n",
    "\n",
    "# ---- RUN ----\n",
    "out_df, hits_df = rebuild_from_downloads(DOWNLOAD_DIR)\n",
    "\n",
    "print(\"\\nRebuilt rows:\", len(out_df), \"| hits:\", len(hits_df))\n",
    "out_df.head(5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a378297d-1716-4689-8296-d06285972bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULTING PTY LTD\\Documents\\01_BD\\96_2026_RIU_Conference_Perth\\riu_stage_flags_REBUILT.csv\n",
      "Saved: C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULTING PTY LTD\\Documents\\01_BD\\96_2026_RIU_Conference_Perth\\riu_stage_flags_REBUILT_hits.csv\n"
     ]
    }
   ],
   "source": [
    "tmp = OUT_CSV_REBUILT\n",
    "out_df.to_csv(tmp, index=False, encoding=\"utf-8\")\n",
    "hits_df.to_csv(tmp.replace(\".csv\", \"_hits.csv\"), index=False, encoding=\"utf-8\")\n",
    "print(\"Saved:\", tmp)\n",
    "print(\"Saved:\", tmp.replace(\".csv\", \"_hits.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c032348-3747-401c-882e-508c4c74301c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrying 46 companies out of 91 total.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/46 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[RETRY] FFM - FireFly Metals Ltd (fireflymetals.com.au)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|                                                                                 | 1/46 [00:15<11:21, 15.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[RETRY] RXL - Rox Resources Ltd (roxresources.com.au)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|                                                                               | 2/46 [00:23<08:23, 11.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[RETRY] NMG - New Murchison Gold Ltd (newmurchgold.com.au)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|                                                                             | 3/46 [00:33<07:27, 10.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[RETRY] MEI - Meteoric Resources NL (meteoric.com.au)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|                                                                           | 4/46 [00:59<11:39, 16.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[RETRY] AAR - Astral Resources NL (astralresources.com.au)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|                                                                          | 5/46 [01:32<15:23, 22.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved: C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULTING PTY LTD\\Documents\\01_BD\\96_2026_RIU_Conference_Perth\\riu_stage_flags_FINAL.checkpoint_retry.csv\n",
      "\n",
      "[RETRY] BTR - Brightstar Resources Ltd (brightstarresources.com.au)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|                                                                        | 6/46 [02:14<19:28, 29.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[RETRY] CTM - Centaurus Metals Ltd (centaurus.com.au)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|                                                                      | 7/46 [02:23<14:41, 22.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[RETRY] RHI - Red Hill Minerals Ltd (redhillminerals.com.au)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|                                                                    | 8/46 [02:25<10:09, 16.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[RETRY] IVR - Investigator Silver Ltd (investres.com.au)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|                                                                  | 9/46 [02:56<12:47, 20.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[RETRY] BM1 - Ballard Mining Ltd (ballardmining.com.au)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|                                                                | 10/46 [03:25<13:55, 23.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved: C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULTING PTY LTD\\Documents\\01_BD\\96_2026_RIU_Conference_Perth\\riu_stage_flags_FINAL.checkpoint_retry.csv\n",
      "\n",
      "[RETRY] HRZ - Horizon Minerals Ltd (horizonminerals.com.au)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|                                                              | 11/46 [04:02<16:04, 27.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[RETRY] PLA - Pacific Lime & Cement Ltd (placltd.com)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|                                                            | 12/46 [04:11<12:22, 21.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[RETRY] AUE - Aurum Resources Ltd (aurumres.com.au)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|                                                          | 13/46 [04:27<11:07, 20.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[RETRY] DLI - Delta Lithium Ltd (deltalithium.com.au)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|                                                         | 14/46 [04:57<12:17, 23.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[RETRY] GBZ - GBM Resources Ltd (gbmr.com.au)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|                                                       | 15/46 [05:09<10:07, 19.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved: C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULTING PTY LTD\\Documents\\01_BD\\96_2026_RIU_Conference_Perth\\riu_stage_flags_FINAL.checkpoint_retry.csv\n",
      "\n",
      "[RETRY] DRE - Dreadnought Resources Ltd (dreadnoughtresources.com.au)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|                                                     | 16/46 [05:40<11:31, 23.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[RETRY] AGE - Alligator Energy Ltd (alligatorenergy.com.au)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|                                                   | 17/46 [06:11<12:17, 25.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[RETRY] KSN - Kingston Resources Ltd (kingstonresources.com.au)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|                                                  | 18/46 [06:22<09:49, 21.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[RETRY] TM1 - Terra Metals Ltd (terrametals.com.au)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|                                                | 19/46 [07:23<14:54, 33.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[RETRY] PTR - PTR Minerals Ltd (ptrminerals.com.au)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|                                              | 20/46 [07:59<14:45, 34.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved: C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULTING PTY LTD\\Documents\\01_BD\\96_2026_RIU_Conference_Perth\\riu_stage_flags_FINAL.checkpoint_retry.csv\n",
      "\n",
      "[RETRY] LM8 - Lunnon Metals Ltd (lunnonmetals.com.au)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|                                            | 21/46 [09:26<20:47, 49.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[RETRY] YRL - Yandal Resources Ltd (yandalresources.com.au)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|                                          | 22/46 [09:58<17:51, 44.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[RETRY] BSX - Blackstone Minerals Ltd (blackstoneminerals.com.au)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|                                         | 23/46 [10:29<15:29, 40.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[RETRY] COD - Coda Minerals Ltd (codaminerals.com)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|                                       | 24/46 [11:00<13:48, 37.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[RETRY] CRS - Caprice Resources Ltd (capriceresources.com)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|                                     | 25/46 [12:23<17:53, 51.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved: C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULTING PTY LTD\\Documents\\01_BD\\96_2026_RIU_Conference_Perth\\riu_stage_flags_FINAL.checkpoint_retry.csv\n",
      "\n",
      "[RETRY] RTR - Rumble Resources Ltd (rumbleresources.com.au)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|                                   | 26/46 [12:32<12:50, 38.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[RETRY] M79 - Mammoth Minerals Ltd (firetailresources.com.au)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|                                 | 27/46 [12:52<10:29, 33.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[RETRY] BCA - Black Canyon Ltd (blackcanyon.com.au)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|                                | 28/46 [14:47<17:18, 57.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[RETRY] AR3 - Australian Rare Earths Ltd (ar3.com.au)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|                              | 29/46 [15:20<14:11, 50.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[RETRY] OR3 - Ore Resources Ltd (oreresources.com.au)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|                            | 30/46 [15:51<11:52, 44.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved: C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULTING PTY LTD\\Documents\\01_BD\\96_2026_RIU_Conference_Perth\\riu_stage_flags_FINAL.checkpoint_retry.csv\n",
      "\n",
      "[RETRY] PGO - Pacgold Ltd (pacgold.com.au)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|                          | 31/46 [16:02<08:34, 34.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[RETRY] GA8 - GoldArc Resources Ltd (goldarcres.com.au)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|                         | 32/46 [16:11<06:13, 26.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[RETRY] IPT - Impact Minerals Ltd (impactminerals.com.au)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|                       | 33/46 [16:38<05:48, 26.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[RETRY] DYM - Dynamic Metals Ltd (dynamicmetals.com.au)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|                     | 34/46 [17:28<06:48, 34.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[RETRY] WIN - WIN Metals Ltd (winmetals.com.au)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|                   | 35/46 [18:01<06:10, 33.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved: C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULTING PTY LTD\\Documents\\01_BD\\96_2026_RIU_Conference_Perth\\riu_stage_flags_FINAL.checkpoint_retry.csv\n",
      "\n",
      "[RETRY] PGM - Platina Resources Ltd (platinaresources.com.au)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|                 | 36/46 [18:17<04:41, 28.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[RETRY] TG6 - TG Metals Ltd (tgmetals.com.au)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|                | 37/46 [19:01<04:58, 33.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[RETRY] SGA - Sarytogan Graphite Ltd (sarytogangraphite.com.au)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|              | 38/46 [19:16<03:39, 27.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[RETRY] VMS - Venari Minerals NL (venariminerals.com)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|            | 39/46 [19:50<03:26, 29.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[RETRY] LAT - Latitude 66 Ltd (lat66.com)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|          | 40/46 [20:06<02:33, 25.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved: C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULTING PTY LTD\\Documents\\01_BD\\96_2026_RIU_Conference_Perth\\riu_stage_flags_FINAL.checkpoint_retry.csv\n",
      "\n",
      "[RETRY] IVG - InVert Graphite Ltd (invertgraphite.com.au)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|         | 41/46 [20:14<01:40, 20.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[RETRY] FMR - FMR Resources Ltd (fmrresources.com.au)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|       | 42/46 [20:23<01:07, 16.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[RETRY] ATT - Altitude Minerals Ltd (coppersearch.com.au)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|     | 43/46 [20:38<00:49, 16.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[RETRY] YAR - Yari Minerals Ltd (yariminerals.com.au)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|   | 44/46 [21:13<00:43, 21.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[RETRY] AM5 - Antares Metals Ltd (antaresmetals.com.au)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%| | 45/46 [21:45<00:24, 24.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved: C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULTING PTY LTD\\Documents\\01_BD\\96_2026_RIU_Conference_Perth\\riu_stage_flags_FINAL.checkpoint_retry.csv\n",
      "\n",
      "[RETRY] SLM - Solis Minerals Ltd (solisminerals.com)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 46/46 [22:18<00:00, 29.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wrote retry-only results: C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULTING PTY LTD\\Documents\\01_BD\\96_2026_RIU_Conference_Perth\\riu_stage_flags_FINAL_retry_only.csv\n",
      "Wrote FINAL compiled results: C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULTING PTY LTD\\Documents\\01_BD\\96_2026_RIU_Conference_Perth\\riu_stage_flags_FINAL.csv\n",
      "Wrote hits-only file: C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULTING PTY LTD\\Documents\\01_BD\\96_2026_RIU_Conference_Perth\\riu_stage_flags_FINAL_hits.csv  (hits=26)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Company</th>\n",
       "      <th>Website</th>\n",
       "      <th>Quarterly_URL</th>\n",
       "      <th>Annual_URL</th>\n",
       "      <th>Quarterly_PDF</th>\n",
       "      <th>Annual_PDF</th>\n",
       "      <th>Resource_Definition_Flag</th>\n",
       "      <th>PFS_Flag</th>\n",
       "      <th>Evidence_Summary_50w</th>\n",
       "      <th>Evidence_Snippets</th>\n",
       "      <th>Notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RMS</td>\n",
       "      <td>Ramelius Resources Ltd</td>\n",
       "      <td>rameliusresources.com.au</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>[QUARTERLY] mineral resource and there is no c...</td>\n",
       "      <td>[QUARTERLY] mineral resource and there is no c...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHN</td>\n",
       "      <td>Chalice Mining Ltd</td>\n",
       "      <td>chalicemining.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>[QUARTERLY] 1.1.2 Pre-Feasibility Study [QUART...</td>\n",
       "      <td>[QUARTERLY] 1.1.2 Pre-Feasibility Study\\n[QUAR...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SVL</td>\n",
       "      <td>Silver Mines Ltd</td>\n",
       "      <td>silvermines.com.au</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>[QUARTERLY] targets were prepared by a Compete...</td>\n",
       "      <td>[QUARTERLY] targets were prepared by a Compete...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AZY</td>\n",
       "      <td>Antipa Minerals Ltd</td>\n",
       "      <td>antipaminerals.com.au</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>[QUARTERLY] intersections, confirming the targ...</td>\n",
       "      <td>[QUARTERLY] intersections, confirming the targ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>WWI</td>\n",
       "      <td>West Wits Mining Ltd</td>\n",
       "      <td>westwitsmining.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>[QUARTERLY] 2022 Scoping Study: 5 Development ...</td>\n",
       "      <td>[QUARTERLY] 2022 Scoping Study: 5 Development ...</td>\n",
       "      <td>no_quarterly_named_pdf_used_latest_any</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>BGD</td>\n",
       "      <td>Barton Gold Holdings Ltd</td>\n",
       "      <td>bartongold.com.au</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>[QUARTERLY] it intends to prioritise for explo...</td>\n",
       "      <td>[QUARTERLY] it intends to prioritise for explo...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>HCH</td>\n",
       "      <td>Hot Chili Ltd</td>\n",
       "      <td>hotchili.net.au</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>[QUARTERLY]  commencement of the Costa Fuego ...</td>\n",
       "      <td>[QUARTERLY]  commencement of the Costa Fuego ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>RNU</td>\n",
       "      <td>Renascor Resources Ltd</td>\n",
       "      <td>renascor.com.au</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>[QUARTERLY] Siviour Definitive Feasibility Stu...</td>\n",
       "      <td>[QUARTERLY] Siviour Definitive Feasibility Stu...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>HRN</td>\n",
       "      <td>Horizon Gold Ltd</td>\n",
       "      <td>horizongold.com.au</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>[QUARTERLY] In March 2016 a scoping study for ...</td>\n",
       "      <td>[QUARTERLY] In March 2016 a scoping study for ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>ARL</td>\n",
       "      <td>Ardea Resources Ltd</td>\n",
       "      <td>ardearesources.com.au</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>[QUARTERLY]  Complete the infill resource def...</td>\n",
       "      <td>[QUARTERLY]  Complete the infill resource def...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>MMA</td>\n",
       "      <td>Maronan Metals Ltd</td>\n",
       "      <td>maronanmetals.com.au</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>[QUARTERLY] An internal review using the previ...</td>\n",
       "      <td>[QUARTERLY] An internal review using the previ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>MAT</td>\n",
       "      <td>Matsa Resources Ltd</td>\n",
       "      <td>matsa.com.au</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>[QUARTERLY] Definitive Feasibility Study. [QUA...</td>\n",
       "      <td>[QUARTERLY] Definitive Feasibility Study.\\n[QU...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>HAS</td>\n",
       "      <td>Hastings Technology Metals Ltd</td>\n",
       "      <td>hastingstechmetals.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>[QUARTERLY] from the infill drilling programme...</td>\n",
       "      <td>[QUARTERLY] from the infill drilling programme...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>PC2</td>\n",
       "      <td>PC Gold Ltd</td>\n",
       "      <td>pcgold.com.au</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>[QUARTERLY] resource estimation and JORC explo...</td>\n",
       "      <td>[QUARTERLY] resource estimation and JORC explo...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>DEV</td>\n",
       "      <td>DevEx Resources Ltd</td>\n",
       "      <td>devexresources.com.au</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>[QUARTERLY] hence will need to be reported afr...</td>\n",
       "      <td>[QUARTERLY] hence will need to be reported afr...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>FLG</td>\n",
       "      <td>Flagship Minerals Ltd</td>\n",
       "      <td>flagshipminerals.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>[QUARTERLY] insufficient exploration to estima...</td>\n",
       "      <td>[QUARTERLY] insufficient exploration to estima...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>CRS</td>\n",
       "      <td>Caprice Resources Ltd</td>\n",
       "      <td>capriceresources.com</td>\n",
       "      <td>https://www.capriceresources.com/crs/wp-conten...</td>\n",
       "      <td>https://www.capriceresources.com/crs/wp-conten...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>[OTHER] evaluation work (Scoping Study / Feasi...</td>\n",
       "      <td>[OTHER] evaluation work (Scoping Study / Feasi...</td>\n",
       "      <td>sitemap_pages=120; visited_pages=35; pdf_candi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>RDM</td>\n",
       "      <td>Red Metal Ltd</td>\n",
       "      <td>redmetal.com.au</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>[ANNUAL] an infill drilling program comprising...</td>\n",
       "      <td>[ANNUAL] an infill drilling program comprising...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>CXU</td>\n",
       "      <td>Cauldron Energy Ltd</td>\n",
       "      <td>cauldronenergy.com.au</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>[QUARTERLY] Our current prioritiesDelivery of...</td>\n",
       "      <td>[QUARTERLY] Our current prioritiesDelivery of...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>ORD</td>\n",
       "      <td>Ordell Minerals Ltd</td>\n",
       "      <td>ordellminerals.com.au</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>[ANNUAL] Tranche 4 Will vest and become exerci...</td>\n",
       "      <td>[ANNUAL] Tranche 4 Will vest and become exerci...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Ticker                         Company                   Website  \\\n",
       "0     RMS          Ramelius Resources Ltd  rameliusresources.com.au   \n",
       "2     CHN              Chalice Mining Ltd         chalicemining.com   \n",
       "6     SVL                Silver Mines Ltd        silvermines.com.au   \n",
       "9     AZY             Antipa Minerals Ltd     antipaminerals.com.au   \n",
       "13    WWI            West Wits Mining Ltd        westwitsmining.com   \n",
       "16    BGD        Barton Gold Holdings Ltd         bartongold.com.au   \n",
       "18    HCH                   Hot Chili Ltd           hotchili.net.au   \n",
       "24    RNU          Renascor Resources Ltd           renascor.com.au   \n",
       "30    HRN                Horizon Gold Ltd        horizongold.com.au   \n",
       "32    ARL             Ardea Resources Ltd     ardearesources.com.au   \n",
       "35    MMA              Maronan Metals Ltd      maronanmetals.com.au   \n",
       "36    MAT             Matsa Resources Ltd              matsa.com.au   \n",
       "37    HAS  Hastings Technology Metals Ltd    hastingstechmetals.com   \n",
       "38    PC2                     PC Gold Ltd             pcgold.com.au   \n",
       "39    DEV             DevEx Resources Ltd     devexresources.com.au   \n",
       "46    FLG           Flagship Minerals Ltd      flagshipminerals.com   \n",
       "48    CRS           Caprice Resources Ltd      capriceresources.com   \n",
       "49    RDM                   Red Metal Ltd           redmetal.com.au   \n",
       "53    CXU             Cauldron Energy Ltd     cauldronenergy.com.au   \n",
       "59    ORD             Ordell Minerals Ltd     ordellminerals.com.au   \n",
       "\n",
       "                                        Quarterly_URL  \\\n",
       "0                                                 NaN   \n",
       "2                                                 NaN   \n",
       "6                                                 NaN   \n",
       "9                                                 NaN   \n",
       "13                                                NaN   \n",
       "16                                                NaN   \n",
       "18                                                NaN   \n",
       "24                                                NaN   \n",
       "30                                                NaN   \n",
       "32                                                NaN   \n",
       "35                                                NaN   \n",
       "36                                                NaN   \n",
       "37                                                NaN   \n",
       "38                                                NaN   \n",
       "39                                                NaN   \n",
       "46                                                NaN   \n",
       "48  https://www.capriceresources.com/crs/wp-conten...   \n",
       "49                                                NaN   \n",
       "53                                                NaN   \n",
       "59                                                NaN   \n",
       "\n",
       "                                           Annual_URL  \\\n",
       "0                                                 NaN   \n",
       "2                                                 NaN   \n",
       "6                                                 NaN   \n",
       "9                                                 NaN   \n",
       "13                                                NaN   \n",
       "16                                                NaN   \n",
       "18                                                NaN   \n",
       "24                                                NaN   \n",
       "30                                                NaN   \n",
       "32                                                NaN   \n",
       "35                                                NaN   \n",
       "36                                                NaN   \n",
       "37                                                NaN   \n",
       "38                                                NaN   \n",
       "39                                                NaN   \n",
       "46                                                NaN   \n",
       "48  https://www.capriceresources.com/crs/wp-conten...   \n",
       "49                                                NaN   \n",
       "53                                                NaN   \n",
       "59                                                NaN   \n",
       "\n",
       "                                        Quarterly_PDF  \\\n",
       "0   C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "2   C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "6   C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "9   C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "13  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "16  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "18  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "24  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "30  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "32  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "35  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "36  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "37  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "38  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "39  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "46  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "48                                                NaN   \n",
       "49  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "53  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "59  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "\n",
       "                                           Annual_PDF  \\\n",
       "0   C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "2   C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "6   C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "9   C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "13  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "16  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "18  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "24  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "30  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "32  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "35  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "36  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "37  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "38  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "39  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "46  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "48                                                NaN   \n",
       "49  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "53  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "59  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "\n",
       "   Resource_Definition_Flag PFS_Flag  \\\n",
       "0                         Y        Y   \n",
       "2                         N        Y   \n",
       "6                         Y        Y   \n",
       "9                         Y        Y   \n",
       "13                        N        Y   \n",
       "16                        Y        Y   \n",
       "18                        N        Y   \n",
       "24                        N        Y   \n",
       "30                        N        Y   \n",
       "32                        Y        Y   \n",
       "35                        Y        Y   \n",
       "36                        N        Y   \n",
       "37                        Y        Y   \n",
       "38                        Y        Y   \n",
       "39                        Y        Y   \n",
       "46                        Y        Y   \n",
       "48                        Y        Y   \n",
       "49                        Y        N   \n",
       "53                        Y        Y   \n",
       "59                        Y        Y   \n",
       "\n",
       "                                 Evidence_Summary_50w  \\\n",
       "0   [QUARTERLY] mineral resource and there is no c...   \n",
       "2   [QUARTERLY] 1.1.2 Pre-Feasibility Study [QUART...   \n",
       "6   [QUARTERLY] targets were prepared by a Compete...   \n",
       "9   [QUARTERLY] intersections, confirming the targ...   \n",
       "13  [QUARTERLY] 2022 Scoping Study: 5 Development ...   \n",
       "16  [QUARTERLY] it intends to prioritise for explo...   \n",
       "18  [QUARTERLY]  commencement of the Costa Fuego ...   \n",
       "24  [QUARTERLY] Siviour Definitive Feasibility Stu...   \n",
       "30  [QUARTERLY] In March 2016 a scoping study for ...   \n",
       "32  [QUARTERLY]  Complete the infill resource def...   \n",
       "35  [QUARTERLY] An internal review using the previ...   \n",
       "36  [QUARTERLY] Definitive Feasibility Study. [QUA...   \n",
       "37  [QUARTERLY] from the infill drilling programme...   \n",
       "38  [QUARTERLY] resource estimation and JORC explo...   \n",
       "39  [QUARTERLY] hence will need to be reported afr...   \n",
       "46  [QUARTERLY] insufficient exploration to estima...   \n",
       "48  [OTHER] evaluation work (Scoping Study / Feasi...   \n",
       "49  [ANNUAL] an infill drilling program comprising...   \n",
       "53  [QUARTERLY] Our current prioritiesDelivery of...   \n",
       "59  [ANNUAL] Tranche 4 Will vest and become exerci...   \n",
       "\n",
       "                                    Evidence_Snippets  \\\n",
       "0   [QUARTERLY] mineral resource and there is no c...   \n",
       "2   [QUARTERLY] 1.1.2 Pre-Feasibility Study\\n[QUAR...   \n",
       "6   [QUARTERLY] targets were prepared by a Compete...   \n",
       "9   [QUARTERLY] intersections, confirming the targ...   \n",
       "13  [QUARTERLY] 2022 Scoping Study: 5 Development ...   \n",
       "16  [QUARTERLY] it intends to prioritise for explo...   \n",
       "18  [QUARTERLY]  commencement of the Costa Fuego ...   \n",
       "24  [QUARTERLY] Siviour Definitive Feasibility Stu...   \n",
       "30  [QUARTERLY] In March 2016 a scoping study for ...   \n",
       "32  [QUARTERLY]  Complete the infill resource def...   \n",
       "35  [QUARTERLY] An internal review using the previ...   \n",
       "36  [QUARTERLY] Definitive Feasibility Study.\\n[QU...   \n",
       "37  [QUARTERLY] from the infill drilling programme...   \n",
       "38  [QUARTERLY] resource estimation and JORC explo...   \n",
       "39  [QUARTERLY] hence will need to be reported afr...   \n",
       "46  [QUARTERLY] insufficient exploration to estima...   \n",
       "48  [OTHER] evaluation work (Scoping Study / Feasi...   \n",
       "49  [ANNUAL] an infill drilling program comprising...   \n",
       "53  [QUARTERLY] Our current prioritiesDelivery of...   \n",
       "59  [ANNUAL] Tranche 4 Will vest and become exerci...   \n",
       "\n",
       "                                                Notes  \n",
       "0                                                 NaN  \n",
       "2                                                 NaN  \n",
       "6                                                 NaN  \n",
       "9                                                 NaN  \n",
       "13             no_quarterly_named_pdf_used_latest_any  \n",
       "16                                                NaN  \n",
       "18                                                NaN  \n",
       "24                                                NaN  \n",
       "30                                                NaN  \n",
       "32                                                NaN  \n",
       "35                                                NaN  \n",
       "36                                                NaN  \n",
       "37                                                NaN  \n",
       "38                                                NaN  \n",
       "39                                                NaN  \n",
       "46                                                NaN  \n",
       "48  sitemap_pages=120; visited_pages=35; pdf_candi...  \n",
       "49                                                NaN  \n",
       "53                                                NaN  \n",
       "59                                                NaN  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# RIU - PASS 2 (Retry only the \"missing/no-info\" companies)\n",
    "# - Reads:\n",
    "#     1) companies_with_website.csv  (Ticker, Company, Website)\n",
    "#     2) riu_stage_flags_REBUILT.csv (your 91-row rebuilt results)\n",
    "# - Identifies rows that look \"missing\" (no evidence, no PDFs, or no_text errors)\n",
    "# - Crawls ONLY those websites, finds likely PDFs, downloads a few, extracts text,\n",
    "#   flags Resource Definition / PFS, summarizes evidence\n",
    "# - Merges back into a compiled 91-row output\n",
    "#\n",
    "# Notes:\n",
    "# - Uses PyMuPDF (fitz) for extraction (quiet, robust).\n",
    "# - Does NOT require admin rights.\n",
    "# - OCR is OPTIONAL (off by default). If you enable it, it avoids --clean so no \"unpaper\" required.\n",
    "# ============================================================\n",
    "\n",
    "import re\n",
    "import time\n",
    "import subprocess\n",
    "import datetime as dt\n",
    "from dataclasses import dataclass, asdict\n",
    "from pathlib import Path\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# PATHS (EDIT THESE)\n",
    "# -------------------------\n",
    "COMPANIES_CSV = r\"C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULTING PTY LTD\\Documents\\01_BD\\96_2026_RIU_Conference_Perth\\companies_with_website.csv\"\n",
    "REBUILT_CSV   = r\"C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULTING PTY LTD\\Documents\\01_BD\\96_2026_RIU_Conference_Perth\\riu_stage_flags_REBUILT.csv\"\n",
    "\n",
    "# Where to put NEW downloads from this retry run\n",
    "RETRY_DOWNLOAD_DIR = Path(r\"C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULTING PTY LTD\\Documents\\01_BD\\96_2026_RIU_Conference_Perth\\downloaded_reports_retry\")\n",
    "RETRY_DEBUG_DIR    = Path(r\"C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULTING PTY LTD\\Documents\\01_BD\\96_2026_RIU_Conference_Perth\\debug_pages_retry\")\n",
    "\n",
    "# Output (write to a NEW filename to avoid OneDrive/Excel lock pain)\n",
    "OUT_FINAL_CSV = r\"C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULTING PTY LTD\\Documents\\01_BD\\96_2026_RIU_Conference_Perth\\riu_stage_flags_FINAL.csv\"\n",
    "OUT_RETRY_ONLY_CSV = OUT_FINAL_CSV.replace(\".csv\", \"_retry_only.csv\")\n",
    "OUT_FINAL_HITS_CSV = OUT_FINAL_CSV.replace(\".csv\", \"_hits.csv\")\n",
    "\n",
    "# -------------------------\n",
    "# RUNTIME SETTINGS\n",
    "# -------------------------\n",
    "REQUEST_TIMEOUT = 25\n",
    "SLEEP_S = 0.4\n",
    "MAX_PAGES_TO_VISIT = 35       # per company crawl budget\n",
    "MAX_SITEMAP_URLS = 4000       # per company (filtered)\n",
    "MAX_SITEMAP_CHILDREN = 25\n",
    "MAX_PDFS_TO_SCAN = 3          # scan top N candidate PDFs\n",
    "\n",
    "# Optional OCR (OFF by default; only used when extracted text is empty)\n",
    "# If you enable this, it will call the `ocrmypdf` executable.\n",
    "DO_OCR_IF_NO_TEXT = False\n",
    "\n",
    "USER_AGENT = (\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "    \"(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# KEYWORDS / HEURISTICS\n",
    "# -------------------------\n",
    "INTENT_RE = re.compile(\n",
    "    r\"\\b(will|plan(?:s|ned)?|intend(?:s|ed)?|to\\s+(?:commence|start|undertake|complete|deliver|progress|advance)|\"\n",
    "    r\"target(?:s|ing)?|scheduled|expected|underway|next\\s+quarter)\\b\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "RESOURCE_RE_LIST = [\n",
    "    r\"\\bresource definition\\b\",\n",
    "    r\"\\bresource drilling\\b\",\n",
    "    r\"\\binfill drilling\\b\",\n",
    "    r\"\\bmineral resource\\b\",\n",
    "    r\"\\bresource estimate\\b\",\n",
    "    r\"\\bmaiden resource\\b\",\n",
    "    r\"\\bJORC\\b\",\n",
    "    r\"\\bMRE\\b\",\n",
    "    r\"\\bupgrade\\b.{0,40}\\bresource\\b\",\n",
    "]\n",
    "PFS_RE_LIST = [\n",
    "    r\"\\bpre[-\\s]?feasibility\\b\",\n",
    "    r\"\\bPFS\\b\",\n",
    "    r\"\\bfeasibility study\\b\",\n",
    "    r\"\\bDFS\\b\",\n",
    "    r\"\\bdefinitive feasibility\\b\",\n",
    "    r\"\\bscoping study\\b\",\n",
    "]\n",
    "\n",
    "RESOURCE_RE = re.compile(\"|\".join(RESOURCE_RE_LIST), re.IGNORECASE)\n",
    "PFS_RE      = re.compile(\"|\".join(PFS_RE_LIST), re.IGNORECASE)\n",
    "\n",
    "QUARTERLY_HINT = re.compile(r\"(quarterly|appendix\\s*4c|activities\\s+report|quarter\\s+report|3[-\\s]?month|appendix\\s*4d)\", re.IGNORECASE)\n",
    "ANNUAL_HINT    = re.compile(r\"(annual\\s+report|appendix\\s*4e|year\\s+end|full\\s+year|financial\\s+report|annual\\s+financial|annual\\s+results)\", re.IGNORECASE)\n",
    "\n",
    "REPORT_URL_HINT = re.compile(\n",
    "    r\"(investor|investors|asx|announce|announcement|release|news|media|report|results|financial|presentation|quarter|appendix|4c|4e)\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "PDF_EXT_RE = re.compile(r\"\\.pdf(\\?|$)\", re.IGNORECASE)\n",
    "PRESENTATION_HINT = re.compile(r\"(presentation|investor[-\\s_]*pres|deck|slides)\", re.IGNORECASE)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# UTILITIES\n",
    "# -------------------------\n",
    "def is_blank(x) -> bool:\n",
    "    if x is None:\n",
    "        return True\n",
    "    s = str(x)\n",
    "    return (s.strip() == \"\") or (s.strip().lower() == \"nan\")\n",
    "\n",
    "def safe_filename(s: str) -> str:\n",
    "    return re.sub(r\"[^a-zA-Z0-9._-]+\", \"_\", s)[:180]\n",
    "\n",
    "def normalize_site(site: str) -> str:\n",
    "    site = (site or \"\").strip()\n",
    "    if not site:\n",
    "        return \"\"\n",
    "    if not site.startswith((\"http://\", \"https://\")):\n",
    "        site = \"https://\" + site\n",
    "    return site.rstrip(\"/\")\n",
    "\n",
    "def same_domain(base: str, url: str) -> bool:\n",
    "    try:\n",
    "        b = urlparse(base)\n",
    "        u = urlparse(url)\n",
    "        return (b.netloc.lower() == u.netloc.lower()) or (u.netloc.lower().endswith(b.netloc.lower()))\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def make_session() -> requests.Session:\n",
    "    s = requests.Session()\n",
    "    s.headers.update({\"User-Agent\": USER_AGENT, \"Accept-Language\": \"en-US,en;q=0.9\"})\n",
    "    return s\n",
    "\n",
    "def fetch_text(sess: requests.Session, url: str) -> tuple[int, str, str]:\n",
    "    try:\n",
    "        r = sess.get(url, timeout=REQUEST_TIMEOUT, allow_redirects=True)\n",
    "        ctype = (r.headers.get(\"content-type\") or \"\").lower()\n",
    "        return r.status_code, ctype, r.text or \"\"\n",
    "    except Exception:\n",
    "        return 0, \"\", \"\"\n",
    "\n",
    "def download_file(sess: requests.Session, url: str, outpath: Path) -> bool:\n",
    "    try:\n",
    "        r = sess.get(url, timeout=REQUEST_TIMEOUT, stream=True, allow_redirects=True)\n",
    "        if r.status_code >= 400:\n",
    "            return False\n",
    "        ctype = (r.headers.get(\"content-type\") or \"\").lower()\n",
    "        if (\"pdf\" not in ctype) and (not PDF_EXT_RE.search(url)):\n",
    "            return False\n",
    "        outpath.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(outpath, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=1024 * 64):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "        return outpath.exists() and outpath.stat().st_size > 10_000\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def extract_pdf_text_fitz(pdf_path: Path) -> str:\n",
    "    try:\n",
    "        doc = fitz.open(str(pdf_path))\n",
    "        parts = []\n",
    "        for page in doc:\n",
    "            t = page.get_text(\"text\") or \"\"\n",
    "            if t.strip():\n",
    "                parts.append(t)\n",
    "        doc.close()\n",
    "        return \"\\n\".join(parts)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def try_ocr(pdf_in: Path, pdf_out: Path) -> bool:\n",
    "    \"\"\"\n",
    "    OCR optional. Avoids --clean so no 'unpaper' needed.\n",
    "    Requires: ocrmypdf executable + tesseract + ghostscript available.\n",
    "    \"\"\"\n",
    "    cmd = [\n",
    "        \"ocrmypdf\",\n",
    "        \"--deskew\",\n",
    "        \"--rotate-pages\",\n",
    "        \"--optimize\", \"1\",\n",
    "        \"--skip-text\",\n",
    "        str(pdf_in),\n",
    "        str(pdf_out),\n",
    "    ]\n",
    "    try:\n",
    "        p = subprocess.run(cmd, capture_output=True, text=True)\n",
    "        return (p.returncode == 0) and pdf_out.exists() and pdf_out.stat().st_size > 10_000\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def split_sentences(text: str) -> list[str]:\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "    text = re.sub(r\"\\n{2,}\", \"\\n\", text)\n",
    "    sents = re.split(r\"(?<=[\\.\\?\\!])\\s+|\\n+\", text)\n",
    "    return [s.strip() for s in sents if s and len(s.strip()) > 20]\n",
    "\n",
    "def trim_to_50_words(s: str) -> str:\n",
    "    words = s.split()\n",
    "    if len(words) <= 50:\n",
    "        return s.strip()\n",
    "    return \" \".join(words[:50]).strip() + \"\"\n",
    "\n",
    "def safe_dt(y, m, d):\n",
    "    try:\n",
    "        return dt.datetime(y, m, d)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def guess_date_from_text(s: str):\n",
    "    s = s or \"\"\n",
    "\n",
    "    # YYYY-MM-DD or YYYY/MM/DD\n",
    "    m = re.search(r\"(20\\d{2})[-/](\\d{1,2})[-/](\\d{1,2})\", s)\n",
    "    if m:\n",
    "        y, mo, d = map(int, m.groups())\n",
    "        x = safe_dt(y, mo, d)\n",
    "        if x:\n",
    "            return x\n",
    "\n",
    "    # DD MMM YYYY\n",
    "    m = re.search(r\"(\\d{1,2})\\s+(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s+(20\\d{2})\", s, re.IGNORECASE)\n",
    "    if m:\n",
    "        d = int(m.group(1)); mon = m.group(2).lower(); y = int(m.group(3))\n",
    "        months = {\"jan\":1,\"feb\":2,\"mar\":3,\"apr\":4,\"may\":5,\"jun\":6,\"jul\":7,\"aug\":8,\"sep\":9,\"oct\":10,\"nov\":11,\"dec\":12}\n",
    "        x = safe_dt(y, months[mon[:3]], d)\n",
    "        if x:\n",
    "            return x\n",
    "\n",
    "    # MMM YYYY\n",
    "    m = re.search(r\"(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s+(20\\d{2})\", s, re.IGNORECASE)\n",
    "    if m:\n",
    "        mon = m.group(1).lower(); y = int(m.group(2))\n",
    "        months = {\"jan\":1,\"feb\":2,\"mar\":3,\"apr\":4,\"may\":5,\"jun\":6,\"jul\":7,\"aug\":8,\"sep\":9,\"oct\":10,\"nov\":11,\"dec\":12}\n",
    "        return dt.datetime(y, months[mon[:3]], 1)\n",
    "\n",
    "    # YYYYMMDD in URL/file (validate!)\n",
    "    m = re.search(r\"(20\\d{2})(\\d{2})(\\d{2})\", s)\n",
    "    if m:\n",
    "        y, mo, d = map(int, m.groups())\n",
    "        x = safe_dt(y, mo, d)\n",
    "        if x:\n",
    "            return x\n",
    "\n",
    "    return None\n",
    "\n",
    "def extract_pdf_links_from_html(base_url: str, html: str) -> list[dict]:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    out = []\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = (a.get(\"href\") or \"\").strip()\n",
    "        if not href:\n",
    "            continue\n",
    "        absu = urljoin(base_url, href)\n",
    "        if not PDF_EXT_RE.search(absu):\n",
    "            continue\n",
    "        label = \" \".join(a.get_text(\" \", strip=True).split())\n",
    "        out.append({\n",
    "            \"url\": absu,\n",
    "            \"label\": label,\n",
    "            \"date_guess\": guess_date_from_text(label + \" \" + absu),\n",
    "            \"source_page\": base_url\n",
    "        })\n",
    "    return out\n",
    "\n",
    "def get_sitemaps_from_robots(sess: requests.Session, site: str) -> list[str]:\n",
    "    robots = site + \"/robots.txt\"\n",
    "    st, ct, txt = fetch_text(sess, robots)\n",
    "    time.sleep(SLEEP_S)\n",
    "    if st >= 400 or not txt:\n",
    "        return []\n",
    "    sitemaps = []\n",
    "    for line in txt.splitlines():\n",
    "        if line.lower().startswith(\"sitemap:\"):\n",
    "            sm = line.split(\":\", 1)[1].strip()\n",
    "            if sm:\n",
    "                sitemaps.append(sm)\n",
    "    return sitemaps\n",
    "\n",
    "def get_sitemap_urls(sess: requests.Session, site: str) -> list[str]:\n",
    "    starts = [\"/sitemap.xml\", \"/sitemap_index.xml\", \"/wp-sitemap.xml\"]\n",
    "    all_urls = []\n",
    "\n",
    "    # robots first (often points to the real sitemap)\n",
    "    candidate_sitemaps = get_sitemaps_from_robots(sess, site) + [site + p for p in starts]\n",
    "\n",
    "    seen_sm = set()\n",
    "    for sm in candidate_sitemaps:\n",
    "        if sm in seen_sm:\n",
    "            continue\n",
    "        seen_sm.add(sm)\n",
    "\n",
    "        status, ctype, txt = fetch_text(sess, sm)\n",
    "        time.sleep(SLEEP_S)\n",
    "        if status >= 400 or not txt:\n",
    "            continue\n",
    "        if (\"xml\" not in ctype) and (\"<urlset\" not in txt.lower()) and (\"<sitemapindex\" not in txt.lower()):\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(txt, \"xml\")\n",
    "\n",
    "        # sitemap index?\n",
    "        sitemap_locs = [loc.get_text(strip=True) for loc in soup.find_all(\"sitemap\") for loc in loc.find_all(\"loc\")]\n",
    "        if sitemap_locs:\n",
    "            for child in sitemap_locs[:MAX_SITEMAP_CHILDREN]:\n",
    "                st2, ct2, t2 = fetch_text(sess, child)\n",
    "                time.sleep(SLEEP_S)\n",
    "                if st2 >= 400 or not t2:\n",
    "                    continue\n",
    "                s2 = BeautifulSoup(t2, \"xml\")\n",
    "                locs = [loc.get_text(strip=True) for loc in s2.find_all(\"loc\")]\n",
    "                for u in locs:\n",
    "                    if REPORT_URL_HINT.search(u):\n",
    "                        all_urls.append(u)\n",
    "                if len(all_urls) >= MAX_SITEMAP_URLS:\n",
    "                    return all_urls[:MAX_SITEMAP_URLS]\n",
    "        else:\n",
    "            # plain sitemap\n",
    "            locs = [loc.get_text(strip=True) for loc in soup.find_all(\"loc\")]\n",
    "            for u in locs:\n",
    "                if REPORT_URL_HINT.search(u):\n",
    "                    all_urls.append(u)\n",
    "            if all_urls:\n",
    "                return all_urls[:MAX_SITEMAP_URLS]\n",
    "\n",
    "    return all_urls[:MAX_SITEMAP_URLS]\n",
    "\n",
    "def discover_report_pages(site: str) -> list[str]:\n",
    "    paths = [\n",
    "        \"/investors\", \"/investor\", \"/investor-centre\", \"/investor-centre/asx-announcements\",\n",
    "        \"/asx-announcements\", \"/announcements\", \"/asx-releases\", \"/asx-reports\",\n",
    "        \"/reports\", \"/financial-reports\", \"/results\", \"/news\", \"/media\",\n",
    "        \"/investors/asx-announcements\", \"/investors/announcements\", \"/investors/reports\",\n",
    "    ]\n",
    "    return [site + p for p in paths]\n",
    "\n",
    "def score_pdf_candidate(c: dict) -> float:\n",
    "    label = (c.get(\"label\") or \"\")\n",
    "    url = (c.get(\"url\") or \"\")\n",
    "    text = f\"{label} {url}\"\n",
    "    score = 0.0\n",
    "\n",
    "    if QUARTERLY_HINT.search(text):\n",
    "        score += 50\n",
    "    if ANNUAL_HINT.search(text):\n",
    "        score += 45\n",
    "    if REPORT_URL_HINT.search(text):\n",
    "        score += 15\n",
    "    if PRESENTATION_HINT.search(text):\n",
    "        score -= 10  # keep it, but lower priority\n",
    "\n",
    "    d = c.get(\"date_guess\")\n",
    "    if isinstance(d, dt.datetime):\n",
    "        # recency bump: more recent => higher\n",
    "        days_ago = (dt.datetime.utcnow() - d).days\n",
    "        score += max(0, 25 - min(25, days_ago / 30.0))  # gentle bump\n",
    "\n",
    "    return score\n",
    "\n",
    "def evidence_from_text(text: str) -> tuple[bool, bool, list[str]]:\n",
    "    sents = split_sentences(text)\n",
    "    resource_hits, pfs_hits = [], []\n",
    "\n",
    "    for s in sents:\n",
    "        has_intent = bool(INTENT_RE.search(s))\n",
    "        if RESOURCE_RE.search(s) and (has_intent or (\"program\" in s.lower()) or (\"next\" in s.lower())):\n",
    "            resource_hits.append(s)\n",
    "        if PFS_RE.search(s) and (has_intent or (\"study\" in s.lower()) or (\"engineering\" in s.lower())):\n",
    "            pfs_hits.append(s)\n",
    "\n",
    "    resource_hits = sorted(resource_hits, key=len)[:2]\n",
    "    pfs_hits      = sorted(pfs_hits, key=len)[:2]\n",
    "    return (len(resource_hits) > 0), (len(pfs_hits) > 0), (resource_hits + pfs_hits)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# INPUT READERS\n",
    "# -------------------------\n",
    "def read_companies(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, sep=None, engine=\"python\")\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    colmap = {c.lower(): c for c in df.columns}\n",
    "\n",
    "    def pick(*names):\n",
    "        for n in names:\n",
    "            if n in colmap:\n",
    "                return colmap[n]\n",
    "        return None\n",
    "\n",
    "    c_company = pick(\"company\", \"name\", \"co\", \"issuer\")\n",
    "    c_ticker  = pick(\"ticker\", \"asx\", \"code\", \"symbol\")\n",
    "    c_web     = pick(\"website\", \"web\", \"url\", \"site\")\n",
    "\n",
    "    if not c_company or not c_ticker or not c_web:\n",
    "        raise ValueError(f\"Missing required columns. Found: {list(df.columns)}. Need Company/Ticker/Website (or similar).\")\n",
    "\n",
    "    df = df.rename(columns={c_company: \"Company\", c_ticker: \"Ticker\", c_web: \"Website\"})\n",
    "    df[\"Ticker\"] = df[\"Ticker\"].astype(str).str.strip().str.upper()\n",
    "    df[\"Company\"] = df[\"Company\"].astype(str).str.strip()\n",
    "    df[\"Website\"] = df[\"Website\"].astype(str).str.strip()\n",
    "    return df\n",
    "\n",
    "def read_rebuilt(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    df[\"Ticker\"] = df[\"Ticker\"].astype(str).str.strip().str.upper()\n",
    "    return df\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# MISSING/RETRY DETECTOR\n",
    "# -------------------------\n",
    "def needs_retry(row) -> bool:\n",
    "    notes = (str(row.get(\"Notes\") or \"\")).lower()\n",
    "    summ  = str(row.get(\"Evidence_Summary_50w\") or \"\")\n",
    "    snip  = str(row.get(\"Evidence_Snippets\") or \"\")\n",
    "    rflag = str(row.get(\"Resource_Definition_Flag\") or \"N\").upper()\n",
    "    pflag = str(row.get(\"PFS_Flag\") or \"N\").upper()\n",
    "    qpdf  = str(row.get(\"Quarterly_PDF\") or \"\")\n",
    "    apdf  = str(row.get(\"Annual_PDF\") or \"\")\n",
    "\n",
    "    no_evidence = (is_blank(summ) and is_blank(snip) and rflag != \"Y\" and pflag != \"Y\")\n",
    "    no_pdfs = (is_blank(qpdf) and is_blank(apdf))\n",
    "    bad_text = (\"no_text\" in notes) or (\"extract_failed\" in notes)\n",
    "    missing = (\"no_pdfs_in_folder\" in notes) or (\"no_pdf\" in notes) or (\"download_failed\" in notes)\n",
    "\n",
    "    return no_evidence and (no_pdfs or bad_text or missing)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# WEB RETRY CORE\n",
    "# -------------------------\n",
    "@dataclass\n",
    "class RetryRow:\n",
    "    Ticker: str\n",
    "    Company: str\n",
    "    Website: str\n",
    "    Quarterly_URL: str\n",
    "    Annual_URL: str\n",
    "    Quarterly_PDF: str\n",
    "    Annual_PDF: str\n",
    "    Resource_Definition_Flag: str\n",
    "    PFS_Flag: str\n",
    "    Evidence_Summary_50w: str\n",
    "    Evidence_Snippets: str\n",
    "    Notes: str\n",
    "\n",
    "def crawl_for_pdfs(sess: requests.Session, site: str, debug_dir: Path) -> tuple[list[dict], list[str]]:\n",
    "    notes = []\n",
    "    debug_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    pages = get_sitemap_urls(sess, site)\n",
    "    if pages:\n",
    "        notes.append(f\"sitemap_pages={len(pages)}\")\n",
    "    else:\n",
    "        notes.append(\"no_sitemap_or_no_filtered_urls\")\n",
    "        pages = discover_report_pages(site)\n",
    "\n",
    "    visited = set()\n",
    "    queue = []\n",
    "\n",
    "    for u in pages[:250]:\n",
    "        if REPORT_URL_HINT.search(u):\n",
    "            queue.append(u)\n",
    "\n",
    "    pdf_candidates = []\n",
    "\n",
    "    while queue and len(visited) < MAX_PAGES_TO_VISIT:\n",
    "        url = queue.pop(0)\n",
    "        if url in visited:\n",
    "            continue\n",
    "        if not same_domain(site, url):\n",
    "            continue\n",
    "        visited.add(url)\n",
    "\n",
    "        st, ct, html = fetch_text(sess, url)\n",
    "        time.sleep(SLEEP_S)\n",
    "        if st >= 400 or not html:\n",
    "            continue\n",
    "\n",
    "        # Save debug HTML snapshot (optional but helpful)\n",
    "        try:\n",
    "            (debug_dir / f\"{len(visited):02d}_{safe_filename(url)}.html\").write_text(html, encoding=\"utf-8\", errors=\"ignore\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # Extract PDFs\n",
    "        pdfs = extract_pdf_links_from_html(url, html)\n",
    "        pdf_candidates.extend(pdfs)\n",
    "\n",
    "        # Enqueue more internal links (limited)\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        new_links = 0\n",
    "        for a in soup.find_all(\"a\", href=True):\n",
    "            href = (a.get(\"href\") or \"\").strip()\n",
    "            if not href:\n",
    "                continue\n",
    "            absu = urljoin(url, href)\n",
    "            if absu in visited:\n",
    "                continue\n",
    "            if not same_domain(site, absu):\n",
    "                continue\n",
    "            if REPORT_URL_HINT.search(absu):\n",
    "                queue.append(absu)\n",
    "                new_links += 1\n",
    "                if new_links >= 25:\n",
    "                    break\n",
    "\n",
    "    # Dedup\n",
    "    seen = set()\n",
    "    dedup = []\n",
    "    for c in pdf_candidates:\n",
    "        u = c.get(\"url\")\n",
    "        if not u or u in seen:\n",
    "            continue\n",
    "        seen.add(u)\n",
    "        if not c.get(\"date_guess\"):\n",
    "            c[\"date_guess\"] = guess_date_from_text((c.get(\"label\",\"\") + \" \" + u))\n",
    "        c[\"score\"] = score_pdf_candidate(c)\n",
    "        dedup.append(c)\n",
    "\n",
    "    notes.append(f\"visited_pages={len(visited)}\")\n",
    "    notes.append(f\"pdf_candidates_raw={len(pdf_candidates)}\")\n",
    "    notes.append(f\"pdf_candidates_dedup={len(dedup)}\")\n",
    "\n",
    "    # Sort best-first\n",
    "    dedup.sort(key=lambda x: x.get(\"score\", 0.0), reverse=True)\n",
    "    return dedup, notes\n",
    "\n",
    "def pick_quarterly_annual(cands: list[dict]) -> tuple[dict|None, dict|None, str]:\n",
    "    \"\"\"\n",
    "    Try to pick best quarterly + annual. If we can't, pick top two distinct PDFs.\n",
    "    \"\"\"\n",
    "    if not cands:\n",
    "        return None, None, \"no_pdf_candidates\"\n",
    "\n",
    "    q = None\n",
    "    a = None\n",
    "\n",
    "    for c in cands:\n",
    "        t = f\"{c.get('label','')} {c.get('url','')}\"\n",
    "        if (q is None) and QUARTERLY_HINT.search(t):\n",
    "            q = c\n",
    "        if (a is None) and ANNUAL_HINT.search(t):\n",
    "            a = c\n",
    "        if q and a:\n",
    "            break\n",
    "\n",
    "    note = \"\"\n",
    "    if q and a and q.get(\"url\") == a.get(\"url\"):\n",
    "        a = None\n",
    "\n",
    "    if (q is None) and (a is None):\n",
    "        note = \"no_quarterly_or_annual_hints_used_top2\"\n",
    "        q = cands[0]\n",
    "        a = cands[1] if len(cands) > 1 else None\n",
    "    elif q is None and a is not None:\n",
    "        note = \"no_quarterly_hint_used_best_other_as_quarterly\"\n",
    "        # pick another best distinct\n",
    "        q = next((x for x in cands if x.get(\"url\") != a.get(\"url\")), a)\n",
    "    elif a is None and q is not None:\n",
    "        note = \"no_annual_hint_used_best_other_as_annual\"\n",
    "        a = next((x for x in cands if x.get(\"url\") != q.get(\"url\")), None)\n",
    "\n",
    "    return q, a, note\n",
    "\n",
    "def process_retry_company(ticker: str, company: str, website: str) -> RetryRow:\n",
    "    tkr = (ticker or \"\").strip().upper()\n",
    "    site = normalize_site(website)\n",
    "\n",
    "    if not tkr or not site:\n",
    "        return RetryRow(tkr, company, website, \"\", \"\", \"\", \"\", \"N\", \"N\", \"\", \"\", \"missing ticker/website\")\n",
    "\n",
    "    sess = make_session()\n",
    "    out_dir = RETRY_DOWNLOAD_DIR / tkr\n",
    "    dbg_dir = RETRY_DEBUG_DIR / tkr\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    dbg_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    cands, notes = crawl_for_pdfs(sess, site, dbg_dir)\n",
    "    if not cands:\n",
    "        return RetryRow(tkr, company, website, \"\", \"\", \"\", \"\", \"N\", \"N\", \"\", \"\", \"; \".join(notes + [\"no_pdf_links_found\"]))\n",
    "\n",
    "    q_doc, a_doc, pick_note = pick_quarterly_annual(cands)\n",
    "    if pick_note:\n",
    "        notes.append(pick_note)\n",
    "\n",
    "    # We'll scan up to MAX_PDFS_TO_SCAN from the TOP of the candidate list\n",
    "    # (this improves hit rate when websites label things inconsistently)\n",
    "    scan_list = []\n",
    "    if q_doc:\n",
    "        scan_list.append(q_doc)\n",
    "    if a_doc and (a_doc.get(\"url\") not in {x.get(\"url\") for x in scan_list}):\n",
    "        scan_list.append(a_doc)\n",
    "    # fill with additional top candidates\n",
    "    for c in cands:\n",
    "        if len(scan_list) >= MAX_PDFS_TO_SCAN:\n",
    "            break\n",
    "        if c.get(\"url\") and c.get(\"url\") not in {x.get(\"url\") for x in scan_list}:\n",
    "            scan_list.append(c)\n",
    "\n",
    "    resource_flag = \"N\"\n",
    "    pfs_flag = \"N\"\n",
    "    snippets = []\n",
    "\n",
    "    quarterly_url = q_doc.get(\"url\") if q_doc else \"\"\n",
    "    annual_url    = a_doc.get(\"url\") if a_doc else \"\"\n",
    "\n",
    "    quarterly_pdf = \"\"\n",
    "    annual_pdf = \"\"\n",
    "\n",
    "    for idx, doc in enumerate(scan_list, start=1):\n",
    "        url = doc.get(\"url\",\"\")\n",
    "        if not url:\n",
    "            continue\n",
    "\n",
    "        # Decide a tag for filename\n",
    "        tag = \"other\"\n",
    "        t = f\"{doc.get('label','')} {url}\"\n",
    "        if QUARTERLY_HINT.search(t):\n",
    "            tag = \"quarterly\"\n",
    "        elif ANNUAL_HINT.search(t):\n",
    "            tag = \"annual\"\n",
    "\n",
    "        d = doc.get(\"date_guess\")\n",
    "        datestr = d.strftime(\"%Y%m%d\") if isinstance(d, dt.datetime) else \"unknown\"\n",
    "        pdf_path = out_dir / f\"{tkr}_{tag}_{datestr}_{idx}.pdf\"\n",
    "\n",
    "        ok = True\n",
    "        if not pdf_path.exists() or pdf_path.stat().st_size < 10_000:\n",
    "            ok = download_file(sess, url, pdf_path)\n",
    "            time.sleep(SLEEP_S)\n",
    "\n",
    "        if not ok:\n",
    "            notes.append(f\"download_failed_{tag}_{idx}\")\n",
    "            continue\n",
    "        else:\n",
    "            notes.append(f\"downloaded_{tag}_{idx}\")\n",
    "\n",
    "        txt = extract_pdf_text_fitz(pdf_path)\n",
    "\n",
    "        # OCR option if no text\n",
    "        if DO_OCR_IF_NO_TEXT and not txt.strip():\n",
    "            ocr_out = pdf_path.with_name(pdf_path.stem + \"_ocr.pdf\")\n",
    "            if try_ocr(pdf_path, ocr_out):\n",
    "                notes.append(f\"ocr_ok_{tag}_{idx}\")\n",
    "                txt = extract_pdf_text_fitz(ocr_out)\n",
    "                # Prefer OCR'd file for paths\n",
    "                pdf_path = ocr_out\n",
    "            else:\n",
    "                notes.append(f\"ocr_failed_{tag}_{idx}\")\n",
    "\n",
    "        if not txt.strip():\n",
    "            notes.append(f\"no_text_{tag}_{idx}\")\n",
    "            continue\n",
    "\n",
    "        r_ok, p_ok, hits = evidence_from_text(txt)\n",
    "        if r_ok:\n",
    "            resource_flag = \"Y\"\n",
    "        if p_ok:\n",
    "            pfs_flag = \"Y\"\n",
    "\n",
    "        if hits:\n",
    "            for h in hits:\n",
    "                snippets.append(f\"[{tag.upper()}] {h}\")\n",
    "\n",
    "        # capture best pdf path for quarterly/annual if it matches tag or if blank\n",
    "        if tag == \"quarterly\" and not quarterly_pdf:\n",
    "            quarterly_pdf = str(pdf_path)\n",
    "        if tag == \"annual\" and not annual_pdf:\n",
    "            annual_pdf = str(pdf_path)\n",
    "\n",
    "        # stop early if both flags hit\n",
    "        if resource_flag == \"Y\" and pfs_flag == \"Y\":\n",
    "            break\n",
    "\n",
    "    # If we never got a quarterly/annual pdf path, fall back to first scanned\n",
    "    if not quarterly_pdf and scan_list:\n",
    "        quarterly_pdf = str((RETRY_DOWNLOAD_DIR / tkr / f\"{tkr}_other_unknown_1.pdf\")) if False else \"\"\n",
    "    # (We keep it blank unless we have a real file path above; avoids lying.)\n",
    "\n",
    "    summary = \"\"\n",
    "    snippet_txt = \"\"\n",
    "    if snippets:\n",
    "        combined = \" \".join(snippets)\n",
    "        summary = trim_to_50_words(combined)\n",
    "        snippet_txt = \"\\n\".join(snippets[:8])\n",
    "\n",
    "    return RetryRow(\n",
    "        Ticker=tkr,\n",
    "        Company=company,\n",
    "        Website=website,\n",
    "        Quarterly_URL=quarterly_url,\n",
    "        Annual_URL=annual_url,\n",
    "        Quarterly_PDF=quarterly_pdf,\n",
    "        Annual_PDF=annual_pdf,\n",
    "        Resource_Definition_Flag=resource_flag,\n",
    "        PFS_Flag=pfs_flag,\n",
    "        Evidence_Summary_50w=summary,\n",
    "        Evidence_Snippets=snippet_txt,\n",
    "        Notes=\"; \".join(notes)\n",
    "    )\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# MERGE + RUN\n",
    "# -------------------------\n",
    "def merge_results(companies_df: pd.DataFrame, rebuilt_df: pd.DataFrame, retry_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Start from companies list (91). Add rebuilt fields. Then overwrite/augment with retry results where present.\n",
    "    \"\"\"\n",
    "    base = companies_df.copy()\n",
    "\n",
    "    # join rebuilt on Ticker\n",
    "    base = base.merge(rebuilt_df, on=\"Ticker\", how=\"left\", suffixes=(\"\", \"_rebuilt\"))\n",
    "\n",
    "    # join retry on Ticker\n",
    "    base = base.merge(retry_df, on=[\"Ticker\", \"Company\", \"Website\"], how=\"left\", suffixes=(\"\", \"_retry\"))\n",
    "\n",
    "    # For each output column, if retry has a non-blank value, use it; else keep rebuilt.\n",
    "    cols = [\n",
    "        \"Quarterly_URL\", \"Annual_URL\",\n",
    "        \"Quarterly_PDF\", \"Annual_PDF\",\n",
    "        \"Resource_Definition_Flag\", \"PFS_Flag\",\n",
    "        \"Evidence_Summary_50w\", \"Evidence_Snippets\",\n",
    "        \"Notes\"\n",
    "    ]\n",
    "\n",
    "    for c in cols:\n",
    "        c_retry = c + \"_retry\"\n",
    "        if c_retry in base.columns:\n",
    "            base[c] = base[c].where(base[c_retry].apply(is_blank), base[c_retry])\n",
    "\n",
    "    # Clean up helper columns\n",
    "    drop_cols = [c for c in base.columns if c.endswith(\"_rebuilt\") or c.endswith(\"_retry\")]\n",
    "    base = base.drop(columns=drop_cols, errors=\"ignore\")\n",
    "\n",
    "    # Ensure consistent column order\n",
    "    desired = [\"Ticker\",\"Company\",\"Website\",\"Quarterly_URL\",\"Annual_URL\",\"Quarterly_PDF\",\"Annual_PDF\",\n",
    "               \"Resource_Definition_Flag\",\"PFS_Flag\",\"Evidence_Summary_50w\",\"Evidence_Snippets\",\"Notes\"]\n",
    "    for c in desired:\n",
    "        if c not in base.columns:\n",
    "            base[c] = \"\"\n",
    "    base = base[desired]\n",
    "\n",
    "    return base\n",
    "\n",
    "def run_retry_pass():\n",
    "    RETRY_DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    RETRY_DEBUG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    companies = read_companies(COMPANIES_CSV)\n",
    "    rebuilt = read_rebuilt(REBUILT_CSV)\n",
    "\n",
    "    # Ensure rebuilt has all expected columns\n",
    "    for col in [\"Quarterly_PDF\",\"Annual_PDF\",\"Resource_Definition_Flag\",\"PFS_Flag\",\"Evidence_Summary_50w\",\"Evidence_Snippets\",\"Notes\"]:\n",
    "        if col not in rebuilt.columns:\n",
    "            rebuilt[col] = \"\"\n",
    "\n",
    "    # Combine for retry detection (needs website + company)\n",
    "    combined = companies.merge(rebuilt, on=\"Ticker\", how=\"left\")\n",
    "\n",
    "    # Identify the missing/no-info set\n",
    "    retry_set = combined[combined.apply(needs_retry, axis=1)].copy()\n",
    "    print(f\"Retrying {len(retry_set)} companies out of {len(companies)} total.\\n\")\n",
    "\n",
    "    retry_rows = []\n",
    "    checkpoint_path = Path(OUT_FINAL_CSV).with_suffix(\".checkpoint_retry.csv\")\n",
    "\n",
    "    for _, r in tqdm(retry_set.iterrows(), total=len(retry_set)):\n",
    "        tkr = r[\"Ticker\"]\n",
    "        co  = r[\"Company\"]\n",
    "        web = r[\"Website\"]\n",
    "\n",
    "        print(f\"\\n[RETRY] {tkr} - {co} ({web})\")\n",
    "        row = process_retry_company(tkr, co, web)\n",
    "        retry_rows.append(asdict(row))\n",
    "\n",
    "        # checkpoint every ~5 companies (so crashes don't wipe progress)\n",
    "        if len(retry_rows) % 5 == 0:\n",
    "            pd.DataFrame(retry_rows).to_csv(checkpoint_path, index=False, encoding=\"utf-8\")\n",
    "            print(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "    retry_df = pd.DataFrame(retry_rows)\n",
    "    retry_df.to_csv(OUT_RETRY_ONLY_CSV, index=False, encoding=\"utf-8\")\n",
    "    print(f\"\\nWrote retry-only results: {OUT_RETRY_ONLY_CSV}\")\n",
    "\n",
    "    final_df = merge_results(companies, rebuilt, retry_df)\n",
    "    final_df.to_csv(OUT_FINAL_CSV, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Wrote FINAL compiled results: {OUT_FINAL_CSV}\")\n",
    "\n",
    "    hits = final_df[(final_df[\"Resource_Definition_Flag\"]==\"Y\") | (final_df[\"PFS_Flag\"]==\"Y\")].copy()\n",
    "    hits.to_csv(OUT_FINAL_HITS_CSV, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Wrote hits-only file: {OUT_FINAL_HITS_CSV}  (hits={len(hits)})\")\n",
    "\n",
    "    return final_df, retry_df, hits\n",
    "\n",
    "\n",
    "# ---- RUN IT ----\n",
    "final_df, retry_df, hits_df = run_retry_pass()\n",
    "hits_df.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa78bb04-cc1a-4fc9-bad5-eb3943b4b64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 91/91 [1:21:19<00:00, 53.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULTING PTY LTD\\Documents\\01_BD\\96_2026_RIU_Conference_Perth\\riu_scoping_feasibility_FINAL.csv\n",
      "Saved hits: C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULTING PTY LTD\\Documents\\01_BD\\96_2026_RIU_Conference_Perth\\riu_scoping_feasibility_FINAL_hits.csv (hits=29/91)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Company</th>\n",
       "      <th>Website</th>\n",
       "      <th>Local_PDF_Count</th>\n",
       "      <th>Local_Scoping_Flag</th>\n",
       "      <th>Local_Feasibility_Flag</th>\n",
       "      <th>Local_Evidence_Summary_50w</th>\n",
       "      <th>Local_Evidence_Snippets</th>\n",
       "      <th>Local_Evidence_PDFs</th>\n",
       "      <th>Web_Checked</th>\n",
       "      <th>...</th>\n",
       "      <th>Scoping_Flag</th>\n",
       "      <th>Feasibility_Flag</th>\n",
       "      <th>Evidence_Summary_50w</th>\n",
       "      <th>Evidence_Snippets</th>\n",
       "      <th>Resource_Definition_Flag</th>\n",
       "      <th>PFS_Flag</th>\n",
       "      <th>Quarterly_URL</th>\n",
       "      <th>Annual_URL</th>\n",
       "      <th>Quarterly_PDF</th>\n",
       "      <th>Annual_PDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RMS</td>\n",
       "      <td>Ramelius Resources Ltd</td>\n",
       "      <td>rameliusresources.com.au</td>\n",
       "      <td>2</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>[LOCAL] deposit was optimised and a Scoping St...</td>\n",
       "      <td>[LOCAL] deposit was optimised and a Scoping St...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>N</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>[LOCAL] deposit was optimised and a Scoping St...</td>\n",
       "      <td>[LOCAL] deposit was optimised and a Scoping St...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHN</td>\n",
       "      <td>Chalice Mining Ltd</td>\n",
       "      <td>chalicemining.com</td>\n",
       "      <td>2</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>[LOCAL] the Scoping Study testwork phase. [LOC...</td>\n",
       "      <td>[LOCAL] the Scoping Study testwork phase.\\n[LO...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>N</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>[LOCAL] the Scoping Study testwork phase. [LOC...</td>\n",
       "      <td>[LOCAL] the Scoping Study testwork phase.\\n[LO...</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SVL</td>\n",
       "      <td>Silver Mines Ltd</td>\n",
       "      <td>silvermines.com.au</td>\n",
       "      <td>2</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>[LOCAL] in the Kramer Hills Project, Lustrum G...</td>\n",
       "      <td>[LOCAL] in the Kramer Hills Project, Lustrum G...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>N</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>[LOCAL] in the Kramer Hills Project, Lustrum G...</td>\n",
       "      <td>[LOCAL] in the Kramer Hills Project, Lustrum G...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>STK</td>\n",
       "      <td>Strickland Metals Ltd</td>\n",
       "      <td>stricklandmetals.com.au</td>\n",
       "      <td>2</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Y</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>[WEB] Developers / near-producers are progress...</td>\n",
       "      <td>[WEB] Developers / near-producers are progress...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AZY</td>\n",
       "      <td>Antipa Minerals Ltd</td>\n",
       "      <td>antipaminerals.com.au</td>\n",
       "      <td>2</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>[LOCAL] confirming the Scoping Study metallurg...</td>\n",
       "      <td>[LOCAL] confirming the Scoping Study metallurg...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>N</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>[LOCAL] confirming the Scoping Study metallurg...</td>\n",
       "      <td>[LOCAL] confirming the Scoping Study metallurg...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>MAU</td>\n",
       "      <td>Magnetic Resources NL</td>\n",
       "      <td>magres.com.au</td>\n",
       "      <td>2</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Y</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>[WEB] We expect the updated Feasibility will a...</td>\n",
       "      <td>[WEB] We expect the updated Feasibility will a...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>WWI</td>\n",
       "      <td>West Wits Mining Ltd</td>\n",
       "      <td>westwitsmining.com</td>\n",
       "      <td>1</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>[LOCAL] 2022 Scoping Study: 5 Development stag...</td>\n",
       "      <td>[LOCAL] 2022 Scoping Study: 5 Development stag...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>N</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>[LOCAL] 2022 Scoping Study: 5 Development stag...</td>\n",
       "      <td>[LOCAL] 2022 Scoping Study: 5 Development stag...</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>BGD</td>\n",
       "      <td>Barton Gold Holdings Ltd</td>\n",
       "      <td>bartongold.com.au</td>\n",
       "      <td>2</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>[LOCAL] and a Pre-Feasibility Study. [LOCAL] c...</td>\n",
       "      <td>[LOCAL] and a Pre-Feasibility Study.\\n[LOCAL] ...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>N</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>[LOCAL] and a Pre-Feasibility Study. [LOCAL] c...</td>\n",
       "      <td>[LOCAL] and a Pre-Feasibility Study.\\n[LOCAL] ...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>HCH</td>\n",
       "      <td>Hot Chili Ltd</td>\n",
       "      <td>hotchili.net.au</td>\n",
       "      <td>2</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>[LOCAL] These funds place us in a strong posit...</td>\n",
       "      <td>[LOCAL] These funds place us in a strong posit...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>N</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>[LOCAL] These funds place us in a strong posit...</td>\n",
       "      <td>[LOCAL] These funds place us in a strong posit...</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>RNU</td>\n",
       "      <td>Renascor Resources Ltd</td>\n",
       "      <td>renascor.com.au</td>\n",
       "      <td>2</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>[LOCAL] Siviour Definitive Feasibility Study P...</td>\n",
       "      <td>[LOCAL] Siviour Definitive Feasibility Study P...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>N</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>[LOCAL] Siviour Definitive Feasibility Study P...</td>\n",
       "      <td>[LOCAL] Siviour Definitive Feasibility Study P...</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>HRN</td>\n",
       "      <td>Horizon Gold Ltd</td>\n",
       "      <td>horizongold.com.au</td>\n",
       "      <td>2</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>[LOCAL] In March 2016 a scoping study for a fr...</td>\n",
       "      <td>[LOCAL] In March 2016 a scoping study for a fr...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>N</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>[LOCAL] In March 2016 a scoping study for a fr...</td>\n",
       "      <td>[LOCAL] In March 2016 a scoping study for a fr...</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>ARL</td>\n",
       "      <td>Ardea Resources Ltd</td>\n",
       "      <td>ardearesources.com.au</td>\n",
       "      <td>2</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>[LOCAL] Definitive Feasibility Study, Goongarr...</td>\n",
       "      <td>[LOCAL] Definitive Feasibility Study, Goongarr...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>N</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>[LOCAL] Definitive Feasibility Study, Goongarr...</td>\n",
       "      <td>[LOCAL] Definitive Feasibility Study, Goongarr...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>MMA</td>\n",
       "      <td>Maronan Metals Ltd</td>\n",
       "      <td>maronanmetals.com.au</td>\n",
       "      <td>2</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>[LOCAL] Work is well advanced on the PEA or sc...</td>\n",
       "      <td>[LOCAL] Work is well advanced on the PEA or sc...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>N</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>[LOCAL] Work is well advanced on the PEA or sc...</td>\n",
       "      <td>[LOCAL] Work is well advanced on the PEA or sc...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>MAT</td>\n",
       "      <td>Matsa Resources Ltd</td>\n",
       "      <td>matsa.com.au</td>\n",
       "      <td>2</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>[LOCAL] two Scoping Studies and Linden publish...</td>\n",
       "      <td>[LOCAL] two Scoping Studies and Linden publish...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>N</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>[LOCAL] two Scoping Studies and Linden publish...</td>\n",
       "      <td>[LOCAL] two Scoping Studies and Linden publish...</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>HAS</td>\n",
       "      <td>Hastings Technology Metals Ltd</td>\n",
       "      <td>hastingstechmetals.com</td>\n",
       "      <td>2</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>[LOCAL] technical feasibility are demonstrated...</td>\n",
       "      <td>[LOCAL] technical feasibility are demonstrated...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>N</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>[LOCAL] technical feasibility are demonstrated...</td>\n",
       "      <td>[LOCAL] technical feasibility are demonstrated...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>PC2</td>\n",
       "      <td>PC Gold Ltd</td>\n",
       "      <td>pcgold.com.au</td>\n",
       "      <td>2</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>[LOCAL] Feasibility Study Underway  Targeting...</td>\n",
       "      <td>[LOCAL] Feasibility Study Underway  Targeting...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>N</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>[LOCAL] Feasibility Study Underway  Targeting...</td>\n",
       "      <td>[LOCAL] Feasibility Study Underway  Targeting...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>DEV</td>\n",
       "      <td>DevEx Resources Ltd</td>\n",
       "      <td>devexresources.com.au</td>\n",
       "      <td>2</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>[LOCAL] 2 Laramide Resources Limited, Westmore...</td>\n",
       "      <td>[LOCAL] 2 Laramide Resources Limited, Westmore...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>N</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>[LOCAL] 2 Laramide Resources Limited, Westmore...</td>\n",
       "      <td>[LOCAL] 2 Laramide Resources Limited, Westmore...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>FLG</td>\n",
       "      <td>Flagship Minerals Ltd</td>\n",
       "      <td>flagshipminerals.com</td>\n",
       "      <td>2</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>[LOCAL] ADVANCE PANTANILLO TO FEASIBILITY AND ...</td>\n",
       "      <td>[LOCAL] ADVANCE PANTANILLO TO FEASIBILITY AND ...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>N</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>[LOCAL] ADVANCE PANTANILLO TO FEASIBILITY AND ...</td>\n",
       "      <td>[LOCAL] ADVANCE PANTANILLO TO FEASIBILITY AND ...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>CRS</td>\n",
       "      <td>Caprice Resources Ltd</td>\n",
       "      <td>capriceresources.com</td>\n",
       "      <td>2</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>[LOCAL] evaluation work (Scoping Study / Feasi...</td>\n",
       "      <td>[LOCAL] evaluation work (Scoping Study / Feasi...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>N</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>[LOCAL] evaluation work (Scoping Study / Feasi...</td>\n",
       "      <td>[LOCAL] evaluation work (Scoping Study / Feasi...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>https://www.capriceresources.com/crs/wp-conten...</td>\n",
       "      <td>https://www.capriceresources.com/crs/wp-conten...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>RDM</td>\n",
       "      <td>Red Metal Ltd</td>\n",
       "      <td>redmetal.com.au</td>\n",
       "      <td>2</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Y</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>[WEB] At year end Scoping Studies were nearing...</td>\n",
       "      <td>[WEB] At year end Scoping Studies were nearing...</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>CXU</td>\n",
       "      <td>Cauldron Energy Ltd</td>\n",
       "      <td>cauldronenergy.com.au</td>\n",
       "      <td>2</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>[LOCAL] assurance of an economic development c...</td>\n",
       "      <td>[LOCAL] assurance of an economic development c...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>N</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>[LOCAL] assurance of an economic development c...</td>\n",
       "      <td>[LOCAL] assurance of an economic development c...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>CAE</td>\n",
       "      <td>Cannindah Resources Ltd</td>\n",
       "      <td>cannindah.com.au</td>\n",
       "      <td>2</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Y</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>[WEB] A 2022 scoping study pointed to the [WEB...</td>\n",
       "      <td>[WEB] A 2022 scoping study pointed to the\\n[WE...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>ORD</td>\n",
       "      <td>Ordell Minerals Ltd</td>\n",
       "      <td>ordellminerals.com.au</td>\n",
       "      <td>2</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>[LOCAL] Exploration and evaluation costs, incl...</td>\n",
       "      <td>[LOCAL] Exploration and evaluation costs, incl...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>N</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>[LOCAL] Exploration and evaluation costs, incl...</td>\n",
       "      <td>[LOCAL] Exploration and evaluation costs, incl...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>IDA</td>\n",
       "      <td>Indiana Resources Ltd</td>\n",
       "      <td>indianaresources.com.au</td>\n",
       "      <td>2</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>[LOCAL] executive with extensive experience in...</td>\n",
       "      <td>[LOCAL] executive with extensive experience in...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>N</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>[LOCAL] executive with extensive experience in...</td>\n",
       "      <td>[LOCAL] executive with extensive experience in...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>ALM</td>\n",
       "      <td>Alma Metals Ltd</td>\n",
       "      <td>almametals.com.au</td>\n",
       "      <td>2</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>[LOCAL] Scoping Study planned to commence late...</td>\n",
       "      <td>[LOCAL] Scoping Study planned to commence late...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>N</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>[LOCAL] Scoping Study planned to commence late...</td>\n",
       "      <td>[LOCAL] Scoping Study planned to commence late...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "      <td>C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25 rows  26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Ticker                         Company                   Website  \\\n",
       "0     RMS          Ramelius Resources Ltd  rameliusresources.com.au   \n",
       "2     CHN              Chalice Mining Ltd         chalicemining.com   \n",
       "6     SVL                Silver Mines Ltd        silvermines.com.au   \n",
       "7     STK           Strickland Metals Ltd   stricklandmetals.com.au   \n",
       "9     AZY             Antipa Minerals Ltd     antipaminerals.com.au   \n",
       "10    MAU           Magnetic Resources NL             magres.com.au   \n",
       "13    WWI            West Wits Mining Ltd        westwitsmining.com   \n",
       "16    BGD        Barton Gold Holdings Ltd         bartongold.com.au   \n",
       "18    HCH                   Hot Chili Ltd           hotchili.net.au   \n",
       "24    RNU          Renascor Resources Ltd           renascor.com.au   \n",
       "30    HRN                Horizon Gold Ltd        horizongold.com.au   \n",
       "32    ARL             Ardea Resources Ltd     ardearesources.com.au   \n",
       "35    MMA              Maronan Metals Ltd      maronanmetals.com.au   \n",
       "36    MAT             Matsa Resources Ltd              matsa.com.au   \n",
       "37    HAS  Hastings Technology Metals Ltd    hastingstechmetals.com   \n",
       "38    PC2                     PC Gold Ltd             pcgold.com.au   \n",
       "39    DEV             DevEx Resources Ltd     devexresources.com.au   \n",
       "46    FLG           Flagship Minerals Ltd      flagshipminerals.com   \n",
       "48    CRS           Caprice Resources Ltd      capriceresources.com   \n",
       "49    RDM                   Red Metal Ltd           redmetal.com.au   \n",
       "53    CXU             Cauldron Energy Ltd     cauldronenergy.com.au   \n",
       "58    CAE         Cannindah Resources Ltd          cannindah.com.au   \n",
       "59    ORD             Ordell Minerals Ltd     ordellminerals.com.au   \n",
       "67    IDA           Indiana Resources Ltd   indianaresources.com.au   \n",
       "69    ALM                 Alma Metals Ltd         almametals.com.au   \n",
       "\n",
       "    Local_PDF_Count Local_Scoping_Flag Local_Feasibility_Flag  \\\n",
       "0                 2                  Y                      Y   \n",
       "2                 2                  Y                      Y   \n",
       "6                 2                  N                      Y   \n",
       "7                 2                  N                      N   \n",
       "9                 2                  Y                      Y   \n",
       "10                2                  N                      N   \n",
       "13                1                  Y                      Y   \n",
       "16                2                  N                      Y   \n",
       "18                2                  N                      Y   \n",
       "24                2                  N                      Y   \n",
       "30                2                  Y                      N   \n",
       "32                2                  Y                      Y   \n",
       "35                2                  Y                      Y   \n",
       "36                2                  Y                      Y   \n",
       "37                2                  N                      Y   \n",
       "38                2                  N                      Y   \n",
       "39                2                  Y                      Y   \n",
       "46                2                  N                      Y   \n",
       "48                2                  Y                      Y   \n",
       "49                2                  N                      N   \n",
       "53                2                  Y                      N   \n",
       "58                2                  N                      N   \n",
       "59                2                  N                      Y   \n",
       "67                2                  N                      Y   \n",
       "69                2                  Y                      Y   \n",
       "\n",
       "                           Local_Evidence_Summary_50w  \\\n",
       "0   [LOCAL] deposit was optimised and a Scoping St...   \n",
       "2   [LOCAL] the Scoping Study testwork phase. [LOC...   \n",
       "6   [LOCAL] in the Kramer Hills Project, Lustrum G...   \n",
       "7                                                       \n",
       "9   [LOCAL] confirming the Scoping Study metallurg...   \n",
       "10                                                      \n",
       "13  [LOCAL] 2022 Scoping Study: 5 Development stag...   \n",
       "16  [LOCAL] and a Pre-Feasibility Study. [LOCAL] c...   \n",
       "18  [LOCAL] These funds place us in a strong posit...   \n",
       "24  [LOCAL] Siviour Definitive Feasibility Study P...   \n",
       "30  [LOCAL] In March 2016 a scoping study for a fr...   \n",
       "32  [LOCAL] Definitive Feasibility Study, Goongarr...   \n",
       "35  [LOCAL] Work is well advanced on the PEA or sc...   \n",
       "36  [LOCAL] two Scoping Studies and Linden publish...   \n",
       "37  [LOCAL] technical feasibility are demonstrated...   \n",
       "38  [LOCAL] Feasibility Study Underway  Targeting...   \n",
       "39  [LOCAL] 2 Laramide Resources Limited, Westmore...   \n",
       "46  [LOCAL] ADVANCE PANTANILLO TO FEASIBILITY AND ...   \n",
       "48  [LOCAL] evaluation work (Scoping Study / Feasi...   \n",
       "49                                                      \n",
       "53  [LOCAL] assurance of an economic development c...   \n",
       "58                                                      \n",
       "59  [LOCAL] Exploration and evaluation costs, incl...   \n",
       "67  [LOCAL] executive with extensive experience in...   \n",
       "69  [LOCAL] Scoping Study planned to commence late...   \n",
       "\n",
       "                              Local_Evidence_Snippets  \\\n",
       "0   [LOCAL] deposit was optimised and a Scoping St...   \n",
       "2   [LOCAL] the Scoping Study testwork phase.\\n[LO...   \n",
       "6   [LOCAL] in the Kramer Hills Project, Lustrum G...   \n",
       "7                                                       \n",
       "9   [LOCAL] confirming the Scoping Study metallurg...   \n",
       "10                                                      \n",
       "13  [LOCAL] 2022 Scoping Study: 5 Development stag...   \n",
       "16  [LOCAL] and a Pre-Feasibility Study.\\n[LOCAL] ...   \n",
       "18  [LOCAL] These funds place us in a strong posit...   \n",
       "24  [LOCAL] Siviour Definitive Feasibility Study P...   \n",
       "30  [LOCAL] In March 2016 a scoping study for a fr...   \n",
       "32  [LOCAL] Definitive Feasibility Study, Goongarr...   \n",
       "35  [LOCAL] Work is well advanced on the PEA or sc...   \n",
       "36  [LOCAL] two Scoping Studies and Linden publish...   \n",
       "37  [LOCAL] technical feasibility are demonstrated...   \n",
       "38  [LOCAL] Feasibility Study Underway  Targeting...   \n",
       "39  [LOCAL] 2 Laramide Resources Limited, Westmore...   \n",
       "46  [LOCAL] ADVANCE PANTANILLO TO FEASIBILITY AND ...   \n",
       "48  [LOCAL] evaluation work (Scoping Study / Feasi...   \n",
       "49                                                      \n",
       "53  [LOCAL] assurance of an economic development c...   \n",
       "58                                                      \n",
       "59  [LOCAL] Exploration and evaluation costs, incl...   \n",
       "67  [LOCAL] executive with extensive experience in...   \n",
       "69  [LOCAL] Scoping Study planned to commence late...   \n",
       "\n",
       "                                  Local_Evidence_PDFs Web_Checked  ...  \\\n",
       "0   C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...           N  ...   \n",
       "2   C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...           N  ...   \n",
       "6   C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...           N  ...   \n",
       "7                                                               Y  ...   \n",
       "9   C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...           N  ...   \n",
       "10                                                              Y  ...   \n",
       "13  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...           N  ...   \n",
       "16  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...           N  ...   \n",
       "18  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...           N  ...   \n",
       "24  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...           N  ...   \n",
       "30  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...           N  ...   \n",
       "32  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...           N  ...   \n",
       "35  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...           N  ...   \n",
       "36  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...           N  ...   \n",
       "37  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...           N  ...   \n",
       "38  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...           N  ...   \n",
       "39  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...           N  ...   \n",
       "46  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...           N  ...   \n",
       "48  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...           N  ...   \n",
       "49                                                              Y  ...   \n",
       "53  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...           N  ...   \n",
       "58                                                              Y  ...   \n",
       "59  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...           N  ...   \n",
       "67  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...           N  ...   \n",
       "69  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...           N  ...   \n",
       "\n",
       "   Scoping_Flag Feasibility_Flag  \\\n",
       "0             Y                Y   \n",
       "2             Y                Y   \n",
       "6             N                Y   \n",
       "7             Y                Y   \n",
       "9             Y                Y   \n",
       "10            N                Y   \n",
       "13            Y                Y   \n",
       "16            N                Y   \n",
       "18            N                Y   \n",
       "24            N                Y   \n",
       "30            Y                N   \n",
       "32            Y                Y   \n",
       "35            Y                Y   \n",
       "36            Y                Y   \n",
       "37            N                Y   \n",
       "38            N                Y   \n",
       "39            Y                Y   \n",
       "46            N                Y   \n",
       "48            Y                Y   \n",
       "49            Y                Y   \n",
       "53            Y                N   \n",
       "58            Y                Y   \n",
       "59            N                Y   \n",
       "67            N                Y   \n",
       "69            Y                Y   \n",
       "\n",
       "                                 Evidence_Summary_50w  \\\n",
       "0   [LOCAL] deposit was optimised and a Scoping St...   \n",
       "2   [LOCAL] the Scoping Study testwork phase. [LOC...   \n",
       "6   [LOCAL] in the Kramer Hills Project, Lustrum G...   \n",
       "7   [WEB] Developers / near-producers are progress...   \n",
       "9   [LOCAL] confirming the Scoping Study metallurg...   \n",
       "10  [WEB] We expect the updated Feasibility will a...   \n",
       "13  [LOCAL] 2022 Scoping Study: 5 Development stag...   \n",
       "16  [LOCAL] and a Pre-Feasibility Study. [LOCAL] c...   \n",
       "18  [LOCAL] These funds place us in a strong posit...   \n",
       "24  [LOCAL] Siviour Definitive Feasibility Study P...   \n",
       "30  [LOCAL] In March 2016 a scoping study for a fr...   \n",
       "32  [LOCAL] Definitive Feasibility Study, Goongarr...   \n",
       "35  [LOCAL] Work is well advanced on the PEA or sc...   \n",
       "36  [LOCAL] two Scoping Studies and Linden publish...   \n",
       "37  [LOCAL] technical feasibility are demonstrated...   \n",
       "38  [LOCAL] Feasibility Study Underway  Targeting...   \n",
       "39  [LOCAL] 2 Laramide Resources Limited, Westmore...   \n",
       "46  [LOCAL] ADVANCE PANTANILLO TO FEASIBILITY AND ...   \n",
       "48  [LOCAL] evaluation work (Scoping Study / Feasi...   \n",
       "49  [WEB] At year end Scoping Studies were nearing...   \n",
       "53  [LOCAL] assurance of an economic development c...   \n",
       "58  [WEB] A 2022 scoping study pointed to the [WEB...   \n",
       "59  [LOCAL] Exploration and evaluation costs, incl...   \n",
       "67  [LOCAL] executive with extensive experience in...   \n",
       "69  [LOCAL] Scoping Study planned to commence late...   \n",
       "\n",
       "                                    Evidence_Snippets  \\\n",
       "0   [LOCAL] deposit was optimised and a Scoping St...   \n",
       "2   [LOCAL] the Scoping Study testwork phase.\\n[LO...   \n",
       "6   [LOCAL] in the Kramer Hills Project, Lustrum G...   \n",
       "7   [WEB] Developers / near-producers are progress...   \n",
       "9   [LOCAL] confirming the Scoping Study metallurg...   \n",
       "10  [WEB] We expect the updated Feasibility will a...   \n",
       "13  [LOCAL] 2022 Scoping Study: 5 Development stag...   \n",
       "16  [LOCAL] and a Pre-Feasibility Study.\\n[LOCAL] ...   \n",
       "18  [LOCAL] These funds place us in a strong posit...   \n",
       "24  [LOCAL] Siviour Definitive Feasibility Study P...   \n",
       "30  [LOCAL] In March 2016 a scoping study for a fr...   \n",
       "32  [LOCAL] Definitive Feasibility Study, Goongarr...   \n",
       "35  [LOCAL] Work is well advanced on the PEA or sc...   \n",
       "36  [LOCAL] two Scoping Studies and Linden publish...   \n",
       "37  [LOCAL] technical feasibility are demonstrated...   \n",
       "38  [LOCAL] Feasibility Study Underway  Targeting...   \n",
       "39  [LOCAL] 2 Laramide Resources Limited, Westmore...   \n",
       "46  [LOCAL] ADVANCE PANTANILLO TO FEASIBILITY AND ...   \n",
       "48  [LOCAL] evaluation work (Scoping Study / Feasi...   \n",
       "49  [WEB] At year end Scoping Studies were nearing...   \n",
       "53  [LOCAL] assurance of an economic development c...   \n",
       "58  [WEB] A 2022 scoping study pointed to the\\n[WE...   \n",
       "59  [LOCAL] Exploration and evaluation costs, incl...   \n",
       "67  [LOCAL] executive with extensive experience in...   \n",
       "69  [LOCAL] Scoping Study planned to commence late...   \n",
       "\n",
       "   Resource_Definition_Flag PFS_Flag  \\\n",
       "0                         Y        Y   \n",
       "2                         N        Y   \n",
       "6                         Y        Y   \n",
       "7                         N        N   \n",
       "9                         Y        Y   \n",
       "10                        N        N   \n",
       "13                        N        Y   \n",
       "16                        Y        Y   \n",
       "18                        N        Y   \n",
       "24                        N        Y   \n",
       "30                        N        Y   \n",
       "32                        Y        Y   \n",
       "35                        Y        Y   \n",
       "36                        N        Y   \n",
       "37                        Y        Y   \n",
       "38                        Y        Y   \n",
       "39                        Y        Y   \n",
       "46                        Y        Y   \n",
       "48                        Y        Y   \n",
       "49                        Y        N   \n",
       "53                        Y        Y   \n",
       "58                        N        N   \n",
       "59                        Y        Y   \n",
       "67                        Y        Y   \n",
       "69                        Y        Y   \n",
       "\n",
       "                                        Quarterly_URL  \\\n",
       "0                                                 NaN   \n",
       "2                                                 NaN   \n",
       "6                                                 NaN   \n",
       "7                                                 NaN   \n",
       "9                                                 NaN   \n",
       "10                                                NaN   \n",
       "13                                                NaN   \n",
       "16                                                NaN   \n",
       "18                                                NaN   \n",
       "24                                                NaN   \n",
       "30                                                NaN   \n",
       "32                                                NaN   \n",
       "35                                                NaN   \n",
       "36                                                NaN   \n",
       "37                                                NaN   \n",
       "38                                                NaN   \n",
       "39                                                NaN   \n",
       "46                                                NaN   \n",
       "48  https://www.capriceresources.com/crs/wp-conten...   \n",
       "49                                                NaN   \n",
       "53                                                NaN   \n",
       "58                                                NaN   \n",
       "59                                                NaN   \n",
       "67                                                NaN   \n",
       "69                                                NaN   \n",
       "\n",
       "                                           Annual_URL  \\\n",
       "0                                                 NaN   \n",
       "2                                                 NaN   \n",
       "6                                                 NaN   \n",
       "7                                                 NaN   \n",
       "9                                                 NaN   \n",
       "10                                                NaN   \n",
       "13                                                NaN   \n",
       "16                                                NaN   \n",
       "18                                                NaN   \n",
       "24                                                NaN   \n",
       "30                                                NaN   \n",
       "32                                                NaN   \n",
       "35                                                NaN   \n",
       "36                                                NaN   \n",
       "37                                                NaN   \n",
       "38                                                NaN   \n",
       "39                                                NaN   \n",
       "46                                                NaN   \n",
       "48  https://www.capriceresources.com/crs/wp-conten...   \n",
       "49                                                NaN   \n",
       "53                                                NaN   \n",
       "58                                                NaN   \n",
       "59                                                NaN   \n",
       "67                                                NaN   \n",
       "69                                                NaN   \n",
       "\n",
       "                                        Quarterly_PDF  \\\n",
       "0   C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "2   C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "6   C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "7   C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "9   C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "10  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "13  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "16  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "18  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "24  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "30  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "32  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "35  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "36  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "37  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "38  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "39  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "46  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "48                                                NaN   \n",
       "49  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "53  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "58  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "59  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "67  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "69  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...   \n",
       "\n",
       "                                           Annual_PDF  \n",
       "0   C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...  \n",
       "2   C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...  \n",
       "6   C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...  \n",
       "7   C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...  \n",
       "9   C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...  \n",
       "10  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...  \n",
       "13  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...  \n",
       "16  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...  \n",
       "18  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...  \n",
       "24  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...  \n",
       "30  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...  \n",
       "32  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...  \n",
       "35  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...  \n",
       "36  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...  \n",
       "37  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...  \n",
       "38  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...  \n",
       "39  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...  \n",
       "46  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...  \n",
       "48                                                NaN  \n",
       "49  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...  \n",
       "53  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...  \n",
       "58  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...  \n",
       "59  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...  \n",
       "67  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...  \n",
       "69  C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULT...  \n",
       "\n",
       "[25 rows x 26 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# RIU - Scoping / Feasibility flagger (Local PDFs first + Web fallback)\n",
    "# - Scans your existing downloaded PDFs for:\n",
    "#     \"Scoping Study\", \"Scoping Studies\", \"Feasibility\"\n",
    "# - If no local hits/no text, does a light web pass:\n",
    "#     crawl site -> pick top PDFs -> download -> scan\n",
    "# - Outputs compiled CSVs\n",
    "#\n",
    "# Requirements:\n",
    "#   pip/conda install: requests beautifulsoup4 lxml pandas pymupdf tqdm\n",
    "# ============================================================\n",
    "\n",
    "import re\n",
    "import time\n",
    "import datetime as dt\n",
    "from dataclasses import dataclass, asdict\n",
    "from pathlib import Path\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import fitz  # PyMuPDF\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# PATHS (EDIT THESE)\n",
    "# -------------------------\n",
    "COMPANIES_CSV = r\"C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULTING PTY LTD\\Documents\\01_BD\\96_2026_RIU_Conference_Perth\\companies_with_website.csv\"\n",
    "# Optional: your prior compiled file (used only to carry-over previous columns if you want)\n",
    "PRIOR_FINAL_CSV = r\"C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULTING PTY LTD\\Documents\\01_BD\\96_2026_RIU_Conference_Perth\\riu_stage_flags_FINAL.csv\"\n",
    "\n",
    "# Scan these folders for already-downloaded PDFs\n",
    "DOWNLOAD_DIRS = [\n",
    "    Path(r\"C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULTING PTY LTD\\Documents\\01_BD\\96_2026_RIU_Conference_Perth\\downloaded_reports\"),\n",
    "    Path(r\"C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULTING PTY LTD\\Documents\\01_BD\\96_2026_RIU_Conference_Perth\\downloaded_reports_retry\"),\n",
    "]\n",
    "\n",
    "# Web fallback downloads go here\n",
    "FALLBACK_DOWNLOAD_DIR = Path(r\"C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULTING PTY LTD\\Documents\\01_BD\\96_2026_RIU_Conference_Perth\\downloaded_reports_scoping_fallback\")\n",
    "FALLBACK_DEBUG_DIR    = Path(r\"C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULTING PTY LTD\\Documents\\01_BD\\96_2026_RIU_Conference_Perth\\debug_pages_scoping_fallback\")\n",
    "\n",
    "OUT_FINAL = r\"C:\\Users\\julian.diaz\\OneDrive - XENITH CONSULTING PTY LTD\\Documents\\01_BD\\96_2026_RIU_Conference_Perth\\riu_scoping_feasibility_FINAL.csv\"\n",
    "OUT_HITS  = OUT_FINAL.replace(\".csv\", \"_hits.csv\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# RUNTIME SETTINGS\n",
    "# -------------------------\n",
    "REQUEST_TIMEOUT = 25\n",
    "SLEEP_S = 0.4\n",
    "MAX_PAGES_TO_VISIT = 30\n",
    "MAX_SITEMAP_URLS = 3500\n",
    "MAX_SITEMAP_CHILDREN = 25\n",
    "MAX_FALLBACK_PDFS_TO_SCAN = 3     # only when local has no hits/no text\n",
    "\n",
    "USER_AGENT = (\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "    \"(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    ")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# KEYWORDS\n",
    "# -------------------------\n",
    "SCOPE_RE = re.compile(r\"\\bscoping\\s+stud(?:y|ies)\\b\", re.IGNORECASE)\n",
    "FEAS_RE  = re.compile(r\"\\bfeasibility\\b\", re.IGNORECASE)\n",
    "\n",
    "# If you want to focus on \"study context\", keep a light intent cue\n",
    "INTENT_RE = re.compile(\n",
    "    r\"\\b(will|plan(?:s|ned)?|intend(?:s|ed)?|commence|start|complete|deliver|progress|advance|\"\n",
    "    r\"target(?:s|ing)?|scheduled|expected|underway|next\\s+quarter|FY\\d{2}|CY\\d{2})\\b\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "REPORT_URL_HINT = re.compile(\n",
    "    r\"(investor|investors|asx|announce|announcement|release|news|media|report|results|financial|presentation|quarter|appendix|4c|4e)\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "PDF_EXT_RE = re.compile(r\"\\.pdf(\\?|$)\", re.IGNORECASE)\n",
    "PRESENTATION_HINT = re.compile(r\"(presentation|investor[-\\s_]*pres|deck|slides)\", re.IGNORECASE)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# HELPERS\n",
    "# -------------------------\n",
    "def is_blank(x) -> bool:\n",
    "    if x is None:\n",
    "        return True\n",
    "    s = str(x)\n",
    "    return (s.strip() == \"\") or (s.strip().lower() == \"nan\")\n",
    "\n",
    "def normalize_site(site: str) -> str:\n",
    "    site = (site or \"\").strip()\n",
    "    if not site:\n",
    "        return \"\"\n",
    "    if not site.startswith((\"http://\", \"https://\")):\n",
    "        site = \"https://\" + site\n",
    "    return site.rstrip(\"/\")\n",
    "\n",
    "def safe_filename(s: str) -> str:\n",
    "    return re.sub(r\"[^a-zA-Z0-9._-]+\", \"_\", s)[:180]\n",
    "\n",
    "def same_domain(base: str, url: str) -> bool:\n",
    "    try:\n",
    "        b = urlparse(base)\n",
    "        u = urlparse(url)\n",
    "        return (b.netloc.lower() == u.netloc.lower()) or (u.netloc.lower().endswith(b.netloc.lower()))\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def make_session() -> requests.Session:\n",
    "    s = requests.Session()\n",
    "    s.headers.update({\"User-Agent\": USER_AGENT, \"Accept-Language\": \"en-US,en;q=0.9\"})\n",
    "    return s\n",
    "\n",
    "def fetch_text(sess: requests.Session, url: str) -> tuple[int, str, str]:\n",
    "    try:\n",
    "        r = sess.get(url, timeout=REQUEST_TIMEOUT, allow_redirects=True)\n",
    "        ctype = (r.headers.get(\"content-type\") or \"\").lower()\n",
    "        return r.status_code, ctype, r.text or \"\"\n",
    "    except Exception:\n",
    "        return 0, \"\", \"\"\n",
    "\n",
    "def download_file(sess: requests.Session, url: str, outpath: Path) -> bool:\n",
    "    try:\n",
    "        r = sess.get(url, timeout=REQUEST_TIMEOUT, stream=True, allow_redirects=True)\n",
    "        if r.status_code >= 400:\n",
    "            return False\n",
    "        ctype = (r.headers.get(\"content-type\") or \"\").lower()\n",
    "        if (\"pdf\" not in ctype) and (not PDF_EXT_RE.search(url)):\n",
    "            return False\n",
    "        outpath.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(outpath, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=1024 * 64):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "        return outpath.exists() and outpath.stat().st_size > 10_000\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def extract_pdf_text_fitz(pdf_path: Path) -> str:\n",
    "    try:\n",
    "        doc = fitz.open(str(pdf_path))\n",
    "        parts = []\n",
    "        for page in doc:\n",
    "            t = page.get_text(\"text\") or \"\"\n",
    "            if t.strip():\n",
    "                parts.append(t)\n",
    "        doc.close()\n",
    "        return \"\\n\".join(parts)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def split_sentences(text: str) -> list[str]:\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "    text = re.sub(r\"\\n{2,}\", \"\\n\", text)\n",
    "    sents = re.split(r\"(?<=[\\.\\?\\!])\\s+|\\n+\", text)\n",
    "    return [s.strip() for s in sents if s and len(s.strip()) > 25]\n",
    "\n",
    "def trim_to_50_words(s: str) -> str:\n",
    "    words = s.split()\n",
    "    if len(words) <= 50:\n",
    "        return s.strip()\n",
    "    return \" \".join(words[:50]).strip() + \"\"\n",
    "\n",
    "def evidence_from_text_scoping_feas(text: str):\n",
    "    sents = split_sentences(text)\n",
    "    scope_hits = []\n",
    "    feas_hits = []\n",
    "\n",
    "    for s in sents:\n",
    "        # Prefer sentences that look like \"real statements\" (intent cues),\n",
    "        # but don't require it (many reports are bullet-ish).\n",
    "        has_intent = bool(INTENT_RE.search(s))\n",
    "\n",
    "        if SCOPE_RE.search(s):\n",
    "            scope_hits.append((has_intent, s))\n",
    "        if FEAS_RE.search(s):\n",
    "            feas_hits.append((has_intent, s))\n",
    "\n",
    "    # rank: intent-bearing first, then shorter\n",
    "    def rank(hit):\n",
    "        has_intent, sent = hit\n",
    "        return (1 if has_intent else 0, -len(sent))\n",
    "\n",
    "    scope_hits.sort(key=rank, reverse=True)\n",
    "    feas_hits.sort(key=rank, reverse=True)\n",
    "\n",
    "    scope_sents = [s for _, s in scope_hits[:2]]\n",
    "    feas_sents  = [s for _, s in feas_hits[:2]]\n",
    "\n",
    "    return (len(scope_sents) > 0), (len(feas_sents) > 0), (scope_sents + feas_sents)\n",
    "\n",
    "def read_companies(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, sep=None, engine=\"python\")\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    colmap = {c.lower(): c for c in df.columns}\n",
    "\n",
    "    def pick(*names):\n",
    "        for n in names:\n",
    "            if n in colmap:\n",
    "                return colmap[n]\n",
    "        return None\n",
    "\n",
    "    c_company = pick(\"company\", \"name\", \"co\", \"issuer\")\n",
    "    c_ticker  = pick(\"ticker\", \"asx\", \"code\", \"symbol\")\n",
    "    c_web     = pick(\"website\", \"web\", \"url\", \"site\")\n",
    "\n",
    "    if not c_company or not c_ticker or not c_web:\n",
    "        raise ValueError(f\"Missing required columns. Found: {list(df.columns)}. Need Company/Ticker/Website (or similar).\")\n",
    "\n",
    "    df = df.rename(columns={c_company: \"Company\", c_ticker: \"Ticker\", c_web: \"Website\"})\n",
    "    df[\"Ticker\"] = df[\"Ticker\"].astype(str).str.strip().str.upper()\n",
    "    df[\"Company\"] = df[\"Company\"].astype(str).str.strip()\n",
    "    df[\"Website\"] = df[\"Website\"].astype(str).str.strip()\n",
    "    return df\n",
    "\n",
    "def build_pdf_index(download_dirs: list[Path]) -> dict[str, list[Path]]:\n",
    "    \"\"\"\n",
    "    Map Ticker -> list of PDFs found under DOWNLOAD_DIR/TICKER/**.pdf\n",
    "    \"\"\"\n",
    "    idx: dict[str, list[Path]] = {}\n",
    "    for d in download_dirs:\n",
    "        if not d.exists():\n",
    "            continue\n",
    "        for tkr_dir in d.iterdir():\n",
    "            if not tkr_dir.is_dir():\n",
    "                continue\n",
    "            tkr = tkr_dir.name.strip().upper()\n",
    "            pdfs = list(tkr_dir.rglob(\"*.pdf\"))\n",
    "            if not pdfs:\n",
    "                continue\n",
    "            idx.setdefault(tkr, []).extend(pdfs)\n",
    "    # de-dupe paths\n",
    "    for k in list(idx.keys()):\n",
    "        seen = set()\n",
    "        unique = []\n",
    "        for p in idx[k]:\n",
    "            sp = str(p)\n",
    "            if sp in seen:\n",
    "                continue\n",
    "            seen.add(sp)\n",
    "            unique.append(p)\n",
    "        idx[k] = unique\n",
    "    return idx\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# WEB FALLBACK (only for those with no local hits)\n",
    "# -------------------------\n",
    "def get_sitemaps_from_robots(sess: requests.Session, site: str) -> list[str]:\n",
    "    st, ct, txt = fetch_text(sess, site + \"/robots.txt\")\n",
    "    time.sleep(SLEEP_S)\n",
    "    if st >= 400 or not txt:\n",
    "        return []\n",
    "    sitemaps = []\n",
    "    for line in txt.splitlines():\n",
    "        if line.lower().startswith(\"sitemap:\"):\n",
    "            sm = line.split(\":\", 1)[1].strip()\n",
    "            if sm:\n",
    "                sitemaps.append(sm)\n",
    "    return sitemaps\n",
    "\n",
    "def get_sitemap_urls(sess: requests.Session, site: str) -> list[str]:\n",
    "    starts = [\"/sitemap.xml\", \"/sitemap_index.xml\", \"/wp-sitemap.xml\"]\n",
    "    all_urls = []\n",
    "    candidate_sitemaps = get_sitemaps_from_robots(sess, site) + [site + p for p in starts]\n",
    "\n",
    "    seen_sm = set()\n",
    "    for sm in candidate_sitemaps:\n",
    "        if sm in seen_sm:\n",
    "            continue\n",
    "        seen_sm.add(sm)\n",
    "\n",
    "        status, ctype, txt = fetch_text(sess, sm)\n",
    "        time.sleep(SLEEP_S)\n",
    "        if status >= 400 or not txt:\n",
    "            continue\n",
    "        if (\"xml\" not in ctype) and (\"<urlset\" not in txt.lower()) and (\"<sitemapindex\" not in txt.lower()):\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(txt, \"xml\")\n",
    "        sitemap_locs = [loc.get_text(strip=True) for loc in soup.find_all(\"sitemap\") for loc in loc.find_all(\"loc\")]\n",
    "\n",
    "        if sitemap_locs:\n",
    "            for child in sitemap_locs[:MAX_SITEMAP_CHILDREN]:\n",
    "                st2, ct2, t2 = fetch_text(sess, child)\n",
    "                time.sleep(SLEEP_S)\n",
    "                if st2 >= 400 or not t2:\n",
    "                    continue\n",
    "                s2 = BeautifulSoup(t2, \"xml\")\n",
    "                locs = [loc.get_text(strip=True) for loc in s2.find_all(\"loc\")]\n",
    "                for u in locs:\n",
    "                    if REPORT_URL_HINT.search(u):\n",
    "                        all_urls.append(u)\n",
    "                if len(all_urls) >= MAX_SITEMAP_URLS:\n",
    "                    return all_urls[:MAX_SITEMAP_URLS]\n",
    "        else:\n",
    "            locs = [loc.get_text(strip=True) for loc in soup.find_all(\"loc\")]\n",
    "            for u in locs:\n",
    "                if REPORT_URL_HINT.search(u):\n",
    "                    all_urls.append(u)\n",
    "            if all_urls:\n",
    "                return all_urls[:MAX_SITEMAP_URLS]\n",
    "\n",
    "    return all_urls[:MAX_SITEMAP_URLS]\n",
    "\n",
    "def discover_report_pages(site: str) -> list[str]:\n",
    "    paths = [\n",
    "        \"/investors\", \"/investor\", \"/investor-centre\", \"/asx-announcements\",\n",
    "        \"/announcements\", \"/asx-releases\", \"/reports\", \"/financial-reports\",\n",
    "        \"/results\", \"/news\", \"/media\", \"/investors/reports\", \"/investors/announcements\",\n",
    "    ]\n",
    "    return [site + p for p in paths]\n",
    "\n",
    "def extract_pdf_links_from_html(base_url: str, html: str) -> list[dict]:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    out = []\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = (a.get(\"href\") or \"\").strip()\n",
    "        if not href:\n",
    "            continue\n",
    "        absu = urljoin(base_url, href)\n",
    "        if not PDF_EXT_RE.search(absu):\n",
    "            continue\n",
    "        label = \" \".join(a.get_text(\" \", strip=True).split())\n",
    "        out.append({\"url\": absu, \"label\": label, \"source_page\": base_url})\n",
    "    return out\n",
    "\n",
    "def score_pdf_candidate(c: dict) -> float:\n",
    "    \"\"\"\n",
    "    For scoping/feasibility, investor presentations can still be very useful,\n",
    "    so we only mildly penalize them.\n",
    "    \"\"\"\n",
    "    label = (c.get(\"label\") or \"\")\n",
    "    url = (c.get(\"url\") or \"\")\n",
    "    text = f\"{label} {url}\".lower()\n",
    "\n",
    "    score = 0.0\n",
    "    if \"quarter\" in text or \"4c\" in text or \"activities\" in text:\n",
    "        score += 15\n",
    "    if \"annual\" in text or \"4e\" in text:\n",
    "        score += 12\n",
    "    if \"report\" in text or \"results\" in text:\n",
    "        score += 10\n",
    "    if \"scoping\" in text:\n",
    "        score += 25\n",
    "    if \"feasib\" in text:\n",
    "        score += 22\n",
    "    if PRESENTATION_HINT.search(text):\n",
    "        score -= 3\n",
    "\n",
    "    return score\n",
    "\n",
    "def crawl_site_for_best_pdfs(sess: requests.Session, site: str, debug_dir: Path) -> tuple[list[dict], list[str]]:\n",
    "    notes = []\n",
    "    debug_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    pages = get_sitemap_urls(sess, site)\n",
    "    if pages:\n",
    "        notes.append(f\"sitemap_pages={len(pages)}\")\n",
    "    else:\n",
    "        notes.append(\"no_sitemap_or_no_filtered_urls\")\n",
    "        pages = discover_report_pages(site)\n",
    "\n",
    "    visited = set()\n",
    "    queue = [u for u in pages[:250] if REPORT_URL_HINT.search(u)]\n",
    "    pdf_candidates = []\n",
    "\n",
    "    while queue and len(visited) < MAX_PAGES_TO_VISIT:\n",
    "        url = queue.pop(0)\n",
    "        if url in visited:\n",
    "            continue\n",
    "        if not same_domain(site, url):\n",
    "            continue\n",
    "        visited.add(url)\n",
    "\n",
    "        st, ct, html = fetch_text(sess, url)\n",
    "        time.sleep(SLEEP_S)\n",
    "        if st >= 400 or not html:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            (debug_dir / f\"{len(visited):02d}_{safe_filename(url)}.html\").write_text(html, encoding=\"utf-8\", errors=\"ignore\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        pdf_candidates.extend(extract_pdf_links_from_html(url, html))\n",
    "\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        new_links = 0\n",
    "        for a in soup.find_all(\"a\", href=True):\n",
    "            href = (a.get(\"href\") or \"\").strip()\n",
    "            if not href:\n",
    "                continue\n",
    "            absu = urljoin(url, href)\n",
    "            if absu in visited:\n",
    "                continue\n",
    "            if not same_domain(site, absu):\n",
    "                continue\n",
    "            if REPORT_URL_HINT.search(absu):\n",
    "                queue.append(absu)\n",
    "                new_links += 1\n",
    "                if new_links >= 20:\n",
    "                    break\n",
    "\n",
    "    # Dedup + score\n",
    "    seen = set()\n",
    "    dedup = []\n",
    "    for c in pdf_candidates:\n",
    "        u = c.get(\"url\")\n",
    "        if not u or u in seen:\n",
    "            continue\n",
    "        seen.add(u)\n",
    "        c[\"score\"] = score_pdf_candidate(c)\n",
    "        dedup.append(c)\n",
    "\n",
    "    dedup.sort(key=lambda x: x.get(\"score\", 0.0), reverse=True)\n",
    "\n",
    "    notes.append(f\"visited_pages={len(visited)}\")\n",
    "    notes.append(f\"pdf_candidates_raw={len(pdf_candidates)}\")\n",
    "    notes.append(f\"pdf_candidates_dedup={len(dedup)}\")\n",
    "    return dedup, notes\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# OUTPUT MODEL\n",
    "# -------------------------\n",
    "@dataclass\n",
    "class OutRow:\n",
    "    Ticker: str\n",
    "    Company: str\n",
    "    Website: str\n",
    "\n",
    "    # Local scan\n",
    "    Local_PDF_Count: int\n",
    "    Local_Scoping_Flag: str\n",
    "    Local_Feasibility_Flag: str\n",
    "    Local_Evidence_Summary_50w: str\n",
    "    Local_Evidence_Snippets: str\n",
    "    Local_Evidence_PDFs: str\n",
    "\n",
    "    # Web fallback\n",
    "    Web_Checked: str\n",
    "    Web_Scoping_Flag: str\n",
    "    Web_Feasibility_Flag: str\n",
    "    Web_Evidence_Summary_50w: str\n",
    "    Web_Evidence_Snippets: str\n",
    "    Web_Evidence_PDFs: str\n",
    "    Notes: str\n",
    "\n",
    "    # Final combined flags\n",
    "    Scoping_Flag: str\n",
    "    Feasibility_Flag: str\n",
    "    Evidence_Summary_50w: str\n",
    "    Evidence_Snippets: str\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# CORE PER-COMPANY PROCESS\n",
    "# -------------------------\n",
    "def scan_local_pdfs(tkr: str, pdfs: list[Path]):\n",
    "    scope_flag = \"N\"\n",
    "    feas_flag = \"N\"\n",
    "    snippets = []\n",
    "    hit_pdfs = []\n",
    "\n",
    "    for pdf_path in pdfs:\n",
    "        txt = extract_pdf_text_fitz(pdf_path)\n",
    "        if not txt.strip():\n",
    "            continue\n",
    "\n",
    "        s_ok, f_ok, hits = evidence_from_text_scoping_feas(txt)\n",
    "        if s_ok:\n",
    "            scope_flag = \"Y\"\n",
    "        if f_ok:\n",
    "            feas_flag = \"Y\"\n",
    "        if hits:\n",
    "            hit_pdfs.append(str(pdf_path))\n",
    "            for h in hits:\n",
    "                snippets.append(f\"[LOCAL] {h}\")\n",
    "\n",
    "        # early exit if we have both\n",
    "        if scope_flag == \"Y\" and feas_flag == \"Y\":\n",
    "            # still okay to stop early; we just want evidence, not exhaustive counts\n",
    "            break\n",
    "\n",
    "    if snippets:\n",
    "        summary = trim_to_50_words(\" \".join(snippets))\n",
    "        snippet_txt = \"\\n\".join(snippets[:8])\n",
    "    else:\n",
    "        summary, snippet_txt = \"\", \"\"\n",
    "\n",
    "    return scope_flag, feas_flag, summary, snippet_txt, \"; \".join(hit_pdfs[:5])\n",
    "\n",
    "def scan_web_fallback(tkr: str, company: str, website: str):\n",
    "    site = normalize_site(website)\n",
    "    if not site:\n",
    "        return (\"N\",\"N\",\"\",\"\",\"\", \"missing website\")\n",
    "\n",
    "    sess = make_session()\n",
    "    out_dir = FALLBACK_DOWNLOAD_DIR / tkr\n",
    "    dbg_dir = FALLBACK_DEBUG_DIR / tkr\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    dbg_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    cands, notes = crawl_site_for_best_pdfs(sess, site, dbg_dir)\n",
    "    if not cands:\n",
    "        return (\"N\",\"N\",\"\",\"\",\"\", \"; \".join(notes + [\"no_pdf_links_found\"]))\n",
    "\n",
    "    # Download + scan top few\n",
    "    scope_flag = \"N\"\n",
    "    feas_flag = \"N\"\n",
    "    snippets = []\n",
    "    hit_pdfs = []\n",
    "\n",
    "    for i, c in enumerate(cands[:MAX_FALLBACK_PDFS_TO_SCAN], start=1):\n",
    "        url = c.get(\"url\",\"\")\n",
    "        if not url:\n",
    "            continue\n",
    "        pdf_path = out_dir / f\"{tkr}_fallback_{i}.pdf\"\n",
    "        ok = download_file(sess, url, pdf_path)\n",
    "        time.sleep(SLEEP_S)\n",
    "        if not ok:\n",
    "            notes.append(f\"download_failed_{i}\")\n",
    "            continue\n",
    "        notes.append(f\"downloaded_{i}\")\n",
    "\n",
    "        txt = extract_pdf_text_fitz(pdf_path)\n",
    "        if not txt.strip():\n",
    "            notes.append(f\"no_text_{i}\")\n",
    "            continue\n",
    "\n",
    "        s_ok, f_ok, hits = evidence_from_text_scoping_feas(txt)\n",
    "        if s_ok:\n",
    "            scope_flag = \"Y\"\n",
    "        if f_ok:\n",
    "            feas_flag = \"Y\"\n",
    "        if hits:\n",
    "            hit_pdfs.append(str(pdf_path))\n",
    "            for h in hits:\n",
    "                snippets.append(f\"[WEB] {h}\")\n",
    "\n",
    "        if scope_flag == \"Y\" and feas_flag == \"Y\":\n",
    "            break\n",
    "\n",
    "    if snippets:\n",
    "        summary = trim_to_50_words(\" \".join(snippets))\n",
    "        snippet_txt = \"\\n\".join(snippets[:8])\n",
    "    else:\n",
    "        summary, snippet_txt = \"\", \"\"\n",
    "\n",
    "    return scope_flag, feas_flag, summary, snippet_txt, \"; \".join(hit_pdfs[:5]), \"; \".join(notes)\n",
    "\n",
    "\n",
    "def main():\n",
    "    FALLBACK_DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    FALLBACK_DEBUG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    companies = read_companies(COMPANIES_CSV)\n",
    "\n",
    "    # Local PDF index\n",
    "    pdf_index = build_pdf_index(DOWNLOAD_DIRS)\n",
    "\n",
    "    # Optional: bring forward prior columns (not required)\n",
    "    prior = None\n",
    "    try:\n",
    "        prior = pd.read_csv(PRIOR_FINAL_CSV)\n",
    "        prior.columns = [c.strip() for c in prior.columns]\n",
    "        prior[\"Ticker\"] = prior[\"Ticker\"].astype(str).str.strip().str.upper()\n",
    "    except Exception:\n",
    "        prior = None\n",
    "\n",
    "    out_rows = []\n",
    "\n",
    "    for _, r in tqdm(companies.iterrows(), total=len(companies)):\n",
    "        tkr = r[\"Ticker\"].strip().upper()\n",
    "        company = r[\"Company\"]\n",
    "        website = r[\"Website\"]\n",
    "\n",
    "        local_pdfs = pdf_index.get(tkr, [])\n",
    "        local_scope, local_feas, local_sum, local_snips, local_hit_pdfs = scan_local_pdfs(tkr, local_pdfs)\n",
    "\n",
    "        notes = []\n",
    "        web_checked = \"N\"\n",
    "        web_scope = \"N\"\n",
    "        web_feas = \"N\"\n",
    "        web_sum = \"\"\n",
    "        web_snips = \"\"\n",
    "        web_hit_pdfs = \"\"\n",
    "\n",
    "        # Web fallback only if local had no evidence\n",
    "        if local_scope == \"N\" and local_feas == \"N\":\n",
    "            web_checked = \"Y\"\n",
    "            ws, wf, wsum, wsnips, whit, wnotes = scan_web_fallback(tkr, company, website)\n",
    "            web_scope, web_feas = ws, wf\n",
    "            web_sum, web_snips = wsum, wsnips\n",
    "            web_hit_pdfs = whit\n",
    "            if wnotes:\n",
    "                notes.append(wnotes)\n",
    "\n",
    "        # Final combined\n",
    "        final_scope = \"Y\" if (local_scope == \"Y\" or web_scope == \"Y\") else \"N\"\n",
    "        final_feas  = \"Y\" if (local_feas == \"Y\" or web_feas == \"Y\") else \"N\"\n",
    "\n",
    "        combined_snips = \"\\n\".join([x for x in [local_snips, web_snips] if x.strip()])\n",
    "        combined_sum = trim_to_50_words(combined_snips.replace(\"\\n\", \" \")) if combined_snips.strip() else \"\"\n",
    "\n",
    "        out_rows.append(asdict(OutRow(\n",
    "            Ticker=tkr,\n",
    "            Company=company,\n",
    "            Website=website,\n",
    "\n",
    "            Local_PDF_Count=len(local_pdfs),\n",
    "            Local_Scoping_Flag=local_scope,\n",
    "            Local_Feasibility_Flag=local_feas,\n",
    "            Local_Evidence_Summary_50w=local_sum,\n",
    "            Local_Evidence_Snippets=local_snips,\n",
    "            Local_Evidence_PDFs=local_hit_pdfs,\n",
    "\n",
    "            Web_Checked=web_checked,\n",
    "            Web_Scoping_Flag=web_scope,\n",
    "            Web_Feasibility_Flag=web_feas,\n",
    "            Web_Evidence_Summary_50w=web_sum,\n",
    "            Web_Evidence_Snippets=web_snips,\n",
    "            Web_Evidence_PDFs=web_hit_pdfs,\n",
    "            Notes=\"; \".join(notes),\n",
    "\n",
    "            Scoping_Flag=final_scope,\n",
    "            Feasibility_Flag=final_feas,\n",
    "            Evidence_Summary_50w=combined_sum,\n",
    "            Evidence_Snippets=combined_snips\n",
    "        )))\n",
    "\n",
    "    out_df = pd.DataFrame(out_rows)\n",
    "\n",
    "    # If you want to merge prior flags into this output, do it here (optional)\n",
    "    if prior is not None:\n",
    "        # Keep a few interesting columns if present\n",
    "        keep_cols = [c for c in [\"Resource_Definition_Flag\",\"PFS_Flag\",\"Quarterly_URL\",\"Annual_URL\",\"Quarterly_PDF\",\"Annual_PDF\"] if c in prior.columns]\n",
    "        if keep_cols:\n",
    "            out_df = out_df.merge(prior[[\"Ticker\"] + keep_cols], on=\"Ticker\", how=\"left\")\n",
    "\n",
    "    out_df.to_csv(OUT_FINAL, index=False, encoding=\"utf-8\")\n",
    "    hits_df = out_df[(out_df[\"Scoping_Flag\"]==\"Y\") | (out_df[\"Feasibility_Flag\"]==\"Y\")].copy()\n",
    "    hits_df.to_csv(OUT_HITS, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    print(\"Saved:\", OUT_FINAL)\n",
    "    print(\"Saved hits:\", OUT_HITS, f\"(hits={len(hits_df)}/{len(out_df)})\")\n",
    "    return out_df, hits_df\n",
    "\n",
    "\n",
    "# ---- RUN ----\n",
    "out_df, hits_df = main()\n",
    "hits_df.head(25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7937ff-5fb1-4c7f-bb4b-f5be30c7fcd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-py311] *",
   "language": "python",
   "name": "conda-env-.conda-py311-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
